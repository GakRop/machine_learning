# -*- coding: utf-8 -*-
"""Project 3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jHpgc1c6imtEooEKdhGf1deKbtbMfB2V
"""

# Project 2
#Overall for each part (coding, writing, and video segements) were equally distributed between the two of us Steven Ohms and Gak Roppongi
#A majority of the time we were working together in person, and constantly looked over/edited each others code
#Note whenever a cell says one of our names, we assume a majority of the work in the cell was done by that individual, but was still split between the two of us

import numpy as np
import pandas as pd
import math
import random

#Gak Roppongi
#importing the data sets, and deal with the missing values and one-hot encoding
#Breast cancer, Glass dataset, Soybean datasets are classification
#Abalone, Computer, Forest datasets are regression

breast_cancer_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', 
                                    names = ["radius", "texture", "perimeter", "area", "smoothness", "compactness", "concavity", "concave points", 
                                             "symmetry", "fractal dimension", "class"])
#"symmetry" may be label-encoded, but I'm not sure 
#the sum of number of instances of 1 ... 7 in "class" attribute is 699 = number of all instances
#therefore, "class" should be the categorical attribute
#no categorical value is missed
#There are 16 instances in Groups 1 to 6 that contain a single missing (i.e., unavailable) attribute value, now denoted by "?".  
#699 * 11

glass_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data', 
                            names = ["id", "RI", "Na", "Mg", "Al", "Si", "K", "Ca", "Ba",  "Fe", "type of glass"])
#the "type of glass" is label-encoded
#everything else is numerical attribute
#Number of Attributes: 10 (including an Id#) plus the class attribute -- all attributes are continuously valued
#the last attribute, "class", has label-encoded for 7 categories from 1~7
#no missing attributes values
#214 * 11 

soybean_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data', 
                              names = ["date", "plant-stand", "precip", "temp", "hail", "crop-hist", "area-damaged", 
                                       "severity", "seed-tmt", "germination", "plant-growth", "leaves", "leafspots-halo", 
                                       "leafspots-marg", "leadspot-size", "leaf-shread", "lead-malf", "lead-mild", "stem", 
                                       "lodging", "stem-cankers", "fruiting-bodies", "external-decay", "mycelium", "int-discolor", 
                                       "sclerotia", "fruit-pods", "fruit-spots", "seed", "mold-growth", "seed-discolor", "seed-size", "shriveling", "roots"])
#many of the attributes are string data type and label-encoded
#no missing attribute values
#all values have been normalized
#"roots" is the categorical value
#46 * 36

abalone_dataset = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data",
                              names = ["sex", "length", "diameter", "height", "whole weight", "shucked weight", "viscera weight", "shell weight", "rings"])

#no missing values
#ring is the class
#4177 instances
#one hot encode sex, then hamming distance
#8 features


computer_dataset = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/machine.data",
                               names = ["vendor name", "model name", "MYCT", "MMIN", "MMAX", "CACH", "CHMIN", "CHMAX", "PRP", "ERP"])
#no missing values
#PRP is the class here
#vendor name and model name are string data
#everything else is numerical data
#209 instances
#10 features
#remove vendor/model name

forest_dataset = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv",
                             names = ["X", "Y", "month", "day", "FFMC", "DMC", "DC", "ISI", "temp", "RH", "wind", "rain", "area"])
#no missing values
#517 instances
#12 features
#area - the burned area of the forest (in ha): 0.00 to 1090.84 
#(this output variable is very skewed towards 0.0, 
#thus it may make sense to model with the logarithm transform).
#area is the class here
#month and day are cyclical, so for day, distance between sunday and moday would be 1, or 6, but 1 is lower

breast_class = np.array([2, 4])
glass_class = np.array([1, 2, 3, 5, 6, 7])
soybean_class = np.array([1, 2, 3, 4])
#soybean_class = np.array(['D1', 'D2', 'D3', 'D4'])
abalone_class = np.array(["rings"])
computer_class = np.array(["PRP"])
forest_class = np.array(["area"])

#do not need model name or vendor name in the computer dataset
#row dataset does not contain any data, just attribute names
computer_dataset = computer_dataset.drop("vendor name", axis=1);
computer_dataset = computer_dataset.drop("model name", axis=1);

forest_dataset = forest_dataset.drop(index = 0)


soybean_dataset = soybean_dataset.reset_index(drop = True)

breast_cancer_dataset.sample(frac=1)
breast_cancer_dataset.sort_values(by=["class"], inplace=True)
glass_dataset.sort_values(by=["type of glass"], inplace=True)
soybean_dataset.sort_values(by=["roots"], inplace=True)
abalone_dataset.sort_values(by=["rings"], inplace=True)
computer_dataset.sort_values(by=["PRP"], inplace=True)
forest_dataset.sort_values(by=["area"], inplace=True)

#turns the abalone_dataset into something easier to read, and turns the days of the week and months into integers so distance can be calculated
breast_cancer_dataset = breast_cancer_dataset.replace(["?"], np.nan)

abalone_dataset["sex"] = abalone_dataset["sex"].replace(["M"], ["Male"])
abalone_dataset["sex"] = abalone_dataset["sex"].replace(["F"], ["Female"])
abalone_dataset["sex"] = abalone_dataset["sex"].replace(["I"], ["Infant"])

forest_dataset["month"] = forest_dataset["month"].replace(["jan"], [1])
forest_dataset["month"] = forest_dataset["month"].replace(["feb"], [2])
forest_dataset["month"] = forest_dataset["month"].replace(["mar"], [3])
forest_dataset["month"] = forest_dataset["month"].replace(["apr"], [4])
forest_dataset["month"] = forest_dataset["month"].replace(["may"], [5])
forest_dataset["month"] = forest_dataset["month"].replace(["jun"], [6])
forest_dataset["month"] = forest_dataset["month"].replace(["jul"], [7])
forest_dataset["month"] = forest_dataset["month"].replace(["aug"], [8])
forest_dataset["month"] = forest_dataset["month"].replace(["sep"], [9])
forest_dataset["month"] = forest_dataset["month"].replace(["oct"], [10])
forest_dataset["month"] = forest_dataset["month"].replace(["nov"], [11])
forest_dataset["month"] = forest_dataset["month"].replace(["dec"], [12])

forest_dataset["day"] = forest_dataset["day"].replace(["mon"], [1])
forest_dataset["day"] = forest_dataset["day"].replace(["tue"], [2])
forest_dataset["day"] = forest_dataset["day"].replace(["wed"], [3])
forest_dataset["day"] = forest_dataset["day"].replace(["thu"], [4])
forest_dataset["day"] = forest_dataset["day"].replace(["fri"], [5])
forest_dataset["day"] = forest_dataset["day"].replace(["sat"], [6])
forest_dataset["day"] = forest_dataset["day"].replace(["sun"], [7])

breast_cancer_dataset['concavity'] = breast_cancer_dataset['concavity'].astype(float)

computer_dataset["MYCT"] = computer_dataset["MYCT"].astype(float)
computer_dataset["MMIN"] = computer_dataset["MMIN"].astype(float)
computer_dataset["MMAX"] = computer_dataset["MMAX"].astype(float)
computer_dataset["CACH"] = computer_dataset["CACH"].astype(float)
computer_dataset["CHMIN"] = computer_dataset["CHMIN"].astype(float)
computer_dataset["CHMAX"] = computer_dataset["CHMAX"].astype(float)
computer_dataset["PRP"] = computer_dataset["PRP"].astype(float)
computer_dataset["ERP"] = computer_dataset["ERP"].astype(float)

forest_dataset = forest_dataset.astype(float)

#Steven Ohms
#this section of the code will turn all the vallues in the dataframes into numeric values
#all values are saved as str in the first place
#we will turn them into numeric

#glass, iris, and  soybean don't have the missing value

#this function does mean-filling of the dataset for the missing value
#df is the dataframe we manipulate
#attribute is the attribute in the dataframe we are manipulating

def mean_filling(df, attribute):
  mean = df[attribute].mean()
  df[attribute] = df[attribute].replace(np.nan, mean)
  return df

breast_cancer_dataset = mean_filling(breast_cancer_dataset, "concavity")

#Gak Roppongi
#this function implements the one-hot encoding
#df is the dataset we manipulate
#attribute is the attribute that we manipulate in str
#returns the one-hot encoded dataframe

def onehot(df, attribute):
  # Get one hot encoding of columns B
  one_hot = pd.get_dummies(df[attribute])
  # Drop column B as it is now encoded
  df = df.drop(attribute,axis = 1)
  # Join the encoded df
  df = df.join(one_hot)
  return df

abalone_dataset = onehot(abalone_dataset, "sex")
glass_dataset = onehot(glass_dataset, "type of glass")
soybean_dataset = onehot(soybean_dataset, "roots")
breast_cancer_dataset = onehot(breast_cancer_dataset, "class")

soybean_dataset.rename(columns = {'D1':1}, inplace = True)
soybean_dataset.rename(columns = {'D2':2}, inplace = True)
soybean_dataset.rename(columns = {'D3':3}, inplace = True)
soybean_dataset.rename(columns = {'D4':4}, inplace = True)

"""
Steven Ohms
this section of the code will normalize the dataset
the function will take df and attribute
df is the dataframe we are manipulating
attribute is the attribute we are normalizing
this is the z-score/standard score normalization
normalized value = Xi - Xmean / (standard deviation)
"""

breast_normalized_class = ["radius", "texture", "perimeter", "area", "smoothness", 
                           "compactness", "concavity", "concave points", "symmetry", "fractal dimension"]
glass_normalized_class = ["id", "RI", "Na", "Mg", "Al", "Si", "K", "Ca", "Ba", "Fe"]
abalone_normalized_class = ["length", "diameter", "height", "whole weight", "shucked weight", "viscera weight", "shell weight"]
computer_normalized_class = ["MYCT", "MMIN", "MMAX", "CACH", "CHMIN", "CHMAX", "PRP", "ERP"]
forest_normalized_class = ["X", "Y", "FFMC", "DMC", "DC", "ISI", "temp", "RH", "wind", "rain", "area"]

def normalize(df, attribute):
  column = df[attribute]
  column = (column - column.mean()) / column.std()
  df[attribute] = column
  return df


for a in breast_normalized_class:
  breast_cancer_dataset = normalize(breast_cancer_dataset, a)

for b in glass_normalized_class:
  glass_dataset = normalize(glass_dataset, b)

for c in abalone_normalized_class:
  abalone_dataset = normalize(abalone_dataset, c)

for d in computer_normalized_class:
  computer_dataset = normalize(computer_dataset, d)

for e in forest_normalized_class:
  forest_dataset = normalize(forest_dataset, e)

#merges all the portions together to create a testing set
 
def train_merge(one, two, three, four, five, six, seven, eight, nine):
  final_train = pd.merge(one, two, how="outer")
  final_train = pd.merge(final_train, three, how="outer")
  final_train = pd.merge(final_train, four, how="outer")
  final_train = pd.merge(final_train, five, how="outer")
  final_train = pd.merge(final_train, six, how="outer")
  final_train = pd.merge(final_train, seven, how="outer")
  final_train = pd.merge(final_train, eight, how="outer")
  final_train = pd.merge(final_train, nine, how="outer")
 
  return final_train

def set_split(dataset):
  i = 0
  j = 0
  HP_set = dataset.iloc[0:0]
  one = dataset.iloc[0:0]
  two = dataset.iloc[0:0]
  three = dataset.iloc[0:0]
  four = dataset.iloc[0:0]
  five = dataset.iloc[0:0]
  six = dataset.iloc[0:0]
  seven = dataset.iloc[0:0]
  eight = dataset.iloc[0:0]
  nine = dataset.iloc[0:0]
  ten = dataset.iloc[0:0]
  
  for i in range(dataset.shape[0]):
    if (i % 10 == 0):
      
      HP_set = pd.merge(HP_set, dataset.iloc[i:(i+1)], how="outer")
    else:
      h = j % 10
      if h == 0:
        one = pd.merge(one, dataset.iloc[i:(i+1)], how="outer")
      if h == 1:
        two = pd.merge(two, dataset.iloc[i:(i+1)], how="outer")
      if h == 2:
        three = pd.merge(three, dataset.iloc[i:(i+1)], how="outer")
      if h == 3:
        four = pd.merge(four, dataset.iloc[i:(i+1)], how="outer")
      if h == 4:
        five = pd.merge(five, dataset.iloc[i:(i+1)], how="outer")
      if h == 5:
        six = pd.merge(six, dataset.iloc[i:(i+1)], how="outer")
      if h == 6:
        seven = pd.merge(seven, dataset.iloc[i:(i+1)], how="outer")
      if h == 7:
        eight = pd.merge(eight, dataset.iloc[i:(i+1)], how="outer")
      if h == 8:
        nine = pd.merge(nine, dataset.iloc[i:(i+1)], how="outer")
      if h == 9:
        ten = pd.merge(ten, dataset.iloc[i:(i+1)], how="outer")
      j = j + 1
 
  return HP_set, one, two, three, four, five, six, seven, eight, nine, ten

glass_HP, glass_1, glass_2, glass_3, glass_4, glass_5, glass_6, glass_7, glass_8, glass_9, glass_10 = set_split(glass_dataset)
soybean_HP, soybean_1, soybean_2, soybean_3, soybean_4, soybean_5, soybean_6, soybean_7, soybean_8, soybean_9, soybean_10 = set_split(soybean_dataset)
breast_HP, breast_1, breast_2, breast_3, breast_4, breast_5, breast_6, breast_7, breast_8, breast_9, breast_10 = set_split(breast_cancer_dataset)
forest_HP, forest_1, forest_2, forest_3, forest_4, forest_5, forest_6, forest_7, forest_8, forest_9, forest_10 = set_split(forest_dataset)
computer_HP, computer_1, computer_2, computer_3, computer_4, computer_5, computer_6, computer_7, computer_8, computer_9, computer_10 = set_split(computer_dataset)
abalone_HP, abalone_1, abalone_2, abalone_3, abalone_4, abalone_5, abalone_6, abalone_7, abalone_8, abalone_9, abalone_10 = set_split(abalone_dataset)

breast_training_1 = train_merge(breast_10, breast_2, breast_3, breast_4, breast_5, breast_6, breast_7, breast_8, breast_9)
breast_training_2 = train_merge(breast_10, breast_1, breast_3, breast_4, breast_5, breast_6, breast_7, breast_8, breast_9)
breast_training_3 = train_merge(breast_10, breast_1, breast_2, breast_4, breast_5, breast_6, breast_7, breast_8, breast_9)
breast_training_4 = train_merge(breast_10, breast_1, breast_2, breast_3, breast_5, breast_6, breast_7, breast_8, breast_9)
breast_training_5 = train_merge(breast_10, breast_1, breast_2, breast_3, breast_4, breast_6, breast_7, breast_8, breast_9)
breast_training_6 = train_merge(breast_10, breast_1, breast_2, breast_3, breast_4, breast_5, breast_7, breast_8, breast_9)
breast_training_7 = train_merge(breast_10, breast_1, breast_2, breast_3, breast_4, breast_5, breast_6, breast_8, breast_9)
breast_training_8 = train_merge(breast_10, breast_1, breast_2, breast_3, breast_4, breast_5, breast_6, breast_7, breast_9)
breast_training_9 = train_merge(breast_10, breast_1, breast_2, breast_3, breast_4, breast_5, breast_6, breast_7, breast_8)
breast_training_10 = train_merge(breast_1, breast_2, breast_3, breast_4, breast_5, breast_6, breast_7, breast_8, breast_9)

soybean_training_1 = train_merge(soybean_10, soybean_2, soybean_3, soybean_4, soybean_5, soybean_6, soybean_7, soybean_8, soybean_9)
soybean_training_2 = train_merge(soybean_10, soybean_1, soybean_3, soybean_4, soybean_5, soybean_6, soybean_7, soybean_8, soybean_9)
soybean_training_3 = train_merge(soybean_10, soybean_1, soybean_2, soybean_4, soybean_5, soybean_6, soybean_7, soybean_8, soybean_9)
soybean_training_4 = train_merge(soybean_10, soybean_1, soybean_2, soybean_3, soybean_5, soybean_6, soybean_7, soybean_8, soybean_9)
soybean_training_5 = train_merge(soybean_10, soybean_1, soybean_2, soybean_3, soybean_4, soybean_6, soybean_7, soybean_8, soybean_9)
soybean_training_6 = train_merge(soybean_10, soybean_1, soybean_2, soybean_3, soybean_4, soybean_5, soybean_7, soybean_8, soybean_9)
soybean_training_7 = train_merge(soybean_10, soybean_1, soybean_2, soybean_3, soybean_4, soybean_5, soybean_6, soybean_8, soybean_9)
soybean_training_8 = train_merge(soybean_10, soybean_1, soybean_2, soybean_3, soybean_4, soybean_5, soybean_6, soybean_7, soybean_9)
soybean_training_9 = train_merge(soybean_10, soybean_1, soybean_2, soybean_3, soybean_4, soybean_5, soybean_6, soybean_7, soybean_8)
soybean_training_10 = train_merge(soybean_1, soybean_2, soybean_3, soybean_4, soybean_5, soybean_6, soybean_7, soybean_8, soybean_9)

glass_training_1 = train_merge(glass_10, glass_2, glass_3, glass_4, glass_5, glass_6, glass_7, glass_8, glass_9)
glass_training_2 = train_merge(glass_10, glass_1, glass_3, glass_4, glass_5, glass_6, glass_7, glass_8, glass_9)
glass_training_3 = train_merge(glass_10, glass_1, glass_2, glass_4, glass_5, glass_6, glass_7, glass_8, glass_9)
glass_training_4 = train_merge(glass_10, glass_1, glass_2, glass_3, glass_5, glass_6, glass_7, glass_8, glass_9)
glass_training_5 = train_merge(glass_10, glass_1, glass_2, glass_3, glass_4, glass_6, glass_7, glass_8, glass_9)
glass_training_6 = train_merge(glass_10, glass_1, glass_2, glass_3, glass_4, glass_5, glass_7, glass_8, glass_9)
glass_training_7 = train_merge(glass_10, glass_1, glass_2, glass_3, glass_4, glass_5, glass_6, glass_8, glass_9)
glass_training_8 = train_merge(glass_10, glass_1, glass_2, glass_3, glass_4, glass_5, glass_6, glass_7, glass_9)
glass_training_9 = train_merge(glass_10, glass_1, glass_2, glass_3, glass_4, glass_5, glass_6, glass_7, glass_8)
glass_training_10 = train_merge(glass_1, glass_2, glass_3, glass_4, glass_5, glass_6, glass_7, glass_8, glass_9)

abalone_training_1 = train_merge(abalone_10, abalone_2, abalone_3, abalone_4, abalone_5, abalone_6, abalone_7, abalone_8, abalone_9)
abalone_training_2 = train_merge(abalone_10, abalone_1, abalone_3, abalone_4, abalone_5, abalone_6, abalone_7, abalone_8, abalone_9)
abalone_training_3 = train_merge(abalone_10, abalone_1, abalone_2, abalone_4, abalone_5, abalone_6, abalone_7, abalone_8, abalone_9)
abalone_training_4 = train_merge(abalone_10, abalone_1, abalone_2, abalone_3, abalone_5, abalone_6, abalone_7, abalone_8, abalone_9)
abalone_training_5 = train_merge(abalone_10, abalone_1, abalone_2, abalone_3, abalone_4, abalone_6, abalone_7, abalone_8, abalone_9)
abalone_training_6 = train_merge(abalone_10, abalone_1, abalone_2, abalone_3, abalone_4, abalone_5, abalone_7, abalone_8, abalone_9)
abalone_training_7 = train_merge(abalone_10, abalone_1, abalone_2, abalone_3, abalone_4, abalone_5, abalone_6, abalone_8, abalone_9)
abalone_training_8 = train_merge(abalone_10, abalone_1, abalone_2, abalone_3, abalone_4, abalone_5, abalone_6, abalone_7, abalone_9)
abalone_training_9 = train_merge(abalone_10, abalone_1, abalone_2, abalone_3, abalone_4, abalone_5, abalone_6, abalone_7, abalone_8)
abalone_training_10 = train_merge(abalone_1, abalone_2, abalone_3, abalone_4, abalone_5, abalone_6, abalone_7, abalone_8, abalone_9)

computer_training_1 = train_merge(computer_10, computer_2, computer_3, computer_4, computer_5, computer_6, computer_7, computer_8, computer_9)
computer_training_2 = train_merge(computer_10, computer_1, computer_3, computer_4, computer_5, computer_6, computer_7, computer_8, computer_9)
computer_training_3 = train_merge(computer_10, computer_1, computer_2, computer_4, computer_5, computer_6, computer_7, computer_8, computer_9)
computer_training_4 = train_merge(computer_10, computer_1, computer_2, computer_3, computer_5, computer_6, computer_7, computer_8, computer_9)
computer_training_5 = train_merge(computer_10, computer_1, computer_2, computer_3, computer_4, computer_6, computer_7, computer_8, computer_9)
computer_training_6 = train_merge(computer_10, computer_1, computer_2, computer_3, computer_4, computer_5, computer_7, computer_8, computer_9)
computer_training_7 = train_merge(computer_10, computer_1, computer_2, computer_3, computer_4, computer_5, computer_6, computer_8, computer_9)
computer_training_8 = train_merge(computer_10, computer_1, computer_2, computer_3, computer_4, computer_5, computer_6, computer_7, computer_9)
computer_training_9 = train_merge(computer_10, computer_1, computer_2, computer_3, computer_4, computer_5, computer_6, computer_7, computer_8)
computer_training_10 = train_merge(computer_1, computer_2, computer_3, computer_4, computer_5, computer_6, computer_7, computer_8, computer_9)

forest_training_1 = train_merge(forest_10, forest_2, forest_3, forest_4, forest_5, forest_6, forest_7, forest_8, forest_9)
forest_training_2 = train_merge(forest_10, forest_1, forest_3, forest_4, forest_5, forest_6, forest_7, forest_8, forest_9)
forest_training_3 = train_merge(forest_10, forest_1, forest_2, forest_4, forest_5, forest_6, forest_7, forest_8, forest_9)
forest_training_4 = train_merge(forest_10, forest_1, forest_2, forest_3, forest_5, forest_6, forest_7, forest_8, forest_9)
forest_training_5 = train_merge(forest_10, forest_1, forest_2, forest_3, forest_4, forest_6, forest_7, forest_8, forest_9)
forest_training_6 = train_merge(forest_10, forest_1, forest_2, forest_3, forest_4, forest_5, forest_7, forest_8, forest_9)
forest_training_7 = train_merge(forest_10, forest_1, forest_2, forest_3, forest_4, forest_5, forest_6, forest_8, forest_9)
forest_training_8 = train_merge(forest_10, forest_1, forest_2, forest_3, forest_4, forest_5, forest_6, forest_7, forest_9)
forest_training_9 = train_merge(forest_10, forest_1, forest_2, forest_3, forest_4, forest_5, forest_6, forest_7, forest_8)
forest_training_10 = train_merge(forest_1, forest_2, forest_3, forest_4, forest_5, forest_6, forest_7, forest_8, forest_9)

"""
0-1 loss function
param: array of results. 
results are array of predicted and actual response of each instance
"""
def zero_loss_func(actual_matrix, pred_matrix):
  correct = 0
  incorrect = 0
  for i in range(len(actual_matrix)):
    if actual_matrix[i] == pred_matrix[i][0]:
      correct = correct+1
    elif actual_matrix[i] != pred_matrix[i][0]:
      incorrect = incorrect + 1
  
  return [correct, incorrect]

# def cross_entropy_func_binary(actual_outputs, predicted_outputs):
#   if actual_outputs == 2:
#     t1 = 1
#     t2 = 0
#   elif actual_outputs == 4:
#     t1 = 0
#     t2 = 1

#   bin_cross_entropy = -((t1 * math.log(predicted_outputs[1][0])) + t2 * math.log(predicted_outputs[1][1]))
#   return(bin_cross_entropy)




# #either 2 or 4

def cros_entropy_func_multivariate(actual_outputs, predicted_outputs, class_names):
  for i in range(len(class_names)):
    if actual_outputs == class_names[i]:
      cross_entropy =  math.log2(predicted_outputs[1][i])

  return cross_entropy

"""
takes the matrix as the argument
calculates the MSE out of the matrix
"""
def Mean_Squared_Error(matrix):
  MSE = 0
  n = len(matrix)
  for i in range(n):
    MSE += (matrix[i][0] - matrix[i][1])**2
  
  MSE = MSE/n
  return MSE

"""
takes the matrix as the argument
calculates the SSR out of the matrix
actual_outputs is the list of actual outputs
predicted_outputs is the list of predicted outputs
"""
def SSR(actual_outputs, predicted_outputs):
  SSR = 0
  for i in range(len(actual_outputs)):
    SSR += (actual_outputs[i] - predicted_outputs[i])**2

  return SSR

"""
takes the matrix as the argument
calculates the SR out of the matrix
actual_outputs is the list of actual outputs
predicted_outputs is the list of predicted outputs
"""
def sum_of_residual(actual_outputs, predicted_outputs):
  SR = 0
  for i in range(len(actual_outputs)):
    SR += (actual_outputs[i] - predicted_outputs[i])

  return SR

"""
these are the test data sets
"""
regression_sample = {"GPA": [3.8, 3.9, 3.95, 3.85, 3.55, 3.65, 3.75, 4.0, 3.6, 3.7,
                             1.5, 1.8, 2.5, 2.4, 2.45, 1.85, 1.95, 2.1, 2.2, 2.3],
                     "year": [25, 22, 23, 24, 25, 24, 23, 20, 22, 21, 
                              8, 7, 6, 5, 3, 10, 9, 7, 9, 4],
                     "income": [1500, 2000, 2500, 2200, 2400, 1750, 2450, 1850, 1650, 2250,
                                600, 500, 800, 900, 700, 100, 200, 300, 400, 1000]}

regression_sample = pd.DataFrame(regression_sample)

classification_sample = {"GPA": [3.8, 3.9, 3.95, 3.85, 3.55, 3.65, 3.75, 4.0, 3.6, 3.7,
                                 1.5, 1.8, 2.5, 2.4, 2.45, 1.85, 1.95, 2.1, 2.2, 2.3],
                         "year": [25, 22, 23, 24, 25, 24, 23, 20, 22, 21, 
                                 8, 7, 6, 5, 3, 10, 9, 7, 9, 4],
                         "income": [1500, 2000, 2500, 2200, 2400, 1750, 2450, 1850, 1650, 2250,
                                   600, 500, 800, 900, 700, 100, 200, 300, 400, 1000],
                         "university": ["yes", "yes", "yes", "yes", "yes", "yes", "yes", "yes", "yes", "yes", 
                                        "no", "no", "no", "no", "no", "no", "no", "no", "no", "no"]}

classification_sample = pd.DataFrame(classification_sample)

classification_sample = onehot(classification_sample, "university")

regression_sample_normalized_class = ["GPA", "year", "income"]
classification_sample_normalized_class = ["GPA", "year", "income"]

regression_sample_class = ["income"]
classification_sample_class = ["yes", "no"]

for z in regression_sample_normalized_class:
  regression_sample = normalize(regression_sample, z)

for y in classification_sample_normalized_class:
  classification_sample = normalize(classification_sample, y)

"""
this is for the hidden layers
we chose logistic regression
"""
def sigmoid_activation(input):
  try:
    sigmoid_activation_output = 1 / (1 + math.exp(- input))
  except OverflowError:
    sigmoid_activation_output = float('inf')
  return sigmoid_activation_output
 
#sigmoid_activation(5)
 
"""
for regression output
param: a number
"""
def linear_activation(input):
  return input
 
"""
for classification output
param: list of numbers, float
"""
def softmax_activation(input_list, class_name):
  current_max = 0
  current_max_pos = 0
  denom = 0
  softmax_output_list = [0 for k in range(len(input_list))]
  for j in range(len(input_list)):
    denom = denom + math.exp(input_list[j])
  for i in range(len(input_list)):
    current_test = math.exp(input_list[i])/denom
    softmax_output_list[i] = current_test
    if i == 0:
      current_max = current_test
      current_max_pos = i
      
    elif current_max < current_test:
      current_max = current_test
      current_max_pos = i
      
      #print("this is the softmax_output_list  ", class_name[current_max_pos], softmax_output_list, current_max_pos)
  return class_name[current_max_pos], softmax_output_list, current_max_pos
 

def cross_entropy_loss(actual_outputs, predicted_outputs, class_names):
  for i in range(len(class_names)):
    if actual_outputs == class_names[i]:
      cross_entropy =  -1 * math.log2(predicted_outputs[1][i])
  return cross_entropy


j = [1, 2, 3]
class_name = [1, 2, 3]
predicted_soft = softmax_activation(j, class_name)
print(predicted_soft)
#print(cross_entropy_loss(3, predicted_soft, class_name))

"""
param: hidden layer, learning_rate, max_ite, min_step, momentum
returns: result list of [actual response, predicted response]

H = the structure of hidden layers, len(H) is the number of layers, len[i] is number of nodes for each layer, list
LR = Learning Rate, float
max_ite = max iteration, int
min_step = minimum step size, float
alpha = momentum, float
dftest = the test data set, pd.DataFrame
dftrain = the training data set, pd.DataFrame
class_name = the list of strings of the name of columns that store the responsive variable of the data set, list
regression = True is the response value of the dataset is numerical, False otherwise, regression=True as default
"""

def initialize_network(structure, dftrain, dftest, class_name, regression=True):
  if structure[0] == 0:
    network = list() #<- initialize a network as list
    n_outputs = len(class_name)
    n_inputs = len(dftrain.columns) - n_outputs
    structure = [n_inputs, n_outputs]

  else:
    network = list() #<- initialize a network as list
    n_outputs = len(class_name)
    n_inputs = len(dftrain.columns) - n_outputs
    structure.insert(0, n_inputs) #<- insert the n_inputs in the beginning of the structure
    structure.append(n_outputs) #<- insert the n_inputs in the end of the structure
    #print(structure)
	
  #"random()" genrates random floating number b/w 0 and 1

  for i in range(1, len(structure)): #<- goes through the for loop for (number of layers - 1)
    weights = [] #<- initialize len(structure) number of hidden layer list

    for j in range(structure[i]): #<- goes though the for loop for number of nodes in hidden layers
      random_list = [] #<- initialize the list of initial weights for the i-th layer
      outputs_list = []

      for k in range(structure[i-1] + 1): #<- the # of weight coming into a node is (the number of nodes in the previous layer + 1)
        random_list.append(random.random()) #<- generates random number for each initial weight and add that in the "random_list"
      
      weights_and_outputs = [{"weights": random_list}, {"outputs": outputs_list}]
      #weights.append({"weights": random_list})
      #weights.append({"outputs": outputs_list})
      weights.append(weights_and_outputs)
      #print(weights)
    
    network.append(weights)
  print(network)
  return network

regression_sample_network = initialize_network([0], regression_sample, regression_sample, regression_sample_class, regression = True)

for layer in regression_sample_network:
  for neuron in layer:
    print(neuron)

"""
Forward propagate input to a network output
network: initialized network, the list of dictionaries of lists of weights
instance: each instance in the training data set
class_name: a list of column name that stores the response variable in the data set, list

this funciton intakes the network (lsit of weights) and each instance in the data set
to run the entire network to obtian the predicted result

this will return the output for each instance in the dataset, and the network it used
return: 
outputs = [actual_outputs, predicted_outputs], list
actual_outputs = list of actual output for each instance in the training dataset, len(actual_outputs) = number of instances in the dataset, list
predicted_outputs = list of predicted output for each instance in the training dataset, len(actual_outputs) = number of instances in the dataset, list
"""
def forward_propagate(dftrain, network, class_name, regression=True):
  #print("forward_prop")
  actual = None
  actual_outputs = []
  predicted_outputs = []
  result = []

  for i in range(dftrain.shape[0]):
    instance = dftrain.loc[i]  
    ###print(instance)
    #assigns the actual response varibale to "actual"
    if regression == True:
      actual = float(instance[class_name])
    
    elif regression != True:
      for column in class_name:
        if instance[column] == 1 or instance[column] == 1.0:
          actual = column
          

    for column in class_name: #<- this goes through each column in the instance to drop the response variable column
      instance = instance.drop(labels = column)
    #print(instance)
    instance = instance.tolist() #<- convert instance (type(inputs) = pandas.series) into list
    
    inputs = instance #<- the inputs are the attribute values in the instance.
    

    for layer in network: #<- this goes through each list of 'weights' dictionaries in each layer.
      ###print(layer)
      new_inputs = [] #<- this stores the input that are calculated at a layer. This becomes the input to the next layer.

      for neuron in layer: #<- this goes through each 'weights' dictionary in each layer
        activation = 0
        weights = neuron[0]["weights"] #<- stores the weights coming in to each neuron in a list
        
        sum = 0
        #print(inputs)
        for i in range(len(inputs)): #<- this goes through each weight in a neuron
          ###print("this is i: ", i)
          sum += weights[i] * inputs[i] #<- multiplies each weight and each input
          ###print("weights of i is: ", weights[i], "   inputs of i is: ", inputs[i])
        ###print("this is the sum of weights[i] times inputs[i]: ", sum)
        bias = weights[len(inputs)]
        ###print("this is the bias where bias is equal to the weights[len(inputs)]", bias)
        sum += bias #<- then, adds the bias to the product of weights and inputs
        ###print("new sum is sum + bias: ", sum)
        activation = sigmoid_activation(sum) #<- call sigmoid activation function and get the output of the neuron
        #neuron['output'] = activation
        #print("current output:", neuron[0]["current output"])
        #new_inputs.append(neuron['output']) #append only the output of the neuron in new_inputs placeholder list
        
        ###print("the outcome of sigmoid activation is: ", activation)
        neuron[1]["outputs"].append(activation)
        new_inputs.append(activation)
      
      inputs = new_inputs #<- update the inputs list with new_inputs list

    
    if regression==True:
      ###print("Inputs sent to linear activation is: ", inputs)
      inputs = float(linear_activation(inputs[0]))



    elif regression!=True:
      #print(inputs)
      ###print("Inputs sent to softmax activation is: ", inputs)
      inputs = softmax_activation(inputs, class_name)
      

    #"inputs" is the final input will be the input for the layer after the final output layer = this input is the output of the entire network
    #actual is the read response variable for each instance
    preidcted = inputs
    actual_outputs.append(actual)
    predicted_outputs.append(preidcted)

  result = [actual_outputs, predicted_outputs]
  return result

"""
this function takes the derivative of the error term and the weight.
then updates the weight according to the derivative.
iterates this step until it hits the maximum iteration or the calculated step size reaches the minimum step size
this function does this only for a 
"""
def gradient_descent(dftrain, network, layer_depth, step_size, neuron_index, neuron, actual_outputs, class_name, LR, max_ite, min_step, regression):
  #print("gradient_descent")
  weights = neuron[0]["weights"] #store the weights list in the dummy variable "weights"
  bias = weights[len(weights)-1] #only the bias, float or int
  weights = weights[0:-1] #only the weights, list
  #print("layer depth", layer_depth)
  a = 0
  condition = True
  #while (a < max_ite == True) or (step_size > min_step == True):
  while (condition == True):
    a += 1

    if a > max_ite:
      condition = False

    #print(a, "th iteration")
    #print("max_ite", max_ite)
    outputs = forward_propagate(dftrain, network, class_name, regression)
    if regression == True:
      actual_outputs = outputs[0]
      predicted_outputs = outputs[1]
      #gradient descent for the bias
      #dSSR/dBias = -2 * sum(actual - preditct)
      #step_size = dSSR/dBias * LR
      #weights[j] = weights[j] - step_size
      #bias = 0.763774618976614, float or int
      #print("bias updating ...")
      dBias = -2 * sum_of_residual(actual_outputs, predicted_outputs) #<- takes the derivative
      print("Dbias is : ", dBias)
      step_size = dBias * LR #<- multiplies the derivative by the learning rate
      print("step_size is: ", step_size)
      bias = bias - step_size #<- update the bias
      print("updataed bias is: ", bias)
    if regression != True:
      
      actual_outputs = outputs[0]
      #print(actual_outputs)
      predicted_outputs = outputs[1]
      #print("THis is predicted", predicted_outputs)
      dBias = 0
      for f in range(len(actual_outputs)):
        for h in range(len(class_name)):
          if actual_outputs[f] == class_name[h]:
            actual_class_pos = h
        if actual_outputs[f] == predicted_outputs[f][0]:
          dBias += dBias + 1 - predicted_outputs[f][1][actual_class_pos]
        elif actual_outputs[f] != predicted_outputs[f][0]:
          dBias += dBias - predicted_outputs[f][1][actual_class_pos]

      print("dBias is: ", dBias)
      step_size = dBias * LR
      print("step size is: ", step_size)
      bias = bias - step_size
      print("updated bias is: ", bias)
    
    if step_size < min_step:
      condition = False
    #---------------------------------------------------------------------------
    #print("before:", network[layer_depth][neuron_index][0]["weights"][len(weights)-1])
    #print("step:", step_size)
    network[layer_depth][neuron_index][0]["weights"][len(weights)-1] = bias
    #print("after:", network[layer_depth][neuron_index][0]["weights"][len(weights)-1])
    #---------------------------------------------------------------------------

    prev_layer = layer_depth - 1
    for b in range(len(weights)):
      #print("weights updating ...")
      #gradient descent for the weights
      #dSSR/dW = -2 * sum(Xi * (actual - predict)) 
      #dSSR/dW = -2 * sum(Xi) * sum(SSR) <- SSR(actual and predicted)
      #step_size = dSSR/dW * LR
      #weights[j] = weights[j] - step_size
      #weights: ex:) [0.13436424411240122, 0.8474337369372327], list

      weight = weights[b]
      Xi = []
      predictor_list = dftrain.columns

      if layer_depth == 0: #this the case where the input is the actual input of the instance
        for c in range(dftrain.shape[0]):
          instance = dftrain.loc[c]
          predictor = predictor_list[b]
          predictor = instance[predictor]
          Xi.append(predictor)
      
      else: #this is where the input for the weight is the output from the previous layer
        layer = network[prev_layer]
        neuron = layer[b]
        Xi = neuron[1]["outputs"] #we want Xi to be a list of inputs to the neuron


      if regression == True:
        product = 0
        for d in range(len(Xi)):
          #print(Xi)
          product += -Xi[d] * (actual_outputs[d] - predicted_outputs[d])
        
        dWi = 2 * product #<- takes the derivative
        print("dWi is: ", dWi)
      #Xi is only an output from the single instance in the training dataset
        step_size = dWi * LR #<- multiplies the derivative by the learning rate
        print("step_size is: ", step_size)
        weight = weight - step_size #<- update the weight
        print("updated weight is: ", weight)

      if regression != True:
        product = 0
        for d in range(len(Xi)):
          for h in range(len(class_name)):
            if actual_outputs[d] == class_name[h]:
              actual_class_pos = h
          if actual_outputs[d] == predicted_outputs[d][0]:
            product += -Xi[d] * (0 - predicted_outputs[d][1][actual_class_pos])
          elif actual_outputs[d] != predicted_outputs[d][0]:
            product += -Xi[d] * (1 - predicted_outputs[d][1][actual_class_pos])

        step_size = product * LR
        weight = weight - step_size
        print("dWi is: ", product)
        print("step_size is: ", step_size)
        print("updated weight is: ", weight)
      if step_size < min_step:
        condition = False

      #---------------------------------------------------------------------------
      #print("length:", len(network[layer_depth][neuron_index][0]["weights"]))
      #print("i:", i)
      #print("before:", network[layer_depth][neuron_index][0]["weights"][b])
      #print("step:", step_size)
      print("before: ", network[layer_depth][neuron_index][0]["weights"][b])
      network[layer_depth][neuron_index][0]["weights"][b] = weight
      print("after:", network[layer_depth][neuron_index][0]["weights"][b])
      #---------------------------------------------------------------------------


    for e in range(len(network)):
      layer = network[e]
      for f in range(len(layer)):
        neuron = layer[f]
        neuron[1]["outputs"].clear()

  return network

"""
this funciton will perform the back propagation to update the weight for the training dataset

the frist loop iterates through the layers in the network
the second loop iterates through the neurons in the layer

"""
def back_propagation(dftrain, network, actual_outputs, LR, max_ite, min_step, class_name, regression):
  #print("back_propagation")
  for layers in reversed(range(len(network))): #goes through each layer in the network in the reversed order (from output to input)
    layer = network[layers]
    layer_depth = layers
    #print(layer_depth)
    for i in range(len(layer)):
      #print(neuron) <- ex) {'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}
      #call the gradient descent for each neuron
      neuron = layer[i]
      step_size = 100
      network = gradient_descent(dftrain, network, layer_depth, step_size, i, neuron, actual_outputs, class_name, LR, max_ite, min_step, regression)

  return network

"""
this funciton trains the network and returns the trained network

param: 
dftrain = training dataset, pandas.dataframe
network = initialized network, list
LR = learning rate for the G.D., float
max_ite = max number of iteration for G.D., int
min_step = minimum step for G.D., float
class_name = class name of the responsive variable, list
regression = whether the dataset is regression dataset or not, bool
"""
def training_network(structure, dftrain, dftest, LR, max_ite, min_step, class_name, regression=True):
  network = initialize_network(structure, dftrain, dftest, class_name, regression)
  actual_outputs = []

  #going through each instance in the training dataset and store the actual response value for each instance in the actual_outputs list
  for a in range(dftrain.shape[0]):
    #print(a,"th instance in training")
    
    if regression == True:
      instance = dftrain.loc[a][class_name]
      actual_outputs.append(instance[class_name])
      #the actual response variable for regression dataset is the response value

    if regression != True:
      for b in range(len(class_name)):
        #print("the cirrent row is: ", dftrain.loc[i])
        #print("the j value is: ", j)
        instance = dftrain.loc[a][class_name[b]]
        #print("the instance is: ", instance)
        
        if instance == 1 or instance == 1.0:
          actual_outputs.append(class_name[b])
          #print("is appended, :", actual_outputs)
          #the actual response variable for regression dataset is the class of the instance
 
  #this is to get the actual outputs for all instances in the dataframe
 
  if regression == True:
    network = back_propagation(dftrain, network, actual_outputs, LR, max_ite, min_step, class_name, regression)
    #class the back_prop func. for each instance in the training dataset and update the network
    pass
 
  elif regression != True:
    network = back_propagation(dftrain, network, actual_outputs, LR, max_ite, min_step, class_name, regression)
 
  return network

regression_sample_network = initialize_network([10, 10], regression_sample, regression_sample, regression_sample_class, regression=True)

for layer in regression_sample_network:
  for neuron in layer:
    print(neuron)

def hyperparameter_tune_soybean(structure, LR, max_ite, min_step):
  "------------------------------------------------------------------------"
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  soybean_trained_network1 = training_network(structure, soybean_training_1, soybean_1, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results1 = forward_propagate(soybean_1, soybean_trained_network1, soybean_class, regression=False)
 
  soybean_trained_network2 = training_network(structure, soybean_training_2, soybean_2, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results2 = forward_propagate(soybean_2, soybean_trained_network2, soybean_class, regression=False)
 
  soybean_trained_network3 = training_network(structure, soybean_training_3, soybean_3, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results3 = forward_propagate(soybean_3, soybean_trained_network3, soybean_class, regression=False)
 
  soybean_trained_network4 = training_network(structure, soybean_training_4, soybean_4, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results4 = forward_propagate(soybean_4, soybean_trained_network4, soybean_class, regression=False)
 
  soybean_trained_network5 = training_network(structure, soybean_training_5, soybean_5, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results5 = forward_propagate(soybean_5, soybean_trained_network5, soybean_class, regression=False)
 
  soybean_trained_network6 = training_network(structure, soybean_training_6, soybean_6, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results6 = forward_propagate(soybean_6, soybean_trained_network6, soybean_class, regression=False)
 
  soybean_trained_network7 = training_network(structure, soybean_training_7, soybean_7, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results7 = forward_propagate(soybean_7, soybean_trained_network7, soybean_class, regression=False)
 
  soybean_trained_network8 = training_network(structure, soybean_training_8, soybean_8, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results8 = forward_propagate(soybean_8, soybean_trained_network8, soybean_class, regression=False)
 
  soybean_trained_network9 = training_network(structure, soybean_training_9, soybean_9, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results9 = forward_propagate(soybean_9, soybean_trained_network9, soybean_class, regression=False)
 
  soybean_trained_network10 = training_network(structure, soybean_training_10, soybean_10, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results10 = forward_propagate(soybean_10, soybean_trained_network10, soybean_class, regression=False)
 
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  print("results for test one")
  print(zero_loss_func(soybean_results1[0], soybean_results1[1]))
  print("results for test two")
  print(zero_loss_func(soybean_results2[0], soybean_results2[1]))
  print("results for test three")
  print(zero_loss_func(soybean_results3[0], soybean_results3[1]))
  print("results for test four")
  print(zero_loss_func(soybean_results4[0], soybean_results4[1]))
  print("results for test five")
  print(zero_loss_func(soybean_results5[0], soybean_results5[1]))
  print("results for test six")
  print(zero_loss_func(soybean_results6[0], soybean_results6[1]))
  print("results for test seven")
  print(zero_loss_func(soybean_results7[0], soybean_results7[1]))
  print("results for test eight")
  print(zero_loss_func(soybean_results8[0], soybean_results8[1]))
  print("results for test nine")
  print(zero_loss_func(soybean_results9[0], soybean_results9[1]))
  print("results for test ten")
  print(zero_loss_func(soybean_results10[0], soybean_results10[1]))
  "------------------------------------------------------------------------"

def hyperparameter_tune_forest(structure, LR, max_ite, min_step):
  "------------------------------------------------------------------------"
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  forest_trained_network1 = training_network(structure, forest_training_1, forest_1, LR, max_ite, min_step, forest_class, regression=True)
  forest_results1 = forward_propagate(forest_1, forest_trained_network1, forest_class, regression=True)
 
  forest_trained_network2 = training_network(structure, forest_training_2, forest_2, LR, max_ite, min_step, forest_class, regression=True)
  forest_results2 = forward_propagate(forest_2, forest_trained_network2, forest_class, regression=True)
 
  forest_trained_network3 = training_network(structure, forest_training_3, forest_3, LR, max_ite, min_step, forest_class, regression=True)
  forest_results3 = forward_propagate(forest_3, forest_trained_network3, forest_class, regression=True)
 
  forest_trained_network4 = training_network(structure, forest_training_4, forest_4, LR, max_ite, min_step, forest_class, regression=True)
  forest_results4 = forward_propagate(forest_4, forest_trained_network4, forest_class, regression=True)
 
  forest_trained_network5 = training_network(structure, forest_training_5, forest_5, LR, max_ite, min_step, forest_class, regression=True)
  forest_results5 = forward_propagate(forest_5, forest_trained_network5, forest_class, regression=True)
 
  forest_trained_network6 = training_network(structure, forest_training_6, forest_6, LR, max_ite, min_step, forest_class, regression=True)
  forest_results6 = forward_propagate(forest_6, forest_trained_network6, forest_class, regression=True)
 
  forest_trained_network7 = training_network(structure, forest_training_7, forest_7, LR, max_ite, min_step, forest_class, regression=True)
  forest_results7 = forward_propagate(forest_7, forest_trained_network7, forest_class, regression=True)
 
  forest_trained_network8 = training_network(structure, forest_training_8, forest_8, LR, max_ite, min_step, forest_class, regression=True)
  forest_results8 = forward_propagate(forest_8, forest_trained_network8, forest_class, regression=True)
 
  forest_trained_network9 = training_network(structure, forest_training_9, forest_9, LR, max_ite, min_step, forest_class, regression=True)
  forest_results9 = forward_propagate(forest_9, forest_trained_network9, forest_class, regression=True)
 
  forest_trained_network10 = training_network(structure, forest_training_10, forest_10, LR, max_ite, min_step, forest_class, regression=True)
  forest_results10 = forward_propagate(forest_10, forest_trained_network10, forest_class, regression=True)
 
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  print("results for test one")
  print(Mean_Squared_Error(forest_results1))
  print("results for test two")
  print(Mean_Squared_Error(forest_results2))
  print("results for test three")
  print(Mean_Squared_Error(forest_results3))
  print("results for test four")
  print(Mean_Squared_Error(forest_results4))
  print("results for test five")
  print(Mean_Squared_Error(forest_results5))
  print("results for test six")
  print(Mean_Squared_Error(forest_results6))
  print("results for test seven")
  print(Mean_Squared_Error(forest_results7))
  print("results for test eight")
  print(Mean_Squared_Error(forest_results8))
  print("results for test nine")
  print(Mean_Squared_Error(forest_results9))
  print("results for test ten")
  print(Mean_Squared_Error(forest_results10))
  "------------------------------------------------------------------------"

def hyperparameter_tune_computer(structure, LR, max_ite, min_step):
  "------------------------------------------------------------------------"
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  computer_trained_network1 = training_network(structure, computer_training_1, computer_1, LR, max_ite, min_step, computer_class, regression=True)
  computer_results1 = forward_propagate(computer_1, computer_trained_network1, computer_class, regression=True)
 
  computer_trained_network2 = training_network(structure, computer_training_2, computer_2, LR, max_ite, min_step, computer_class, regression=True)
  computer_results2 = forward_propagate(computer_2, computer_trained_network2, computer_class, regression=True)
 
  computer_trained_network3 = training_network(structure, computer_training_3, computer_3, LR, max_ite, min_step, computer_class, regression=True)
  computer_results3 = forward_propagate(computer_3, computer_trained_network3, computer_class, regression=True)
 
  computer_trained_network4 = training_network(structure, computer_training_4, computer_4, LR, max_ite, min_step, computer_class, regression=True)
  computer_results4 = forward_propagate(computer_4, computer_trained_network4, computer_class, regression=True)
 
  computer_trained_network5 = training_network(structure, computer_training_5, computer_5, LR, max_ite, min_step, computer_class, regression=True)
  computer_results5 = forward_propagate(computer_5, computer_trained_network5, computer_class, regression=True)
 
  computer_trained_network6 = training_network(structure, computer_training_6, computer_6, LR, max_ite, min_step, computer_class, regression=True)
  computer_results6 = forward_propagate(computer_6, computer_trained_network6, computer_class, regression=True)
 
  computer_trained_network7 = training_network(structure, computer_training_7, computer_7, LR, max_ite, min_step, computer_class, regression=True)
  computer_results7 = forward_propagate(computer_7, computer_trained_network7, computer_class, regression=True)
 
  computer_trained_network8 = training_network(structure, computer_training_8, computer_8, LR, max_ite, min_step, computer_class, regression=True)
  computer_results8 = forward_propagate(computer_8, computer_trained_network8, computer_class, regression=True)
 
  computer_trained_network9 = training_network(structure, computer_training_9, computer_9, LR, max_ite, min_step, computer_class, regression=True)
  computer_results9 = forward_propagate(computer_9, computer_trained_network9, computer_class, regression=True)
 
  computer_trained_network10 = training_network(structure, computer_training_10, computer_10, LR, max_ite, min_step, computer_class, regression=True)
  computer_results10 = forward_propagate(computer_10, computer_trained_network10, computer_class, regression=True)
 
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  print("results for test one")
  print(Mean_Squared_Error(computer_results1))
  print("results for test two")
  print(Mean_Squared_Error(computer_results2))
  print("results for test three")
  print(Mean_Squared_Error(computer_results3))
  print("results for test four")
  print(Mean_Squared_Error(computer_results4))
  print("results for test five")
  print(Mean_Squared_Error(computer_results5))
  print("results for test six")
  print(Mean_Squared_Error(computer_results6))
  print("results for test seven")
  print(Mean_Squared_Error(computer_results7))
  print("results for test eight")
  print(Mean_Squared_Error(computer_results8))
  print("results for test nine")
  print(Mean_Squared_Error(computer_results9))
  print("results for test ten")
  print(Mean_Squared_Error(computer_results10))
  "------------------------------------------------------------------------"

#For the video sample output for 0, 1 and 2 hidden layers
#Classification 
glass_trained_network1 = training_network([0], glass_training_1, glass_1, .05, 10, .01, glass_class, regression = False)
glass_results1 = forward_propagate(glass_1, glass_trained_network1, glass_class, regression=False)
print(glass_results1)
glass_trained_network2 = training_network([10], glass_training_2, glass_2, .05, 10, .01, glass_class, regression = False)
glass_results2 = forward_propagate(glass_2, glass_trained_network1, glass_class, regression=False)
print(glass_results2)
glass_trained_network2 = training_network([10, 10], glass_training_3, glass_3, .05, 10, .01, glass_class, regression = False)
glass_results3 = forward_propagate(glass_3, glass_trained_network1, glass_class, regression=False)
print(glass_results3)

#Regression
forest_trained_network1 = training_network([0], forest_training_1, forest_1, .01, 50, .1, forest_class, regression = True)
forest_results1 = forward_propagate(forest_1, forest_trained_network1, forest_class, regression=True)
print(forest_results1)
forest_trained_network2 = training_network([3], forest_training_2, forest_2, .01, 50, .1, forest_class, regression = True)
forest_results2 = forward_propagate(forest_2, forest_trained_network1, forest_class, regression=True)
print(forest_results2)
forest_trained_network2 = training_network([3, 3], forest_training_3, forest_3, .01, 50, .1, forest_class, regression = True)
forest_results3 = forward_propagate(forest_3, forest_trained_network1, forest_class, regression=True)
print(forest_results3)

classification_sample

#For the video
#classification
#Show a sample model for the smallest of each of your neural network types. This will consist of showing the weight matrices with the inputs/outputs of the layer labeled in some way
#Demonstrate and explain how an example is propagated through a two hidden layer network of your choice. Be sure to show the activations at each layer being calculated correctly.
classification_test = initialize_network([2, 2], classification_sample, classification_sample_class, classification_sample_class, regression = False)
print(classification_test)
classification_results = forward_propagate(classification_sample, classification_test, classification_sample_class, regression=False)
print(classification_results)

#For the video
#Regression 
#Show a sample model for the smallest of each of your neural network types. This will consist of showing the weight matrices with the inputs/outputs of the layer labeled in some way
regression_test = initialize_network([2, 2], regression_sample, regression_sample_class, regression_sample_class, regression = True)
print(regression_test)
regression_results = forward_propagate(regression_sample, regression_test, regression_sample_class, regression=True)
print(regression_results)

#For the video
#Demonstrate the gradient calculation at the output for one classification network and one regression network.
computer_trained_network_weight = training_network([3], computer_training_1, computer_1, .05, 10, .1, computer_class, regression=True)
print(computer_trained_network_weight)

#For the video
#Demonstrate the gradient calculation at the output for one classification network and one regression network.
soybean_trained_network_weights = training_network([3], soybean_training_1, soybean_1, .05, 10, .1, soybean_class, regression=False)
print(soybean_trained_network_weights)

#For the video
#Demonstrate the weight updates occurring on a two-layer network for each of the layers for one classification network and one regression network.
computer_trained_network_weight = training_network([3, 3], computer_training_1, computer_1, .05, 10, .1, computer_class, regression=True)
print(computer_trained_network_weight)

#For the video
#Demonstrate the weight updates occurring on a two-layer network for each of the layers for one classification network and one regression network.

soybean_trained_network_weights = training_network([3, 3], soybean_training_1, soybean_1, .05, 10, .1, soybean_class, regression=False)
print(soybean_trained_network_weights)

#For the video
#Show the average performance over the ten folds for one of the data sets for each of the types of networks
#Regression sample
hyperparameter_tune_computer([3], .05, 10, .1)

#FOR the video
#Show the average performance over the ten folds for one of the data sets for each of the types of networks
#Classification on soybean
hyperparameter_tune_soybean([10], .05, 10, .01)

hyperparameter_tune_forest([5], .05, 10, .01)

glass_dataset

def hyperparameter_tune_breast(structure, LR, max_ite, min_step):
  "------------------------------------------------------------------------"
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  breast_trained_network1 = training_network(structure, breast_training_1, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results1 = forward_propagate(breast_HP, breast_trained_network1, breast_class, regression=False)
 
  breast_trained_network2 = training_network(structure, breast_training_2, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results2 = forward_propagate(breast_HP, breast_trained_network2, breast_class, regression=False)
 
  breast_trained_network3 = training_network(structure, breast_training_3, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results3 = forward_propagate(breast_HP, breast_trained_network3, breast_class, regression=False)
 
  breast_trained_network4 = training_network(structure, breast_training_4, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results4 = forward_propagate(breast_HP, breast_trained_network4, breast_class, regression=False)
 
  breast_trained_network5 = training_network(structure, breast_training_5, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results5 = forward_propagate(breast_HP, breast_trained_network5, breast_class, regression=False)
 
  breast_trained_network6 = training_network(structure, breast_training_6, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results6 = forward_propagate(breast_HP, breast_trained_network6, breast_class, regression=False)
 
  breast_trained_network7 = training_network(structure, breast_training_7, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results7 = forward_propagate(breast_HP, breast_trained_network7, breast_class, regression=False)
 
  breast_trained_network8 = training_network(structure, breast_training_8, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results8 = forward_propagate(breast_HP, breast_trained_network8, breast_class, regression=False)
 
  breast_trained_network9 = training_network(structure, breast_training_9, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results9 = forward_propagate(breast_HP, breast_trained_network9, breast_class, regression=False)
 
  breast_trained_network10 = training_network(structure, breast_training_10, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results10 = forward_propagate(breast_HP, breast_trained_network10, breast_class, regression=False)
 
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  print("results for test one")
  print(zero_loss_func(breast_results1[0], breast_results1[1]))
  print("results for test two")
  print(zero_loss_func(breast_results2[0], breast_results2[1]))
  print("results for test three")
  print(zero_loss_func(breast_results3[0], breast_results3[1]))
  print("results for test four")
  print(zero_loss_func(breast_results4[0], breast_results4[1]))
  print("results for test five")
  print(zero_loss_func(breast_results5[0], breast_results5[1]))
  print("results for test six")
  print(zero_loss_func(breast_results6[0], breast_results6[1]))
  print("results for test seven")
  print(zero_loss_func(breast_results7[0], breast_results7[1]))
  print("results for test eight")
  print(zero_loss_func(breast_results8[0], breast_results8[1]))
  print("results for test nine")
  print(zero_loss_func(breast_results9[0], breast_results9[1]))
  print("results for test ten")
  print(zero_loss_func(breast_results10[0], breast_results10[1]))
  "------------------------------------------------------------------------"



#structure, LR, max_ite, min_step

hyperparameter_tune([1, 1], .1, 10, .1)

#outputs = forward_propagate(regression_sample, regression_sample_network, regression_sample_class, regression=True)
#this is to get the outputs for all instances in the dataframe
soybean_trained_network = training_network([2], soybean_training_1, soybean_1, 0.1, 3, 100, soybean_class, regression=False)
#this is to train the network

soybean_results = forward_propagate(soybean_1, soybean_trained_network, soybean_class, regression=False)

print(soybean_results)
print(zero_loss_func(soybean_results[0], soybean_results[1]))

print(soybean_dataset)

#(structure, dftrain, dftest, class_name, regression=True):
# classification_sample_class classification_sample
classification_test = initialize_network([2], classification_sample, classification_sample_class, classification_sample_class, regression = False)
print(classification_test)
soybean_results = forward_propagate(classification_sample, classification_test, classification_sample_class, regression=False)
print(soybean_results)

computer_network = training_network([4,4,4], computer_HP, computer_1, 0.1, 10, 0.01, computer_class, regression=True)

glass_dataset

soybean_class[4]

soybean_class[3]

print(soybean_class)