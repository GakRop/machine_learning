{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1WMBvNc9roK"
      },
      "outputs": [],
      "source": [
        "# Project 1\n",
        "#Overall for each part (coding, writing, and video segements) were equally distributed between the two of us Steven Ohms and Gak Roppongi\n",
        "#A majority of the time we were working together in person, and constantly looked over/edited each others code\n",
        "#Note whenever a cell says one of our names, we assume a majority of the work in the cell was done by that individual, but was still split between the two of us\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRwmODxJJQqD"
      },
      "outputs": [],
      "source": [
        "#Gak Roppongi\n",
        "#importing the data sets, and deaal with the missing values and one-hot encoding\n",
        "#this website below was referred to to import the data sets from UCI\n",
        "#https://towardsdatascience.com/how-to-use-data-files-from-uci-68b740b4719d\n",
        "\n",
        "breast_cancer_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', \n",
        "                                    names = [\"radius\", \"texture\", \"perimeter\", \"area\", \"smoothness\", \"compactness\", \"concavity\", \"concave points\", \n",
        "                                             \"symmetry\", \"fractal dimension\", \"class\"])\n",
        "#\"symmetry\" may be label-encoded, but I'm not sure \n",
        "#the sum of number of instances of 1 ... 7 in \"class\" attribute is 699 = number of all instances\n",
        "#therefore, \"class\" should be the categorical attribute\n",
        "#no categorical value is missed\n",
        "#There are 16 instances in Groups 1 to 6 that contain a single missing (i.e., unavailable) attribute value, now denoted by \"?\".  \n",
        "#699 * 11\n",
        "\n",
        "glass_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data', \n",
        "                            names = [\"id\", \"RI\", \"Na\", \"Mg\", \"Al\", \"Si\", \"K\", \"Ca\", \"Ba\",  \"Fe\", \"type of glass\"])\n",
        "#the \"type of glass\" is label-encoded\n",
        "#everything else is numerical attribute\n",
        "#Number of Attributes: 10 (including an Id#) plus the class attribute -- all attributes are continuously valued\n",
        "#the last attribute, \"class\", has label-encoded for 7 categories from 1~7\n",
        "#no missing attributes values\n",
        "#214 * 11 \n",
        "\n",
        "iris_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
        "                           names = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"class\"])\n",
        "#\"class\" instances are string data type\n",
        "#attributes 1~4 are numerical\n",
        "#attribute 5 is categorical\n",
        "#no missing attribute values\n",
        "#149 * 5\n",
        "\n",
        "soybean_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data', \n",
        "                              names = [\"date\", \"plant-stand\", \"precip\", \"temp\", \"hail\", \"crop-hist\", \"area-damaged\", \n",
        "                                       \"severity\", \"seed-tmt\", \"germination\", \"plant-growth\", \"leaves\", \"leafspots-halo\", \n",
        "                                       \"leafspots-marg\", \"leadspot-size\", \"leaf-shread\", \"lead-malf\", \"lead-mild\", \"stem\", \n",
        "                                       \"lodging\", \"stem-cankers\", \"fruiting-bodies\", \"external-decay\", \"mycelium\", \"int-discolor\", \n",
        "                                       \"sclerotia\", \"fruit-pods\", \"fruit-spots\", \"seed\", \"mold-growth\", \"seed-discolor\", \"seed-size\", \"shriveling\", \"roots\"])\n",
        "#many of the attributes are string data type and label-encoded\n",
        "#no missing attribute values\n",
        "#all values have been normalized\n",
        "#\"roots\" is the categorical value\n",
        "#46 * 36\n",
        "\n",
        "vote_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data', \n",
        "                           names = [\"class name\", \"handicapped-infants\", \"water-project-cost-sharing\", \"adoption-of-the-budget-resolution\", \n",
        "                                    \"physician-fee-freeze\", \"el-salvador-aid\", \"religious-groups-in-schools\", \"anti-satellite-test-ban\", \n",
        "                                    \"aid-to-nicaraguan-contras\", \"mx-missile\", \"immigration\", \"synfuels-corporation-cutback\", \n",
        "                                    \"education-spending\", \"superfund-right-to-sue\", \"crime\", \"duty-free-exports\", \"export-administration-act-south-africa\"])\n",
        "#most attributes are yes or no, and others are string. Any instance with missing value should be deleted or treated with forward/backward filling.\n",
        "#the first attribute shows whether the instance is by a republicant or a democrat\n",
        "#all other attributes are either yes or no\n",
        "#therefore, all attributes have to be one-hot-encoded\n",
        "#missing attributes are denoted by \"?\"\n",
        "#434 * 17\n",
        "\n",
        "breast_class = np.array([2, 4])\n",
        "glass_class = np.array([1, 2, 3, 5, 6, 7])\n",
        "iris_class = np.array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])\n",
        "soybean_class = np.array(['D1', 'D2', 'D3', 'D4'])\n",
        "vote_class = np.array([\"republican\", \"democrat\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "soybean_dataset = soybean_dataset.reset_index(drop = True)"
      ],
      "metadata": {
        "id": "-JvbzdLtusfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "breast_cancer_dataset.sort_values(by=[\"class\"], inplace=True)\n",
        "glass_dataset.sort_values(by=[\"type of glass\"], inplace=True)\n",
        "iris_dataset.sort_values(by=[\"class\"], inplace=True)\n",
        "soybean_dataset.sort_values(by=[\"roots\"], inplace=True)\n",
        "vote_dataset.sort_values(by=[\"class name\"], inplace=True)"
      ],
      "metadata": {
        "id": "cRE9lrGdg02N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JeclYZc32cf"
      },
      "outputs": [],
      "source": [
        "#Gak Roppongi\n",
        "#we will replace all the \"?\" value with np.nan value so that we can handle it easier\n",
        "\n",
        "breast_cancer_dataset = breast_cancer_dataset.replace([\"?\"], np.nan)\n",
        "glass_dataset = glass_dataset.replace([\"?\"], np.nan)\n",
        "iris_dataset = iris_dataset.replace([\"?\"], np.nan)\n",
        "soybean_dataset = soybean_dataset.replace([\"?\"], np.nan)\n",
        "\n",
        "vote_dataset[\"handicapped-infants\"] = vote_dataset[\"handicapped-infants\"].replace([\"n\"], [\"handicapped-n\"])\n",
        "vote_dataset[\"handicapped-infants\"] = vote_dataset[\"handicapped-infants\"].replace([\"y\"], [\"handicapped-y\"])\n",
        "vote_dataset[\"handicapped-infants\"] = vote_dataset[\"handicapped-infants\"].replace([\"?\"], [\"handicapped-absent\"])\n",
        "vote_dataset[\"water-project-cost-sharing\"] = vote_dataset[\"water-project-cost-sharing\"].replace([\"n\"], [\"water-n\"])\n",
        "vote_dataset[\"water-project-cost-sharing\"] = vote_dataset[\"water-project-cost-sharing\"].replace([\"y\"], [\"water-y\"])\n",
        "vote_dataset[\"water-project-cost-sharing\"] = vote_dataset[\"water-project-cost-sharing\"].replace([\"?\"], [\"water-absent\"])\n",
        "vote_dataset[\"adoption-of-the-budget-resolution\"] = vote_dataset[\"adoption-of-the-budget-resolution\"].replace([\"n\"], [\"adoption-n\"])\n",
        "vote_dataset[\"adoption-of-the-budget-resolution\"] = vote_dataset[\"adoption-of-the-budget-resolution\"].replace([\"y\"], [\"adoption-y\"])\n",
        "vote_dataset[\"adoption-of-the-budget-resolution\"] = vote_dataset[\"adoption-of-the-budget-resolution\"].replace([\"?\"], [\"adoption-absent\"])\n",
        "vote_dataset[\"physician-fee-freeze\"] = vote_dataset[\"physician-fee-freeze\"].replace([\"n\"], [\"physician-n\"])\n",
        "vote_dataset[\"physician-fee-freeze\"] = vote_dataset[\"physician-fee-freeze\"].replace([\"y\"], [\"physician-y\"])\n",
        "vote_dataset[\"physician-fee-freeze\"] = vote_dataset[\"physician-fee-freeze\"].replace([\"?\"], [\"physician-absent\"])\n",
        "vote_dataset[\"el-salvador-aid\"] = vote_dataset[\"el-salvador-aid\"].replace([\"n\"], [\"el-n\"])\n",
        "vote_dataset[\"el-salvador-aid\"] = vote_dataset[\"el-salvador-aid\"].replace([\"y\"], [\"el-y\"])\n",
        "vote_dataset[\"el-salvador-aid\"] = vote_dataset[\"el-salvador-aid\"].replace([\"?\"], [\"el-absent\"])\n",
        "vote_dataset[\"religious-groups-in-schools\"] = vote_dataset[\"religious-groups-in-schools\"].replace([\"n\"], [\"religious-n\"])\n",
        "vote_dataset[\"religious-groups-in-schools\"] = vote_dataset[\"religious-groups-in-schools\"].replace([\"y\"], [\"religious-y\"])\n",
        "vote_dataset[\"religious-groups-in-schools\"] = vote_dataset[\"religious-groups-in-schools\"].replace([\"?\"], [\"religious-absent\"])\n",
        "vote_dataset[\"anti-satellite-test-ban\"] = vote_dataset[\"anti-satellite-test-ban\"].replace([\"n\"], [\"anti-n\"])\n",
        "vote_dataset[\"anti-satellite-test-ban\"] = vote_dataset[\"anti-satellite-test-ban\"].replace([\"y\"], [\"anti-y\"])\n",
        "vote_dataset[\"anti-satellite-test-ban\"] = vote_dataset[\"anti-satellite-test-ban\"].replace([\"?\"], [\"anti-absent\"])\n",
        "vote_dataset[\"aid-to-nicaraguan-contras\"] = vote_dataset[\"aid-to-nicaraguan-contras\"].replace([\"n\"], [\"aid-n\"])\n",
        "vote_dataset[\"aid-to-nicaraguan-contras\"] = vote_dataset[\"aid-to-nicaraguan-contras\"].replace([\"y\"], [\"aid-y\"])\n",
        "vote_dataset[\"aid-to-nicaraguan-contras\"] = vote_dataset[\"aid-to-nicaraguan-contras\"].replace([\"?\"], [\"aid-absent\"])\n",
        "vote_dataset[\"mx-missile\"] = vote_dataset[\"mx-missile\"].replace([\"n\"], [\"mx-n\"])\n",
        "vote_dataset[\"mx-missile\"] = vote_dataset[\"mx-missile\"].replace([\"y\"], [\"mx-y\"])\n",
        "vote_dataset[\"mx-missile\"] = vote_dataset[\"mx-missile\"].replace([\"?\"], [\"mx-absent\"])\n",
        "vote_dataset[\"immigration\"] = vote_dataset[\"immigration\"].replace([\"n\"], [\"immigration-n\"])\n",
        "vote_dataset[\"immigration\"] = vote_dataset[\"immigration\"].replace([\"y\"], [\"immigration-y\"])\n",
        "vote_dataset[\"immigration\"] = vote_dataset[\"immigration\"].replace([\"?\"], [\"immigration-absent\"])\n",
        "vote_dataset[\"synfuels-corporation-cutback\"] = vote_dataset[\"synfuels-corporation-cutback\"].replace([\"n\"], [\"synfuels-n\"])\n",
        "vote_dataset[\"synfuels-corporation-cutback\"] = vote_dataset[\"synfuels-corporation-cutback\"].replace([\"y\"], [\"synfuels-y\"])\n",
        "vote_dataset[\"synfuels-corporation-cutback\"] = vote_dataset[\"synfuels-corporation-cutback\"].replace([\"?\"], [\"synfuels-absent\"])\n",
        "vote_dataset[\"education-spending\"] = vote_dataset[\"education-spending\"].replace([\"n\"], [\"education-n\"])\n",
        "vote_dataset[\"education-spending\"] = vote_dataset[\"education-spending\"].replace([\"y\"], [\"education-y\"])\n",
        "vote_dataset[\"education-spending\"] = vote_dataset[\"education-spending\"].replace([\"?\"], [\"education-absent\"])\n",
        "vote_dataset[\"superfund-right-to-sue\"] = vote_dataset[\"superfund-right-to-sue\"].replace([\"n\"], [\"superfund-n\"])\n",
        "vote_dataset[\"superfund-right-to-sue\"] = vote_dataset[\"superfund-right-to-sue\"].replace([\"y\"], [\"superfund-y\"])\n",
        "vote_dataset[\"superfund-right-to-sue\"] = vote_dataset[\"superfund-right-to-sue\"].replace([\"?\"], [\"superfund-absent\"])\n",
        "vote_dataset[\"crime\"] = vote_dataset[\"crime\"].replace([\"n\"], [\"crime-n\"])\n",
        "vote_dataset[\"crime\"] = vote_dataset[\"crime\"].replace([\"y\"], [\"crime-y\"])\n",
        "vote_dataset[\"crime\"] = vote_dataset[\"crime\"].replace([\"?\"], [\"crime-absent\"])\n",
        "vote_dataset[\"duty-free-exports\"] = vote_dataset[\"duty-free-exports\"].replace([\"n\"], [\"duty-n\"])\n",
        "vote_dataset[\"duty-free-exports\"] = vote_dataset[\"duty-free-exports\"].replace([\"y\"], [\"duty-y\"])\n",
        "vote_dataset[\"duty-free-exports\"] = vote_dataset[\"duty-free-exports\"].replace([\"?\"], [\"duty-absent\"])\n",
        "vote_dataset[\"export-administration-act-south-africa\"] = vote_dataset[\"export-administration-act-south-africa\"].replace([\"n\"], [\"export-n\"])\n",
        "vote_dataset[\"export-administration-act-south-africa\"] = vote_dataset[\"export-administration-act-south-africa\"].replace([\"y\"], [\"export-y\"])\n",
        "vote_dataset[\"export-administration-act-south-africa\"] = vote_dataset[\"export-administration-act-south-africa\"].replace([\"?\"], [\"export-absent\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "#this section of the code will turn all the vallues in the dataframes into numeric values\n",
        "#all values are saved as str in the first place\n",
        "#we will turn them into numeric\n",
        "\n",
        "breast_cancer_dataset['concavity'] = breast_cancer_dataset['concavity'].astype(float)\n",
        "#vote_dataset = vote_dataset.applymap(str)"
      ],
      "metadata": {
        "id": "sfPdHG7GWq84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32zbd1LR3Ub9"
      },
      "outputs": [],
      "source": [
        "#Gak Roppongi\n",
        "#glass, iris, and  soybean don't have the missing value\n",
        "\n",
        "#this function does mean-filling of the dataset for the missing value\n",
        "#df is the dataframe we manipulate\n",
        "#attribute is the attribute in the dataframe we are manipulating\n",
        "\n",
        "def mean_filling(df, attribute):\n",
        "  df[attribute].replace(np.nan, df[attribute].mean())\n",
        "  return df\n",
        "\n",
        "breast_cancer_dataset = mean_filling(breast_cancer_dataset, \"concavity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPCwDpQZs45E"
      },
      "outputs": [],
      "source": [
        "#Gak Roppongi\n",
        "#this function implements the one-hot encoding\n",
        "#df is the dataset we manipulate\n",
        "#attribute is the attribute that we manipulate in str\n",
        "#returns the one-hot encoded dataframe\n",
        "\n",
        "def onehot(df, attribute):\n",
        "  # Get one hot encoding of columns B\n",
        "  one_hot = pd.get_dummies(df[attribute])\n",
        "  # Drop column B as it is now encoded\n",
        "  df = df.drop(attribute,axis = 1)\n",
        "  # Join the encoded df\n",
        "  df = df.join(one_hot)\n",
        "  return df\n",
        "\n",
        "breast_cancer_dataset = onehot(breast_cancer_dataset, \"class\")\n",
        "\n",
        "glass_dataset = onehot(glass_dataset, \"type of glass\")\n",
        "\n",
        "iris_dataset = onehot(iris_dataset, \"class\")\n",
        "\n",
        "soybean_dataset = onehot(soybean_dataset, \"roots\")\n",
        "\n",
        "vote_dataset = onehot(vote_dataset, \"class name\")\n",
        "vote_dataset = onehot(vote_dataset, \"handicapped-infants\")\n",
        "vote_dataset = onehot(vote_dataset, \"water-project-cost-sharing\")\n",
        "vote_dataset = onehot(vote_dataset, \"adoption-of-the-budget-resolution\")\n",
        "vote_dataset = onehot(vote_dataset, \"physician-fee-freeze\")\n",
        "vote_dataset = onehot(vote_dataset, \"el-salvador-aid\")\n",
        "vote_dataset = onehot(vote_dataset, \"religious-groups-in-schools\")\n",
        "vote_dataset = onehot(vote_dataset, \"anti-satellite-test-ban\")\n",
        "vote_dataset = onehot(vote_dataset, \"aid-to-nicaraguan-contras\")\n",
        "vote_dataset = onehot(vote_dataset, \"mx-missile\")\n",
        "vote_dataset = onehot(vote_dataset, \"immigration\")\n",
        "vote_dataset = onehot(vote_dataset, \"synfuels-corporation-cutback\")\n",
        "vote_dataset = onehot(vote_dataset, \"education-spending\")\n",
        "vote_dataset = onehot(vote_dataset, \"superfund-right-to-sue\")\n",
        "vote_dataset = onehot(vote_dataset, \"crime\")\n",
        "vote_dataset = onehot(vote_dataset, \"duty-free-exports\")\n",
        "vote_dataset = onehot(vote_dataset, \"export-administration-act-south-africa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD46tDy-oFeN"
      },
      "outputs": [],
      "source": [
        "#Steven Ohms\n",
        "#this section of the code will normalize the dataset\n",
        "#the function will take df and attribute\n",
        "#df is the dataframe we are manipulating\n",
        "#attribute is the attribute we are normalizing\n",
        "#this is the z-score/standard score normalization\n",
        "\n",
        "def normalize(df, attribute):\n",
        "  \"\"\"\n",
        "  mean = df[attribute].mean()\n",
        "\n",
        "  total = 0\n",
        "  for i in df[attribute]:\n",
        "    total = total + (i - mean)**2\n",
        "\n",
        "  variance = total/len(df[attribute])\n",
        "  std_dev = math.sqrt(variance)\n",
        "\n",
        "  df[attribute] = (df[attribute] - mean)/std_dev\n",
        "  \"\"\"\n",
        "  df = (df-df.min())/(df.max() - df.min())\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "breast_cancer_dataset = normalize(breast_cancer_dataset, \"radius\")\n",
        "breast_cancer_dataset = normalize(breast_cancer_dataset, \"texture\")\n",
        "breast_cancer_dataset = normalize(breast_cancer_dataset, \"perimeter\")\n",
        "breast_cancer_dataset = normalize(breast_cancer_dataset, \"area\")\n",
        "breast_cancer_dataset = normalize(breast_cancer_dataset, \"smoothness\")\n",
        "breast_cancer_dataset = normalize(breast_cancer_dataset, \"compactness\")\n",
        "breast_cancer_dataset = normalize(breast_cancer_dataset, \"concavity\")\n",
        "breast_cancer_dataset = normalize(breast_cancer_dataset, \"concave points\")\n",
        "breast_cancer_dataset = normalize(breast_cancer_dataset, \"symmetry\")\n",
        "breast_cancer_dataset = normalize(breast_cancer_dataset, \"fractal dimension\")\n",
        "\n",
        "glass_dataset = normalize(glass_dataset, \"RI\")\n",
        "glass_dataset = normalize(glass_dataset, \"Na\")\n",
        "glass_dataset = normalize(glass_dataset, \"Mg\")\n",
        "glass_dataset = normalize(glass_dataset, \"Al\")\n",
        "glass_dataset = normalize(glass_dataset, \"Si\")\n",
        "glass_dataset = normalize(glass_dataset, \"K\")\n",
        "glass_dataset = normalize(glass_dataset, \"Ca\")\n",
        "glass_dataset = normalize(glass_dataset, \"Ba\")\n",
        "glass_dataset = normalize(glass_dataset, \"Fe\")\n",
        "\n",
        "iris_dataset = normalize(iris_dataset, \"sepal length\")\n",
        "iris_dataset = normalize(iris_dataset, \"sepal width\")\n",
        "iris_dataset = normalize(iris_dataset, \"petal length\")\n",
        "iris_dataset = normalize(iris_dataset, \"petal width\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "#this code descretizes the previously normalized data to make it so our algorithm can read the data\n",
        "#the first hyperparameter is we have 5 categories for the range of 0.2\n",
        "#we will tune this hyperparameter for 2 categories(0.5), 4 categories(0.25), 5 categories(0.2), and 8 categories(0.125).\n",
        "'''\n",
        "def discretize(df, att_name):\n",
        "  \"\"\"\n",
        "  #2 categories for the range of 0.5\n",
        "  df.loc[df[att_name].between(.5, 1, 'right'), att_name] = 2\n",
        "  df.loc[df[att_name].between(0, .5, 'both'), att_name] = 1\n",
        "  \"\"\"\n",
        " \n",
        "  \"\"\"\n",
        "  #4 categories for the range of 0.25\n",
        "  f.loc[df[att_name].between(.75, 1, 'right'), att_name] = 4\n",
        "  df.loc[df[att_name].between(.5, .75, 'right'), att_name] = 3\n",
        "  df.loc[df[att_name].between(.25, .5, 'right'), att_name] = 2\n",
        "  df.loc[df[att_name].between(0, .25, 'both'), att_name] = 1\n",
        "  \"\"\"\n",
        " \n",
        "  #5 categories for the range of 0.2\n",
        "  df.loc[df[att_name].between(.8, 1, 'right'), att_name] = 5\n",
        "  df.loc[df[att_name].between(.6, .8, 'right'), att_name] = 4\n",
        "  df.loc[df[att_name].between(.4, .6, 'right'), att_name] = 3\n",
        "  df.loc[df[att_name].between(.2, .4, 'right'), att_name] = 2\n",
        "  df.loc[df[att_name].between(0, .2, 'both'), att_name] = 1\n",
        " \n",
        "  \"\"\"\n",
        "  #8 categories for the range of 0.125\n",
        "  df.loc[df[att_name].between(.875, 1, 'right'), att_name] = 8\n",
        "  df.loc[df[att_name].between(.75, .875, 'right'), att_name] = 7\n",
        "  df.loc[df[att_name].between(.625, .75, 'right'), att_name] = 6\n",
        "  df.loc[df[att_name].between(.5, .625, 'right'), att_name] = 5\n",
        "  df.loc[df[att_name].between(.375, .5, 'right'), att_name] = 4\n",
        "  df.loc[df[att_name].between(.25, .375, 'right'), att_name] = 3\n",
        "  df.loc[df[att_name].between(.125, .25, 'right'), att_name] = 2\n",
        "  df.loc[df[att_name].between(0, .125, 'both'), att_name] = 1\n",
        "  \"\"\"\n",
        "  \n",
        " \n",
        "  return df\n",
        "  '''\n",
        "\n",
        "def discretize2(df, att_name):\n",
        "    \n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[50])[0], 1, 'right'), att_name] = 2\n",
        "    df.loc[df[att_name].between(0, np.percentile(df[att_name],[50])[0], 'both'), att_name] = 1\n",
        " \n",
        " \n",
        "def discretize5(df, att_name):\n",
        "    \n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[80])[0], 1, 'right'), att_name] = 5\n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[60])[0], np.percentile(df[att_name],[80])[0], 'both'), att_name] = 4 \n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[40])[0], np.percentile(df[att_name],[60])[0], 'both'), att_name] = 3\n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[20])[0], np.percentile(df[att_name],[40])[0], 'both'), att_name] = 2\n",
        "    df.loc[df[att_name].between(0, np.percentile(df[att_name],[20])[0], 'both'), att_name] = 1  \n",
        " \n",
        "def discretize10(df, att_name):\n",
        "    \n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[90])[0], 1, 'right'), att_name] = 10\n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[80])[0], np.percentile(df[att_name],[90])[0], 'both'), att_name] = 9 \n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[70])[0], np.percentile(df[att_name],[80])[0], 'both'), att_name] = 8\n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[60])[0], np.percentile(df[att_name],[70])[0], 'both'), att_name] = 7\n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[50])[0], np.percentile(df[att_name],[60])[0], 'both'), att_name] = 6 \n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[40])[0], np.percentile(df[att_name],[50])[0], 'both'), att_name] = 5\n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[30])[0], np.percentile(df[att_name],[40])[0], 'both'), att_name] = 4\n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[20])[0], np.percentile(df[att_name],[30])[0], 'both'), att_name] = 3 \n",
        "    df.loc[df[att_name].between(np.percentile(df[att_name],[10])[0], np.percentile(df[att_name],[20])[0], 'both'), att_name] = 2\n",
        "    df.loc[df[att_name].between(0, np.percentile(df[att_name],[10])[0], 'both'), att_name] = 1   \n",
        " \n",
        " \n",
        "def discretize20(df, att_name):\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[95])[0], 1, 'right'), att_name] = 20\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[90])[0], np.percentile(df[att_name],[95])[0], 'both'), att_name] = 19 \n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[85])[0], np.percentile(df[att_name],[90])[0], 'both'), att_name] = 18\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[80])[0], np.percentile(df[att_name],[85])[0], 'both'), att_name] = 17\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[75])[0], np.percentile(df[att_name],[80])[0], 'both'), att_name] = 16 \n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[70])[0], np.percentile(df[att_name],[75])[0], 'both'), att_name] = 15\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[65])[0], np.percentile(df[att_name],[70])[0], 'both'), att_name] = 14\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[60])[0], np.percentile(df[att_name],[65])[0], 'both'), att_name] = 13 \n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[55])[0], np.percentile(df[att_name],[60])[0], 'both'), att_name] = 12\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[50])[0], np.percentile(df[att_name],[55])[0], 'both'), att_name] = 11 \n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[45])[0], np.percentile(df[att_name],[50])[0], 'both'), att_name] = 10\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[40])[0], np.percentile(df[att_name],[45])[0], 'both'), att_name] = 9\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[35])[0], np.percentile(df[att_name],[40])[0], 'both'), att_name] = 8 \n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[30])[0], np.percentile(df[att_name],[35])[0], 'both'), att_name] = 7\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[25])[0], np.percentile(df[att_name],[30])[0], 'both'), att_name] = 6\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[20])[0], np.percentile(df[att_name],[25])[0], 'both'), att_name] = 5 \n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[15])[0], np.percentile(df[att_name],[20])[0], 'both'), att_name] = 4\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[10])[0], np.percentile(df[att_name],[15])[0], 'both'), att_name] = 3\n",
        "  df.loc[df[att_name].between(np.percentile(df[att_name],[5])[0], np.percentile(df[att_name],[10])[0], 'both'), att_name] = 2 \n",
        "  df.loc[df[att_name].between(0, np.percentile(df[att_name],[5])[0], 'both'), att_name] = 1\n",
        " \n",
        "discretize5(glass_dataset, \"id\")\n",
        "discretize10(glass_dataset, \"RI\")\n",
        "discretize2(glass_dataset, \"Na\")\n",
        "discretize2(glass_dataset, \"Mg\")\n",
        "discretize10(glass_dataset, \"Al\")\n",
        "discretize10(glass_dataset, \"Si\")\n",
        "discretize10(glass_dataset, \"K\")\n",
        "discretize10(glass_dataset, \"Ca\")\n",
        "discretize10(glass_dataset, \"Ba\")\n",
        "discretize2(glass_dataset, \"Fe\")\n",
        " \n",
        "discretize5(iris_dataset, \"sepal length\")\n",
        "discretize5(iris_dataset, \"sepal width\")\n",
        "discretize2(iris_dataset, \"petal length\")\n",
        "discretize5(iris_dataset, \"petal width\")\n"
      ],
      "metadata": {
        "id": "ohErD7TAIjRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "\"\"\"\n",
        "shuffling requires 10% of attributes values to be shuffled from each dataset\n",
        "randomization will pick up the 10% of the attribute \n",
        "and shuffle the values inside the attribute\n",
        "so that it will be 10% shuffled overall\n",
        "\"\"\"\n",
        "\n",
        "def shuffle(df, class_name):\n",
        "  pickuped = []\n",
        "  number_of_shuffled_attributes = len(df.columns) - len(class_name)\n",
        "  number_of_attributes_to_pick_up = math.ceil(number_of_shuffled_attributes)\n",
        "\n",
        "  for i in range(number_of_attributes_to_pick_up):\n",
        "    index_of_attribute_to_be_shuffled = random.randint(0, number_of_attributes_to_pick_up) #<- the attribute to be shuffled is picked here\n",
        "\n",
        "    while index_of_attribute_to_be_shuffled in pickuped:\n",
        "      index_of_attribute_to_be_shuffled = random.randint(0, number_of_attributes_to_pick_up)\n",
        "\n",
        "    df.columns[index_of_attribute_to_be_shuffled]\n",
        "    shuffled_column = df[df.columns[index_of_attribute_to_be_shuffled]].sample(frac=1, random_state=1).reset_index()\n",
        "    df[df.columns[index_of_attribute_to_be_shuffled]] = shuffled_column[df.columns[index_of_attribute_to_be_shuffled]]\n",
        "    pickuped.append(index_of_attribute_to_be_shuffled)\n",
        "    \n",
        "  return df"
      ],
      "metadata": {
        "id": "Z2WGwlCnz_Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "#merges all the portions together to create a testing set\n",
        " \n",
        "def train_merge(one, two, three, four, five, six, seven, eight, nine):\n",
        "  final_train = pd.merge(one, two, how=\"outer\")\n",
        "  final_train = pd.merge(final_train, three, how=\"outer\")\n",
        "  final_train = pd.merge(final_train, four, how=\"outer\")\n",
        "  final_train = pd.merge(final_train, five, how=\"outer\")\n",
        "  final_train = pd.merge(final_train, six, how=\"outer\")\n",
        "  final_train = pd.merge(final_train, seven, how=\"outer\")\n",
        "  final_train = pd.merge(final_train, eight, how=\"outer\")\n",
        "  final_train = pd.merge(final_train, nine, how=\"outer\")\n",
        " \n",
        "  return final_train"
      ],
      "metadata": {
        "id": "v26k4ptnsibg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "#this section of the code will divide each original dataset into several sets\n",
        "#each training sets have equal numbers of instances from each class\n",
        "\n",
        "#breast_cancer_dataset has 458 from 2 and 241 from 4\n",
        "#glass_dataset has 70 from 1, 76 from 2, 17 from 3, 0 from 4, 13 from 5, 9 from 6, and 29 from 7\n",
        "#iris_dataset has 50 from Iris-setosa, 50 from Iris-versicolor, and 50 from Iris-virginica\n",
        "#soybean_dataset has 56 from D1, 55 from D2, 82 from D3, and 112 from D4\n",
        "#vote_dataset has 168 from republican, and 267 from democrat\n",
        "\n",
        "i1 = iris_dataset.iloc[:5]\n",
        "i2 = iris_dataset.iloc[5:10]\n",
        "i3 = iris_dataset.iloc[10:15]\n",
        "i4 = iris_dataset.iloc[15:20]\n",
        "i5 = iris_dataset.iloc[20:25]\n",
        "i6 = iris_dataset.iloc[25:30]\n",
        "i7 = iris_dataset.iloc[30:35]\n",
        "i8 = iris_dataset.iloc[35:40]\n",
        "i9 = iris_dataset.iloc[40:45]\n",
        "i10 = iris_dataset.iloc[45:50]\n",
        "\n",
        "i11 = iris_dataset.iloc[50:55]\n",
        "i12 = iris_dataset.iloc[55:60]\n",
        "i13 = iris_dataset.iloc[60:65]\n",
        "i14 = iris_dataset.iloc[65:70]\n",
        "i15 = iris_dataset.iloc[70:75]\n",
        "i16 = iris_dataset.iloc[75:80]\n",
        "i17 = iris_dataset.iloc[80:85]\n",
        "i18 = iris_dataset.iloc[85:90]\n",
        "i19 = iris_dataset.iloc[90:95]\n",
        "i20 = iris_dataset.iloc[95:100]\n",
        "\n",
        "i21 = iris_dataset.iloc[100:105]\n",
        "i22 = iris_dataset.iloc[105:110]\n",
        "i23 = iris_dataset.iloc[110:115]\n",
        "i24 = iris_dataset.iloc[115:120]\n",
        "i25 = iris_dataset.iloc[120:125]\n",
        "i26 = iris_dataset.iloc[125:130]\n",
        "i27 = iris_dataset.iloc[130:135]\n",
        "i28 = iris_dataset.iloc[135:140]\n",
        "i29 = iris_dataset.iloc[140:145]\n",
        "i30 = iris_dataset.iloc[145:150]\n",
        "\n",
        "iris_one = pd.merge(i1, i11, how = \"outer\")\n",
        "iris_one = pd.merge(iris_one, i21, how = \"outer\")\n",
        "\n",
        "iris_two = pd.merge(i2, i12, how = \"outer\")\n",
        "iris_two = pd.merge(iris_two, i22, how = \"outer\")\n",
        "\n",
        "iris_three = pd.merge(i3, i13, how = \"outer\")\n",
        "iris_three = pd.merge(iris_three, i23, how = \"outer\")\n",
        "\n",
        "iris_four = pd.merge(i4, i14, how = \"outer\")\n",
        "iris_four = pd.merge(iris_four, i24, how = \"outer\")\n",
        "\n",
        "iris_five = pd.merge(i5, i15, how = \"outer\")\n",
        "iris_five = pd.merge(iris_five, i25, how = \"outer\")\n",
        "\n",
        "iris_six = pd.merge(i6, i16, how = \"outer\")\n",
        "iris_six = pd.merge(iris_six, i26, how = \"outer\")\n",
        "\n",
        "iris_seven = pd.merge(i7, i17, how = \"outer\")\n",
        "iris_seven = pd.merge(iris_seven, i27, how = \"outer\")\n",
        "\n",
        "iris_eight = pd.merge(i8, i18, how = \"outer\")\n",
        "iris_eight = pd.merge(iris_eight, i28, how = \"outer\")\n",
        "\n",
        "iris_nine = pd.merge(i9, i19, how = \"outer\")\n",
        "iris_nine = pd.merge(iris_nine, i29, how = \"outer\")\n",
        "\n",
        "iris_ten = pd.merge(i10, i20, how = \"outer\")\n",
        "iris_ten = pd.merge(iris_ten, i30, how = \"outer\")\n",
        "\n",
        "#iris dataset has three sets of 50 for each class\n",
        "#we want three training sets\n",
        "#each training class gets divided into 17/17/16"
      ],
      "metadata": {
        "id": "engRh9CwKGlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "#soybean_dataset has 10, 10, 10, 17\n",
        "#we want 10 training sets\n",
        "s1 = soybean_dataset.iloc[:1]\n",
        "s2 = soybean_dataset.iloc[1:2]\n",
        "s3 = soybean_dataset.iloc[2:3]\n",
        "s4 = soybean_dataset.iloc[3:4]\n",
        "s5 = soybean_dataset.iloc[4:5]\n",
        "s6 = soybean_dataset.iloc[5:6]\n",
        "s7 = soybean_dataset.iloc[6:7]\n",
        "s8 = soybean_dataset.iloc[7:8]\n",
        "s9 = soybean_dataset.iloc[8:9]\n",
        "s10 = soybean_dataset.iloc[9:10]\n",
        " \n",
        "s11 = soybean_dataset.iloc[10:11]\n",
        "s12 = soybean_dataset.iloc[11:12]\n",
        "s13 = soybean_dataset.iloc[12:13]\n",
        "s14 = soybean_dataset.iloc[13:14]\n",
        "s15 = soybean_dataset.iloc[14:15]\n",
        "s16 = soybean_dataset.iloc[15:16]\n",
        "s17 = soybean_dataset.iloc[16:17]\n",
        "s18 = soybean_dataset.iloc[17:18]\n",
        "s19 = soybean_dataset.iloc[18:19]\n",
        "s20 = soybean_dataset.iloc[19:20]\n",
        " \n",
        "s21 = soybean_dataset.iloc[20:21]\n",
        "s22 = soybean_dataset.iloc[21:22]\n",
        "s23 = soybean_dataset.iloc[22:23]\n",
        "s24 = soybean_dataset.iloc[23:24]\n",
        "s25 = soybean_dataset.iloc[24:25]\n",
        "s26 = soybean_dataset.iloc[25:26]\n",
        "s27 = soybean_dataset.iloc[26:27]\n",
        "s28 = soybean_dataset.iloc[27:28]\n",
        "s29 = soybean_dataset.iloc[28:29]\n",
        "s30 = soybean_dataset.iloc[29:30]\n",
        " \n",
        "s31 = soybean_dataset.iloc[30:31]\n",
        "s32 = soybean_dataset.iloc[31:32]\n",
        "s33 = soybean_dataset.iloc[32:33]\n",
        "s34 = soybean_dataset.iloc[33:35]\n",
        "s35 = soybean_dataset.iloc[35:37]\n",
        "s36 = soybean_dataset.iloc[37:39]\n",
        "s37 = soybean_dataset.iloc[39:41]\n",
        "s38 = soybean_dataset.iloc[41:43]\n",
        "s39 = soybean_dataset.iloc[43:45]\n",
        "s40 = soybean_dataset.iloc[45:47]\n",
        "\n",
        "soybean_one = pd.merge(s1, s11, how = \"outer\")\n",
        "soybean_one = pd.merge(soybean_one, s21, how = \"outer\")\n",
        "soybean_one = pd.merge(soybean_one, s31, how = \"outer\")\n",
        "\n",
        "soybean_two = pd.merge(s2, s12, how = \"outer\")\n",
        "soybean_two = pd.merge(soybean_two, s22, how = \"outer\")\n",
        "soybean_two = pd.merge(soybean_two, s32, how = \"outer\")\n",
        "\n",
        "soybean_three = pd.merge(s3, s13, how = \"outer\")\n",
        "soybean_three = pd.merge(soybean_three, s23, how = \"outer\")\n",
        "soybean_three = pd.merge(soybean_three, s33, how = \"outer\")\n",
        "\n",
        "soybean_four = pd.merge(s4, s14, how = \"outer\")\n",
        "soybean_four = pd.merge(soybean_four, s24, how = \"outer\")\n",
        "soybean_four = pd.merge(soybean_four, s34, how = \"outer\")\n",
        "\n",
        "soybean_five = pd.merge(s5, s15, how = \"outer\")\n",
        "soybean_five = pd.merge(soybean_five, s25, how = \"outer\")\n",
        "soybean_five = pd.merge(soybean_five, s35, how = \"outer\")\n",
        "\n",
        "soybean_six = pd.merge(s6, s16, how = \"outer\")\n",
        "soybean_six = pd.merge(soybean_six, s26, how = \"outer\")\n",
        "soybean_six = pd.merge(soybean_six, s36, how = \"outer\")\n",
        "\n",
        "soybean_seven = pd.merge(s7, s17, how = \"outer\")\n",
        "soybean_seven = pd.merge(soybean_seven, s27, how = \"outer\")\n",
        "soybean_seven = pd.merge(soybean_seven, s37, how = \"outer\")\n",
        "\n",
        "soybean_eight = pd.merge(s8, s18, how = \"outer\")\n",
        "soybean_eight = pd.merge(soybean_eight, s28, how = \"outer\")\n",
        "soybean_eight = pd.merge(soybean_eight, s38, how = \"outer\")\n",
        "\n",
        "soybean_nine = pd.merge(s9, s19, how = \"outer\")\n",
        "soybean_nine = pd.merge(soybean_nine, s29, how = \"outer\")\n",
        "soybean_nine = pd.merge(soybean_nine, s39, how = \"outer\")\n",
        "\n",
        "soybean_ten = pd.merge(s10, s20, how = \"outer\")\n",
        "soybean_ten = pd.merge(soybean_ten, s30, how = \"outer\")\n",
        "soybean_ten = pd.merge(soybean_ten, s40, how = \"outer\")"
      ],
      "metadata": {
        "id": "cUvdqVg5xvl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "#vote dataset has two sets, democrat being 168 instances and republican being 267\n",
        "#we want three training sets \n",
        "#each training class will have 89 democrats, and 56 republicans\n",
        " \n",
        "v_1 = vote_dataset.iloc[:26]\n",
        "v_2 = vote_dataset.iloc[26:52]\n",
        "v_3 = vote_dataset.iloc[52:78]\n",
        "v_4 = vote_dataset.iloc[78:105]\n",
        "v_5 = vote_dataset.iloc[105:132]\n",
        "v_6 = vote_dataset.iloc[132:159]\n",
        "v_7 = vote_dataset.iloc[159:186]\n",
        "v_8 = vote_dataset.iloc[186:213]\n",
        "v_9 = vote_dataset.iloc[213:240]\n",
        "v_10 = vote_dataset.iloc[240:267]\n",
        "\n",
        "v_11 = vote_dataset.iloc[267:283]\n",
        "v_12 = vote_dataset.iloc[283:299]\n",
        "v_13 = vote_dataset.iloc[299:316]\n",
        "v_14 = vote_dataset.iloc[316:333]\n",
        "v_15 = vote_dataset.iloc[333:350]\n",
        "v_16 = vote_dataset.iloc[350:367]\n",
        "v_17 = vote_dataset.iloc[367:384]\n",
        "v_18 = vote_dataset.iloc[384:401]\n",
        "v_19 = vote_dataset.iloc[401:418]\n",
        "v_20 = vote_dataset.iloc[418:435]\n",
        "\n",
        "vote_one = pd.merge(v_1, v_11, how = \"outer\")\n",
        "vote_two = pd.merge(v_2, v_12, how = \"outer\")\n",
        "vote_three = pd.merge(v_3, v_13, how = \"outer\")\n",
        "vote_four = pd.merge(v_4, v_14, how = \"outer\")\n",
        "vote_five = pd.merge(v_5, v_15, how = \"outer\")\n",
        "vote_six = pd.merge(v_6, v_16, how = \"outer\")\n",
        "vote_seven = pd.merge(v_7, v_17, how = \"outer\")\n",
        "vote_eight = pd.merge(v_8, v_18, how = \"outer\")\n",
        "vote_nine = pd.merge(v_9, v_19, how = \"outer\")\n",
        "vote_ten = pd.merge(v_10, v_20, how = \"outer\")"
      ],
      "metadata": {
        "id": "k6KdlM5B7E5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "#we have 458 and 241 from each class\n",
        "#we want to have three training sets\n",
        "#class one should be divided into 153, 153, 152\n",
        "#class two should be divided into 80, 80, 81\n",
        "#.iloc[a:b] does not contain a value so it should be a+1 through b\n",
        "#we will combine one + four, two + five, and three + six\n",
        "\n",
        "b_1 = breast_cancer_dataset.iloc[:45]\n",
        "b_2 = breast_cancer_dataset.iloc[45:90]\n",
        "b_3 = breast_cancer_dataset.iloc[90:136]\n",
        "b_4 = breast_cancer_dataset.iloc[136:182]\n",
        "b_5 = breast_cancer_dataset.iloc[182:228]\n",
        "b_6 = breast_cancer_dataset.iloc[228:274]\n",
        "b_7 = breast_cancer_dataset.iloc[274:320]\n",
        "b_8 = breast_cancer_dataset.iloc[320:366]\n",
        "b_9 = breast_cancer_dataset.iloc[366:412]\n",
        "b_10 = breast_cancer_dataset.iloc[412:458]\n",
        " \n",
        "b_11 = breast_cancer_dataset.iloc[458:482]\n",
        "b_12 = breast_cancer_dataset.iloc[482:506]\n",
        "b_13 = breast_cancer_dataset.iloc[506:530]\n",
        "b_14 = breast_cancer_dataset.iloc[530:554]\n",
        "b_15 = breast_cancer_dataset.iloc[554:578]\n",
        "b_16 = breast_cancer_dataset.iloc[578:602]\n",
        "b_17 = breast_cancer_dataset.iloc[602:626]\n",
        "b_18 = breast_cancer_dataset.iloc[626:650]\n",
        "b_19 = breast_cancer_dataset.iloc[650:674]\n",
        "b_20 = breast_cancer_dataset.iloc[674:699]\n",
        " \n",
        "breast_one = pd.merge(b_1, b_11, how=\"outer\")\n",
        "breast_two = pd.merge(b_2, b_12, how=\"outer\")\n",
        "breast_three = pd.merge(b_3, b_13, how=\"outer\")\n",
        "breast_four = pd.merge(b_4, b_14, how=\"outer\")\n",
        "breast_five = pd.merge(b_5, b_15, how=\"outer\")\n",
        "breast_six = pd.merge(b_6, b_16, how=\"outer\")\n",
        "breast_seven = pd.merge(b_7, b_17, how=\"outer\")\n",
        "breast_eight = pd.merge(b_8, b_18, how=\"outer\")\n",
        "breast_nine = pd.merge(b_9, b_19, how=\"outer\")\n",
        "breast_ten = pd.merge(b_10, b_20, how=\"outer\")\n"
      ],
      "metadata": {
        "id": "1-Kkf1gPzGWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "#we have 70, 76, 17, 0, 13, 9, and 29 from each class\n",
        "#we want to have ten training sets\n",
        "#.iloc[a:b] does not contain a value so it should be a+1 through b\n",
        "\n",
        "g1 = glass_dataset.iloc[:7]\n",
        "g2 = glass_dataset.iloc[7:14]\n",
        "g3 = glass_dataset.iloc[14:21]\n",
        "g4 = glass_dataset.iloc[21:28]\n",
        "g5 = glass_dataset.iloc[28:35]\n",
        "g6 = glass_dataset.iloc[35:42]\n",
        "g7 = glass_dataset.iloc[42:49]\n",
        "g8 = glass_dataset.iloc[49:56]\n",
        "g9 = glass_dataset.iloc[56:63]\n",
        "g10 = glass_dataset.iloc[63:70]\n",
        "\n",
        "g11 = glass_dataset.iloc[70:77]\n",
        "g12 = glass_dataset.iloc[77:84]\n",
        "g13 = glass_dataset.iloc[84:91]\n",
        "g14 = glass_dataset.iloc[91:98]\n",
        "g15 = glass_dataset.iloc[98:106]\n",
        "g16 = glass_dataset.iloc[106:114]\n",
        "g17 = glass_dataset.iloc[114:122]\n",
        "g18 = glass_dataset.iloc[122:130]\n",
        "g19 = glass_dataset.iloc[130:138]\n",
        "g20 = glass_dataset.iloc[138:146]\n",
        "\n",
        "g21 = glass_dataset.iloc[146:148]\n",
        "g22 = glass_dataset.iloc[148:150]\n",
        "g23 = glass_dataset.iloc[150:152]\n",
        "g24 = glass_dataset.iloc[152:154]\n",
        "g25 = glass_dataset.iloc[154:156]\n",
        "g26 = glass_dataset.iloc[156:158]\n",
        "g27 = glass_dataset.iloc[158:160]\n",
        "g28 = glass_dataset.iloc[160:161]\n",
        "g29 = glass_dataset.iloc[161:162]\n",
        "g30 = glass_dataset.iloc[162:163]\n",
        "\n",
        "g31 = glass_dataset.iloc[163:164]\n",
        "g32 = glass_dataset.iloc[164:165]\n",
        "g33 = glass_dataset.iloc[165:166]\n",
        "g34 = glass_dataset.iloc[166:167]\n",
        "g35 = glass_dataset.iloc[167:168]\n",
        "g36 = glass_dataset.iloc[168:169]\n",
        "g37 = glass_dataset.iloc[169:170]\n",
        "g38 = glass_dataset.iloc[170:172]\n",
        "g39 = glass_dataset.iloc[172:174]\n",
        "g40 = glass_dataset.iloc[174:176]\n",
        "\n",
        "g41 = glass_dataset.iloc[176:177]\n",
        "g42 = glass_dataset.iloc[177:178]\n",
        "g43 = glass_dataset.iloc[178:179]\n",
        "g44 = glass_dataset.iloc[179:180]\n",
        "g45 = glass_dataset.iloc[180:181]\n",
        "g46 = glass_dataset.iloc[181:182]\n",
        "g47 = glass_dataset.iloc[182:183]\n",
        "g48 = glass_dataset.iloc[183:184]\n",
        "g49 = glass_dataset.iloc[184:185]\n",
        "g50 = glass_dataset.iloc[185:185]\n",
        "\n",
        "g51 = glass_dataset.iloc[185:188]\n",
        "g52 = glass_dataset.iloc[188:191]\n",
        "g53 = glass_dataset.iloc[191:194]\n",
        "g54 = glass_dataset.iloc[194:197]\n",
        "g55 = glass_dataset.iloc[197:200]\n",
        "g56 = glass_dataset.iloc[200:203]\n",
        "g57 = glass_dataset.iloc[203:206]\n",
        "g58 = glass_dataset.iloc[206:209]\n",
        "g59 = glass_dataset.iloc[209:211]\n",
        "g60 = glass_dataset.iloc[211:214]\n",
        "\n",
        "glass_one = pd.merge(g1, g11, how=\"outer\")\n",
        "glass_one = pd.merge(glass_one, g21, how=\"outer\")\n",
        "glass_one = pd.merge(glass_one, g31, how=\"outer\")\n",
        "glass_one = pd.merge(glass_one, g41, how=\"outer\")\n",
        "glass_one = pd.merge(glass_one, g51, how=\"outer\")\n",
        "\n",
        "glass_two = pd.merge(g2, g12, how=\"outer\")\n",
        "glass_two = pd.merge(glass_two, g22, how=\"outer\")\n",
        "glass_two = pd.merge(glass_two, g32, how=\"outer\")\n",
        "glass_two = pd.merge(glass_two, g42, how=\"outer\")\n",
        "glass_two = pd.merge(glass_two, g52, how=\"outer\")\n",
        "\n",
        "glass_three = pd.merge(g3, g13, how=\"outer\")\n",
        "glass_three = pd.merge(glass_three, g23, how=\"outer\")\n",
        "glass_three = pd.merge(glass_three, g33, how=\"outer\")\n",
        "glass_three = pd.merge(glass_three, g43, how=\"outer\")\n",
        "glass_three = pd.merge(glass_three, g53, how=\"outer\")\n",
        "\n",
        "glass_four = pd.merge(g4, g14, how=\"outer\")\n",
        "glass_four = pd.merge(glass_four, g24, how=\"outer\")\n",
        "glass_four = pd.merge(glass_four, g34, how=\"outer\")\n",
        "glass_four = pd.merge(glass_four, g44, how=\"outer\")\n",
        "glass_four = pd.merge(glass_four, g54, how=\"outer\")\n",
        "\n",
        "glass_five = pd.merge(g5, g15, how=\"outer\")\n",
        "glass_five = pd.merge(glass_five, g25, how=\"outer\")\n",
        "glass_five = pd.merge(glass_five, g35, how=\"outer\")\n",
        "glass_five = pd.merge(glass_five, g45, how=\"outer\")\n",
        "glass_five = pd.merge(glass_five, g55, how=\"outer\")\n",
        "\n",
        "glass_six = pd.merge(g6, g16, how=\"outer\")\n",
        "glass_six = pd.merge(glass_six, g26, how=\"outer\")\n",
        "glass_six = pd.merge(glass_six, g36, how=\"outer\")\n",
        "glass_six = pd.merge(glass_six, g46, how=\"outer\")\n",
        "glass_six = pd.merge(glass_six, g56, how=\"outer\")\n",
        "\n",
        "glass_seven = pd.merge(g7, g17, how=\"outer\")\n",
        "glass_seven = pd.merge(glass_seven, g27, how=\"outer\")\n",
        "glass_seven = pd.merge(glass_seven, g37, how=\"outer\")\n",
        "glass_seven = pd.merge(glass_seven, g47, how=\"outer\")\n",
        "glass_seven = pd.merge(glass_seven, g57, how=\"outer\")\n",
        "\n",
        "glass_eight = pd.merge(g8, g18, how=\"outer\")\n",
        "glass_eight = pd.merge(glass_eight, g28, how=\"outer\")\n",
        "glass_eight = pd.merge(glass_eight, g38, how=\"outer\")\n",
        "glass_eight = pd.merge(glass_eight, g48, how=\"outer\")\n",
        "glass_eight = pd.merge(glass_eight, g58, how=\"outer\")\n",
        "\n",
        "glass_nine = pd.merge(g9, g19, how=\"outer\")\n",
        "glass_nine = pd.merge(glass_nine, g29, how=\"outer\")\n",
        "glass_nine = pd.merge(glass_nine, g39, how=\"outer\")\n",
        "glass_nine = pd.merge(glass_nine, g49, how=\"outer\")\n",
        "glass_nine = pd.merge(glass_nine, g59, how=\"outer\")\n",
        "\n",
        "glass_ten = pd.merge(g10, g20, how=\"outer\")\n",
        "glass_ten = pd.merge(glass_ten, g30, how=\"outer\")\n",
        "glass_ten = pd.merge(glass_ten, g40, how=\"outer\")\n",
        "glass_ten = pd.merge(glass_ten, g50, how=\"outer\")\n",
        "glass_ten = pd.merge(glass_ten, g60, how=\"outer\")\n",
        "\n"
      ],
      "metadata": {
        "id": "T-jEIzzS1i0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "#first part\n",
        "#Q(C = ci) = (number of instances in each class)/(number of instances in training sets)\n",
        "\n",
        "def Q(df, attribute):\n",
        "  value = 1.0\n",
        "  occurence = df[attribute].value_counts()[value]\n",
        "  return occurence / df.shape[0]"
      ],
      "metadata": {
        "id": "ONYPARQV_LbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "#second part\n",
        "#F (Aj = ak) = # {(xAj = ak) and (x = ci)} + 1/ Nci + d\n",
        "\n",
        "def F(dftrain, attribute_name, attribute_value, class_name, len_class_name):\n",
        "  sum_count = 0\n",
        "  N = 0\n",
        "  d = len(dftrain.columns) - len_class_name\n",
        "  for k in dftrain.index:\n",
        "    if ((dftrain[attribute_name][k] == attribute_value) and (dftrain[class_name][k] == 1)):\n",
        "      sum_count = sum_count + 1\n",
        "    if dftrain[class_name][k] == 1:\n",
        "      N = N + 1\n",
        "\n",
        "  numerator = sum_count + 1\n",
        "  denominator = N + d\n",
        "  F = numerator / denominator\n",
        "  return F"
      ],
      "metadata": {
        "id": "HgTkr71CDOen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "\"\"\"\n",
        "this function will return the C function\n",
        "\"\"\"\n",
        "def C(instance, dftrain, class_name, len_class_name):\n",
        "  Q_X = Q(dftrain, class_name)\n",
        "  f = 1\n",
        "  \n",
        "  for index, value in instance.items():\n",
        "    f = f * F(dftrain, index, value, class_name, len_class_name)\n",
        "\n",
        "  return Q_X*f"
      ],
      "metadata": {
        "id": "UMzbUfN19F3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "\"\"\"\n",
        "this function will iterate through the rows of the training dataset (instances in the training dataset),\n",
        "and for each instance, this will iterate through the classes.\n",
        "this function will call def C for each combination of the instance in the training dataset and the class\n",
        "\n",
        "probably need two df, one for testing, one for training)\n",
        "\n",
        "param {dftest: the dataset we use as the test dataset, this should be a pandas dataframe\n",
        "       dftrain: the dataset we use as the training dataset, this should be a pandas dataframe\n",
        "       class_name: an array of classes that is in the dataset we use, this must be np.array\n",
        "}\n",
        "\n",
        "return {C: the probability of a certain instance in a dataset belongs to a certain class in a dataset,\n",
        "        we should have the return for each instance in the test dataset\n",
        "        this return should be a number\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def Class(dftest, dftrain, class_name):\n",
        "  matrix = []\n",
        "\n",
        "  for X in range (dftest.shape[0]): #<- maybe we should change this to for index, value dftest.items():, we want the index of the rows in the dataset\n",
        "    P = -1\n",
        "    predict = None\n",
        "    actual = None\n",
        "\n",
        "    for g in class_name: #<- g is the actual class name in the class_name array\n",
        "      if dftest.loc[X][g] == 1.0 or dftest.loc[X][g] == 1: #<- this like of code sees if the attribute under the class name in the X row is labeled as 1 or not\n",
        "        actual = g\n",
        "\n",
        "    for h in class_name: #<- h is the actual class name and for each row, this line of code drops the columns of the attribute with the class name\n",
        "      dftest.loc[X].drop(h)\n",
        "    \n",
        "    for i in range(len(class_name)):\n",
        "      C_X = C(dftest.loc[X], dftrain, class_name[i], len(class_name))\n",
        "      if P < C_X:\n",
        "        P = C_X\n",
        "        predict = class_name[i]\n",
        "    \n",
        "    #print(predict, actual)\n",
        "    matrix.append([predict, actual])\n",
        "  \n",
        "  return matrix"
      ],
      "metadata": {
        "id": "7rqixpHV3wQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "def zero_loss_func(result):\n",
        "  correct = 0\n",
        "  incorrect = 0\n",
        "  for i in range(len(result)):\n",
        "    if result[i][0] == result[i][1]:\n",
        "      correct = correct+1\n",
        "    elif result[i][0] != result[i][1]:\n",
        "      incorrect = incorrect + 1\n",
        "  \n",
        "  return [correct, incorrect]"
      ],
      "metadata": {
        "id": "PdX9W5oPGgLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "def confusion_matrix_breast_cancer(result):\n",
        "  two_two = 0\n",
        "  two_four = 0\n",
        "  four_two = 0\n",
        "  four_four = 0\n",
        "\n",
        "  #predict_actual\n",
        "  \n",
        "  for i in result:\n",
        "    \n",
        "    if (i[0] == i[1]):\n",
        "      if (i[1] == 2): #<- predict is 2 and actual is 2\n",
        "        two_two += 1\n",
        "      \n",
        "      elif (i[1] == 4): #<- predict is 2 and actual is 2\n",
        "        four_four += 1\n",
        "\n",
        "    elif (i[0] != i[1]):\n",
        "      if (i[1] == 2): #<- predict is 4 and actual is 2\n",
        "        four_two += 1\n",
        "\n",
        "      elif (i[1] == 4): #<- predict is 2 and actual is 4\n",
        "        two_four += 1\n",
        "  \n",
        "    matrix = [[two_two, two_four], [four_two, four_four]]\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "NAnFOj0KSNit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "def confusion_matrix_vote(result):\n",
        "  repub_repub = 0\n",
        "  repub_demo = 0\n",
        "  demo_repub = 0\n",
        "  demo_demo = 0\n",
        "\n",
        "  #predict_actual\n",
        "  #2 = republican\n",
        "  #4 = democrat\n",
        "  \n",
        "  for i in result:\n",
        "    \n",
        "    if (i[0] == i[1]):\n",
        "      if (i[1] == \"republican\"): #<- predict is repub and actual is repub\n",
        "        repub_repub += 1\n",
        "      \n",
        "      elif (i[1] == \"democrat\"): #<- predict is demo and actual is demo\n",
        "        demo_demo += 1\n",
        "\n",
        "    elif (i[0] != i[1]):\n",
        "      if (i[1] == \"republican\"): #<- predict is demo and actual is repub\n",
        "        demo_repub += 1\n",
        "\n",
        "      elif (i[1] == \"democrat\"): #<- predict is repub and actual is demo\n",
        "        repub_demo += 1\n",
        "  \n",
        "    matrix = [[repub_repub, repub_demo], \n",
        "              [demo_repub, demo_demo]]\n",
        "    \n",
        "  return matrix"
      ],
      "metadata": {
        "id": "2UyonOsKnp9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "def confusion_matrix_soybean(result):\n",
        "  D1_D1 = 0\n",
        "  D1_D2 = 0\n",
        "  D1_D3 = 0\n",
        "  D1_D4 = 0\n",
        "\n",
        "  D2_D1 = 0\n",
        "  D2_D2 = 0\n",
        "  D2_D3 = 0\n",
        "  D2_D4 = 0\n",
        "\n",
        "  D3_D1 = 0\n",
        "  D3_D2 = 0\n",
        "  D3_D3 = 0\n",
        "  D3_D4 = 0\n",
        "\n",
        "  D4_D1 = 0\n",
        "  D4_D2 = 0\n",
        "  D4_D3 = 0\n",
        "  D4_D4 = 0\n",
        "\n",
        "  #predict_actual\n",
        "  #classes are\n",
        "  #D1\n",
        "  #D2\n",
        "  #D3\n",
        "  #D4\n",
        "  \n",
        "  for i in result:\n",
        "    \"\"\"\n",
        "    i[0] is the predicted value\n",
        "    i[1] is the actual value\n",
        "    \"\"\"\n",
        "    if (i[0] == \"D1\"):\n",
        "      if (i[1] == \"D1\"):\n",
        "        D1_D1 +=1\n",
        "\n",
        "      elif (i[1] == \"D2\"):\n",
        "        D1_D2 +=1\n",
        "\n",
        "      elif (i[1] == \"D3\"):\n",
        "        D1_D3 +=1\n",
        "\n",
        "      elif (i[1] == \"D4\"):\n",
        "        D1_D4 +=1\n",
        "\n",
        "    elif (i[0] == \"D2\"):\n",
        "      if (i[1] == \"D1\"):\n",
        "        D2_D1 +=1\n",
        "\n",
        "      elif (i[1] == \"D2\"):\n",
        "        D2_D2 +=1\n",
        "\n",
        "      elif (i[1] == \"D3\"):\n",
        "        D2_D3 +=1\n",
        "\n",
        "      elif (i[1] == \"D4\"):\n",
        "        D2_D4 +=1\n",
        "\n",
        "    elif (i[0] == \"D3\"):\n",
        "      if (i[1] == \"D1\"):\n",
        "        D3_D1 +=1\n",
        "\n",
        "      elif (i[1] == \"D2\"):\n",
        "        D3_D2 +=1\n",
        "\n",
        "      elif (i[1] == \"D3\"):\n",
        "        D3_D3 +=1\n",
        "\n",
        "      elif (i[1] == \"D4\"):\n",
        "        D3_D4 +=1\n",
        "\n",
        "    elif (i[0] == \"D4\"):\n",
        "      if (i[1] == \"D1\"):\n",
        "        D4_D1 +=1\n",
        "\n",
        "      elif (i[1] == \"D2\"):\n",
        "        D4_D2 +=1\n",
        "\n",
        "      elif (i[1] == \"D3\"):\n",
        "        D4_D3 +=1\n",
        "\n",
        "      elif (i[1] == \"D4\"):\n",
        "        D4_D4 +=1\n",
        "  \n",
        "    matrix = [[D1_D1, D1_D2, D1_D3, D1_D4], \n",
        "              [D2_D1, D2_D2, D2_D3, D2_D4], \n",
        "              [D3_D1, D3_D2, D3_D3,D3_D4], \n",
        "              [D4_D1, D4_D2, D4_D3, D4_D4]]\n",
        "    \n",
        "  return matrix"
      ],
      "metadata": {
        "id": "5wXuNdAvtbPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "def confusion_matrix_glass(result):\n",
        "  one_one = 0\n",
        "  one_two = 0\n",
        "  one_three = 0\n",
        "  one_five = 0\n",
        "  one_six = 0\n",
        "  one_seven = 0\n",
        "\n",
        "  two_one = 0\n",
        "  two_two = 0\n",
        "  two_three = 0\n",
        "  two_five = 0\n",
        "  two_six = 0\n",
        "  two_seven = 0\n",
        "\n",
        "  three_one = 0\n",
        "  three_two = 0\n",
        "  three_three = 0\n",
        "  three_five = 0\n",
        "  three_six = 0\n",
        "  three_seven = 0\n",
        "\n",
        "  five_one = 0\n",
        "  five_two = 0\n",
        "  five_three = 0\n",
        "  five_five = 0\n",
        "  five_six = 0\n",
        "  five_seven = 0\n",
        "\n",
        "  six_one = 0\n",
        "  six_two = 0\n",
        "  six_three = 0\n",
        "  six_five = 0\n",
        "  six_six = 0\n",
        "  six_seven = 0\n",
        "\n",
        "  seven_one = 0\n",
        "  seven_two = 0\n",
        "  seven_three = 0\n",
        "  seven_five = 0\n",
        "  seven_six = 0\n",
        "  seven_seven = 0\n",
        "\n",
        "  #predict_actual\n",
        "  #classes are\n",
        "  #1\n",
        "  #2\n",
        "  #3\n",
        "  #5\n",
        "  #6\n",
        "  #7\n",
        "  \n",
        "  for i in result:\n",
        "    \"\"\"\n",
        "    i[0] is the predicted value\n",
        "    i[1] is the actual value\n",
        "    \"\"\"\n",
        "    if (i[0] == 1):\n",
        "      if (i[1] == 1):\n",
        "        one_one += 1\n",
        "\n",
        "      elif (i[1] == 2):\n",
        "        one_two += 1\n",
        "\n",
        "      elif (i[1] == 3):\n",
        "        one_three += 1\n",
        "\n",
        "      elif (i[1] == 5):\n",
        "        one_five += 1\n",
        "\n",
        "      elif (i[1] == 6):\n",
        "        one_six += 1\n",
        "\n",
        "      elif (i[1] == 7):\n",
        "        one_seven += 1\n",
        "\n",
        "    elif (i[0] == 2):\n",
        "      if (i[1] == 1):\n",
        "        two_one += 1\n",
        "\n",
        "      elif (i[1] == 2):\n",
        "        two_two += 1\n",
        "\n",
        "      elif (i[1] == 3):\n",
        "        two_three += 1\n",
        "\n",
        "      elif (i[1] == 5):\n",
        "        two_five += 1\n",
        "\n",
        "      elif (i[1] == 6):\n",
        "        two_six += 1\n",
        "\n",
        "      elif (i[1] == 7):\n",
        "        two_seven += 1\n",
        "\n",
        "    elif (i[0] == 3):\n",
        "      if (i[1] == 1):\n",
        "        three_one += 1\n",
        "\n",
        "      elif (i[1] == 2):\n",
        "        three_two += 1\n",
        "\n",
        "      elif (i[1] == 3):\n",
        "        three_three += 1\n",
        "\n",
        "      elif (i[1] == 5):\n",
        "        three_five += 1\n",
        "\n",
        "      elif (i[1] == 6):\n",
        "        three_six += 1\n",
        "\n",
        "      elif (i[1] == 7):\n",
        "        three_seven += 1\n",
        "\n",
        "    elif (i[0] == 5):\n",
        "      if (i[1] == 1):\n",
        "        five_one += 1\n",
        "\n",
        "      elif (i[1] == 2):\n",
        "        five_two += 1\n",
        "\n",
        "      elif (i[1] == 3):\n",
        "        five_three += 1\n",
        "\n",
        "      elif (i[1] == 5):\n",
        "        five_five += 1\n",
        "\n",
        "      elif (i[1] == 6):\n",
        "        five_six += 1\n",
        "\n",
        "      elif (i[1] == 7):\n",
        "        five_seven += 1\n",
        "\n",
        "    elif (i[0] == 6):\n",
        "      if (i[1] == 1):\n",
        "        six_one += 1\n",
        "\n",
        "      elif (i[1] == 2):\n",
        "        six_two += 1\n",
        "\n",
        "      elif (i[1] == 3):\n",
        "        six_three += 1\n",
        "\n",
        "      elif (i[1] == 5):\n",
        "        six_five += 1\n",
        "\n",
        "      elif (i[1] == 6):\n",
        "        six_six += 1\n",
        "\n",
        "      elif (i[1] == 7):\n",
        "        six_seven += 1\n",
        "\n",
        "    elif (i[0] == 7):\n",
        "      if (i[1] == 1):\n",
        "        seven_one += 1\n",
        "\n",
        "      elif (i[1] == 2):\n",
        "        seven_two += 1\n",
        "\n",
        "      elif (i[1] == 3):\n",
        "        seven_three += 1\n",
        "\n",
        "      elif (i[1] == 5):\n",
        "        seven_five += 1\n",
        "\n",
        "      elif (i[1] == 6):\n",
        "        seven_six += 1\n",
        "\n",
        "      elif (i[1] == 7):\n",
        "        seven_seven += 1\n",
        "  \n",
        "    matrix = [[one_one, one_two, one_three, one_five, one_six, one_seven], \n",
        "              [two_one, two_two, two_three, two_five, two_six, two_seven], \n",
        "              [three_one, three_two, three_three, three_five, three_six, three_seven], \n",
        "              [five_one, five_two, five_three, five_five, five_six, five_seven],\n",
        "              [six_one, six_two, six_three, six_five, six_six, six_seven],\n",
        "              [seven_one, seven_two, seven_three, seven_five, seven_six, seven_seven]]\n",
        "    \n",
        "  return matrix"
      ],
      "metadata": {
        "id": "c2ajq--Lz7Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "def confusion_matrix_iris(result):\n",
        "  setosa_setosa = 0\n",
        "  setosa_versicolor = 0\n",
        "  setosa_virginica = 0\n",
        "\n",
        "  versicolor_setosa = 0\n",
        "  versicolor_versicolor = 0\n",
        "  versicolor_virginica = 0\n",
        "\n",
        "  virginica_setosa = 0\n",
        "  virginica_versicolor = 0\n",
        "  virginica_virginica = 0\n",
        "\n",
        "  #predict_actual\n",
        "  #classes are\n",
        "  #setosa\n",
        "  #versicolor\n",
        "  #virginica\n",
        "  \n",
        "  for i in result:\n",
        "    \"\"\"\n",
        "    i[0] is the predicted value\n",
        "    i[1] is the actual value\n",
        "    \"\"\"\n",
        "    if (i[0] == \"Iris-setosa\"):\n",
        "      if (i[1] == \"Iris-setosa\"):\n",
        "        setosa_setosa += 1\n",
        "\n",
        "      elif (i[1] == \"Iris-versicolor\"):\n",
        "        setosa_versicolor += 1\n",
        "\n",
        "      elif (i[1] == \"Iris-virginica\"):\n",
        "        setosa_virginica += 1\n",
        "\n",
        "    elif (i[0] == \"Iris-versicolor\"):\n",
        "      if (i[1] == \"Iris-setosa\"):\n",
        "        versicolor_setosa += 1\n",
        "\n",
        "      elif (i[1] == \"Iris-versicolor\"):\n",
        "        versicolor_versicolor += 1\n",
        "\n",
        "      elif (i[1] == \"Iris-virginica\"):\n",
        "        versicolor_virginica += 1\n",
        "\n",
        "    elif (i[0] == \"Iris-virginica\"):\n",
        "      if (i[1] == \"Iris-setosa\"):\n",
        "        virginica_setosa += 1\n",
        "\n",
        "      elif (i[1] == \"Iris-versicolor\"):\n",
        "        virginica_versicolor += 1\n",
        "\n",
        "      elif (i[1] == \"Iris-virginica\"):\n",
        "        virginica_virginica += 1\n",
        "  \n",
        "    matrix = [[setosa_setosa, setosa_versicolor, setosa_virginica], \n",
        "              [versicolor_setosa, versicolor_versicolor, versicolor_virginica], \n",
        "              [virginica_setosa, virginica_versicolor, virginica_virginica]]\n",
        "    \n",
        "  return matrix"
      ],
      "metadata": {
        "id": "aeNz-E14-nD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "#Class(test set, training set, class names)\n",
        "#since each dataset is divided into 3, we will have 3 different combinations of those sets\n",
        "#then the class name array is also defined as setname_class in numpy array\n",
        " \n",
        "breast_training_one = shuffle(train_merge(breast_ten, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine), breast_class)\n",
        "breast_training_two = shuffle(train_merge(breast_ten, breast_one, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine), breast_class)\n",
        "breast_training_three = shuffle(train_merge(breast_ten, breast_one, breast_two, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine), breast_class)\n",
        "breast_training_four = shuffle(train_merge(breast_ten, breast_one, breast_two, breast_three, breast_five, breast_six, breast_seven, breast_eight, breast_nine), breast_class)\n",
        "breast_training_five = shuffle(train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_six, breast_seven, breast_eight, breast_nine), breast_class)\n",
        "breast_training_six = shuffle(train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_five, breast_seven, breast_eight, breast_nine), breast_class)\n",
        "breast_training_seven = shuffle(train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_eight, breast_nine), breast_class)\n",
        "breast_training_eight = shuffle(train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_nine), breast_class)\n",
        "breast_training_nine = shuffle(train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight), breast_class)\n",
        "breast_training_ten = shuffle(train_merge(breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine), breast_class)\n",
        " \n",
        "print(\"breast first training start\")\n",
        "result_breast_one = Class(breast_one, breast_training_one, breast_class)\n",
        "print(\"breast second training start\")\n",
        "result_breast_two = Class(breast_two, breast_training_two, breast_class)\n",
        "print(\"breast third training start\")\n",
        "result_breast_three = Class(breast_three, breast_training_three, breast_class)\n",
        "print(\"breast fourth training start\")\n",
        "result_breast_four = Class(breast_four, breast_training_four, breast_class)\n",
        "print(\"breast fifth training start\")\n",
        "result_breast_five = Class(breast_five, breast_training_five, breast_class)\n",
        "print(\"breast sixth training start\")\n",
        "result_breast_six = Class(breast_six, breast_training_six, breast_class)\n",
        "print(\"breast seventh training start\")\n",
        "result_breast_seven = Class(breast_seven, breast_training_seven, breast_class)\n",
        "print(\"breast eighth training start\")\n",
        "result_breast_eight = Class(breast_eight, breast_training_eight, breast_class)\n",
        "print(\"breast ninth training start\")\n",
        "result_breast_nine = Class(breast_nine, breast_training_nine, breast_class)\n",
        "print(\"breast tenth training start\")\n",
        "result_breast_ten = Class(breast_ten, breast_training_ten, breast_class)\n",
        "\n",
        "print(zero_loss_func(result_breast_one))\n",
        "print(zero_loss_func(result_breast_two))\n",
        "print(zero_loss_func(result_breast_three))\n",
        "print(zero_loss_func(result_breast_four))\n",
        "print(zero_loss_func(result_breast_five))\n",
        "print(zero_loss_func(result_breast_six))\n",
        "print(zero_loss_func(result_breast_seven))\n",
        "print(zero_loss_func(result_breast_eight))\n",
        "print(zero_loss_func(result_breast_nine))\n",
        "print(zero_loss_func(result_breast_ten))\n",
        "\n",
        "print(confusion_matrix_breast_cancer(result_breast_one))\n",
        "print(confusion_matrix_breast_cancer(result_breast_two))\n",
        "print(confusion_matrix_breast_cancer(result_breast_three))\n",
        "print(confusion_matrix_breast_cancer(result_breast_four))\n",
        "print(confusion_matrix_breast_cancer(result_breast_five))\n",
        "print(confusion_matrix_breast_cancer(result_breast_six))\n",
        "print(confusion_matrix_breast_cancer(result_breast_seven))\n",
        "print(confusion_matrix_breast_cancer(result_breast_eight))\n",
        "print(confusion_matrix_breast_cancer(result_breast_nine))\n",
        "print(confusion_matrix_breast_cancer(result_breast_ten))"
      ],
      "metadata": {
        "id": "8RGUSnpVFFkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53541f3-0114-4478-e6bb-544d0a00fd2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "breast first training start\n",
            "breast second training start\n",
            "breast third training start\n",
            "breast fourth training start\n",
            "breast fifth training start\n",
            "breast sixth training start\n",
            "breast seventh training start\n",
            "breast eighth training start\n",
            "breast ninth training start\n",
            "breast tenth training start\n",
            "[67, 2]\n",
            "[67, 2]\n",
            "[70, 0]\n",
            "[70, 0]\n",
            "[69, 1]\n",
            "[70, 0]\n",
            "[70, 0]\n",
            "[68, 2]\n",
            "[68, 2]\n",
            "[71, 0]\n",
            "[[43, 0], [2, 24]]\n",
            "[[43, 0], [2, 24]]\n",
            "[[46, 0], [0, 24]]\n",
            "[[46, 0], [0, 24]]\n",
            "[[45, 0], [1, 24]]\n",
            "[[46, 0], [0, 24]]\n",
            "[[46, 0], [0, 24]]\n",
            "[[44, 0], [2, 24]]\n",
            "[[44, 0], [2, 24]]\n",
            "[[46, 0], [0, 25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "glass_training_one = shuffle(train_merge(glass_ten, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine), glass_class)\n",
        "glass_training_two = shuffle(train_merge(glass_ten, glass_one, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine), glass_class)\n",
        "glass_training_three = shuffle(train_merge(glass_ten, glass_one, glass_two, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine), glass_class)\n",
        "glass_training_four = shuffle(train_merge(glass_ten, glass_one, glass_two, glass_three, glass_five, glass_six, glass_seven, glass_eight, glass_nine), glass_class)\n",
        "glass_training_five = shuffle(train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_six, glass_seven, glass_eight, glass_nine), glass_class)\n",
        "glass_training_six = shuffle(train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_seven, glass_eight, glass_nine), glass_class)\n",
        "glass_training_seven = shuffle(train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_eight, glass_nine), glass_class)\n",
        "glass_training_eight = shuffle(train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_nine), glass_class)\n",
        "glass_training_nine = shuffle(train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight), glass_class)\n",
        "glass_training_ten = shuffle(train_merge(glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine), glass_class)\n",
        " \n",
        "print(\"glass first training start\")\n",
        "result_glass_one = Class(glass_one, glass_training_one, glass_class)\n",
        "print(\"glass second training start\")\n",
        "result_glass_two = Class(glass_two, glass_training_two, glass_class)\n",
        "print(\"glass third training start\")\n",
        "result_glass_three = Class(glass_three, glass_training_three, glass_class)\n",
        "print(\"glass fourth training start\")\n",
        "result_glass_four = Class(glass_four, glass_training_four, glass_class)\n",
        "print(\"glass fifth training start\")\n",
        "result_glass_five = Class(glass_five, glass_training_five, glass_class)\n",
        "print(\"glass sixth training start\")\n",
        "result_glass_six = Class(glass_six, glass_training_six, glass_class)\n",
        "print(\"glass seventh training start\")\n",
        "result_glass_seven = Class(glass_seven, glass_training_seven, glass_class)\n",
        "print(\"glass eighth training start\")\n",
        "result_glass_eight = Class(glass_eight, glass_training_eight, glass_class)\n",
        "print(\"glass ninth training start\")\n",
        "result_glass_nine = Class(glass_nine, glass_training_nine, glass_class)\n",
        "print(\"glass tenth training start\")\n",
        "result_glass_ten = Class(glass_ten, glass_training_ten, glass_class)\n",
        " \n",
        "print(zero_loss_func(result_glass_one))\n",
        "print(zero_loss_func(result_glass_two))\n",
        "print(zero_loss_func(result_glass_three))\n",
        "print(zero_loss_func(result_glass_four))\n",
        "print(zero_loss_func(result_glass_five))\n",
        "print(zero_loss_func(result_glass_six))\n",
        "print(zero_loss_func(result_glass_seven))\n",
        "print(zero_loss_func(result_glass_eight))\n",
        "print(zero_loss_func(result_glass_nine))\n",
        "print(zero_loss_func(result_glass_ten))\n",
        "\n",
        "print(confusion_matrix_glass(result_glass_one))\n",
        "print(confusion_matrix_glass(result_glass_two))\n",
        "print(confusion_matrix_glass(result_glass_three))\n",
        "print(confusion_matrix_glass(result_glass_four))\n",
        "print(confusion_matrix_glass(result_glass_five))\n",
        "print(confusion_matrix_glass(result_glass_six))\n",
        "print(confusion_matrix_glass(result_glass_seven))\n",
        "print(confusion_matrix_glass(result_glass_eight))\n",
        "print(confusion_matrix_glass(result_glass_nine))\n",
        "print(confusion_matrix_glass(result_glass_ten))"
      ],
      "metadata": {
        "id": "qDn4ymGNAZVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c16d549-9d64-460d-99d2-20275cbd21d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glass first training start\n",
            "glass second training start\n",
            "glass third training start\n",
            "glass fourth training start\n",
            "glass fifth training start\n",
            "glass sixth training start\n",
            "glass seventh training start\n",
            "glass eighth training start\n",
            "glass ninth training start\n",
            "glass tenth training start\n",
            "[20, 1]\n",
            "[20, 1]\n",
            "[21, 0]\n",
            "[21, 0]\n",
            "[22, 0]\n",
            "[21, 1]\n",
            "[21, 1]\n",
            "[21, 1]\n",
            "[20, 1]\n",
            "[20, 1]\n",
            "[[7, 0, 0, 0, 1, 0], [0, 7, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3]]\n",
            "[[7, 0, 0, 0, 0, 0], [0, 7, 0, 0, 1, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3]]\n",
            "[[7, 0, 0, 0, 0, 0], [0, 7, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]]\n",
            "[[7, 0, 0, 0, 0, 0], [0, 7, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]]\n",
            "[[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]]\n",
            "[[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 1, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3]]\n",
            "[[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 1, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3]]\n",
            "[[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3]]\n",
            "[[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2]]\n",
            "[[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "\"\"\"\n",
        "F-score = (TP)/(TP+0.5(FP+FN))\n",
        "\"\"\"\n",
        "def F_score6x6(matrix):\n",
        "  TP_1 = matrix[0][0]\n",
        "  TN_1 = matrix[1][1] + matrix[1][2] + matrix[1][3] + matrix[1][4] + matrix[1][5] + matrix[2][1] + matrix[2][2] + matrix[2][3] + matrix[2][4] + matrix[2][5] + matrix[3][1] + matrix[3][2] + matrix[3][3] + matrix[3][4] + matrix[3][5] + matrix[4][1] + matrix[4][2] + matrix[4][3] + matrix[4][4] + matrix[4][5] + matrix[5][1] + matrix[5][2] + matrix[5][3] + matrix[5][4] + matrix[5][5]\n",
        "  FP_1 = matrix[0][1] + matrix[0][2] + matrix[0][3] + matrix[0][4] + matrix[0][5]\n",
        "  FN_1 = matrix[1][0] + matrix[2][0] + matrix[3][0] + matrix[4][0] + matrix[4][0]\n",
        "\n",
        "  TP_2 = matrix[1][1]\n",
        "  TN_2 = matrix[0][0] + matrix[0][2] + matrix[0][3] + matrix[0][4] + matrix[0][5] + matrix[2][0] + matrix[2][2] + matrix[2][3] + matrix[2][4] + matrix[2][5] + matrix[3][0] + matrix[3][2] + matrix[3][3] + matrix[3][4] + matrix[3][5] + matrix[4][0] + matrix[4][2] + matrix[4][3] + matrix[4][4] + matrix[4][5] + matrix[5][0] + matrix[5][2] + matrix[5][3] + matrix[5][4] + matrix[5][5]\n",
        "  FP_2 = matrix[1][0] + matrix[1][2] + matrix[1][3] + matrix[1][4] + matrix[1][5]\n",
        "  FN_2 = matrix[0][1] + matrix[2][1] + matrix[3][1] + matrix[4][1] + matrix[5][1]\n",
        "\n",
        "  TP_3 = matrix[2][2]\n",
        "  TN_3 = matrix[0][0] + matrix[0][1] + matrix[0][3] + matrix[0][4] + matrix[0][5] + matrix[1][0] + matrix[1][1] + matrix[1][3] + matrix[1][4] + matrix[1][5]  + matrix[3][0] + matrix[3][1] + matrix[3][3] + matrix[3][4] + matrix[3][5] + matrix[4][0] + matrix[4][1] + matrix[4][3] + matrix[4][4] + matrix[4][5] + matrix[5][0] + matrix[5][1] + matrix[5][3] + matrix[5][4] + matrix[5][5]\n",
        "  FP_3 = matrix[2][0] + matrix[2][1] + matrix[2][3] + matrix[2][4] + matrix[2][5]\n",
        "  FN_3 = matrix[0][2] + matrix[1][2] + matrix[3][2] + matrix[4][2] + matrix[5][2]\n",
        "\n",
        "  TP_4 = matrix[3][3]\n",
        "  TN_4 = matrix[0][0] + matrix[0][1] + matrix[0][2] + matrix[0][4] + matrix[0][5] + matrix[1][0] + matrix[1][1] + matrix[1][2] + matrix[1][4] + matrix[1][5] + matrix[2][0] + matrix[2][1] + matrix[2][2] + matrix[2][4] + matrix[2][5]  + matrix[4][0] + matrix[4][1] + matrix[4][2] + matrix[4][4] + matrix[4][5] + matrix[5][0] + matrix[5][1] + matrix[5][2] + matrix[5][4] + matrix[5][5]\n",
        "  FP_4 = matrix[3][0] + matrix[3][1] + matrix[3][2] + matrix[3][4] + matrix[3][5]\n",
        "  FN_4 = matrix[0][3] + matrix[1][3] + matrix[2][3] + matrix[4][3] + matrix[5][3]\n",
        "\n",
        "  TP_5 = matrix[4][4]\n",
        "  TN_5 = matrix[0][0] + matrix[0][1] + matrix[0][2] + matrix[0][3] + matrix[0][5] + matrix[1][0] + matrix[1][1] + matrix[1][2] + matrix[1][3] + matrix[1][5] + matrix[2][0] + matrix[2][1] + matrix[2][2] + matrix[2][3] + matrix[2][5] + matrix[3][0] + matrix[3][1] + matrix[3][2] + matrix[3][3] + matrix[3][5] + matrix[5][0] + matrix[5][1] + matrix[5][2] + matrix[5][3] + matrix[5][5]\n",
        "  FP_5 = matrix[4][0] + matrix[4][1] + matrix[4][2] + matrix[4][3] + matrix[4][5]\n",
        "  FN_5 = matrix[0][4] + matrix[1][4] + matrix[2][4] + matrix[3][4] + matrix[5][4]\n",
        "\n",
        "  TP_6 = matrix[5][5]\n",
        "  TN_6 = matrix[0][0] + matrix[0][1] + matrix[0][2] + matrix[0][3] + matrix[0][4] + matrix[1][0] + matrix[1][1] + matrix[1][2] + matrix[1][3] + matrix[1][4] + matrix[2][0] + matrix[2][1] + matrix[2][2] + matrix[2][3] + matrix[2][4] + matrix[3][0] + matrix[3][1] + matrix[3][2] + matrix[3][3] + matrix[3][4] + matrix[4][0] + matrix[4][1] + matrix[4][2] + matrix[4][3] + matrix[4][4]\n",
        "  FP_6 = matrix[5][0] + matrix[5][1] + matrix[5][2] + matrix[5][3] + matrix[5][4]\n",
        "  FN_6 = matrix[0][5] + matrix[1][5] + matrix[2][5] + matrix[3][5] + matrix[4][5]\n",
        "\n",
        "  if (TP_1 + 0.5 * (FP_1 + FN_1)) != 0:\n",
        "    F_score_1 = (TP_1) / (TP_1 + 0.5 * (FP_1 + FN_1))\n",
        "    round(F_score_1, ndigits=4)\n",
        "  else:\n",
        "    F_score_1 = None\n",
        "\n",
        "  if (TP_2 + 0.5 * (FP_2 + FN_2)) != 0:\n",
        "    F_score_2 = (TP_2) / (TP_2 + 0.5 * (FP_2 + FN_2))\n",
        "    round(F_score_2, ndigits=4)\n",
        "  else:\n",
        "    F_score_2 = None\n",
        "  \n",
        "  if (TP_3 + 0.5 * (FP_3 + FN_3)) != 0:\n",
        "    F_score_3 = (TP_3) / (TP_3 + 0.5 * (FP_3 + FN_3))\n",
        "    round(F_score_3, ndigits=4)\n",
        "  else:\n",
        "    F_score_3 = None\n",
        "  \n",
        "  if (TP_4 + 0.5 * (FP_4 + FN_4)) != 0:\n",
        "    F_score_4 = (TP_4) / (TP_4 + 0.5 * (FP_4 + FN_4))\n",
        "    round(F_score_4, ndigits=4)\n",
        "  else:\n",
        "    F_score_4 = None\n",
        "\n",
        "  if (TP_5 + 0.5 * (FP_5 + FN_5)) != 0:\n",
        "    F_score_5 = (TP_5) / (TP_5 + 0.5 * (FP_5 + FN_5))\n",
        "    round(F_score_5, ndigits=4)\n",
        "  else:\n",
        "    F_score_5 = None\n",
        "\n",
        "  if (TP_6 + 0.5 * (FP_6 + FN_6)) != 0:\n",
        "    F_score_6 = (TP_6) / (TP_6 + 0.5 * (FP_6 + FN_6))\n",
        "    round(F_score_6, ndigits=4)\n",
        "  else:\n",
        "    F_score_6 = None\n",
        "\n",
        "  F_score = [F_score_1, F_score_2, F_score_3, \n",
        "             F_score_4, F_score_5, F_score_6]\n",
        "  return F_score"
      ],
      "metadata": {
        "id": "HTr7iUHJ8sGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "F_score_glass_unshuffled_1 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 7, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_unshuffled_2 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 7, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_unshuffled_3 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 7, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_unshuffled_4 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 7, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_unshuffled_5 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_unshuffled_6 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_unshuffled_7 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_unshuffled_8 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_unshuffled_9 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 2]])\n",
        "F_score_glass_unshuffled_0 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3]])\n",
        "\n",
        "F_score_glass_shuffled_1 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 7, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_shuffled_2 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 7, 0, 0, 1, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_shuffled_3 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 7, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_shuffled_4 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 7, 0, 0, 1, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_shuffled_5 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_shuffled_6 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_shuffled_7 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 1, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_shuffled_8 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3]])\n",
        "F_score_glass_shuffled_9 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2]])\n",
        "F_score_glass_shuffled_0 = F_score6x6([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 3]])\n",
        "\n",
        "print(F_score_glass_unshuffled_1)\n",
        "print(F_score_glass_unshuffled_2)\n",
        "print(F_score_glass_unshuffled_3)\n",
        "print(F_score_glass_unshuffled_4)\n",
        "print(F_score_glass_unshuffled_5)\n",
        "print(F_score_glass_unshuffled_6)\n",
        "print(F_score_glass_unshuffled_7)\n",
        "print(F_score_glass_unshuffled_8)\n",
        "print(F_score_glass_unshuffled_9)\n",
        "print(F_score_glass_unshuffled_0)\n",
        "print(\"\\n\")\n",
        "print(F_score_glass_shuffled_1)\n",
        "print(F_score_glass_shuffled_2)\n",
        "print(F_score_glass_shuffled_3)\n",
        "print(F_score_glass_shuffled_4)\n",
        "print(F_score_glass_shuffled_5)\n",
        "print(F_score_glass_shuffled_6)\n",
        "print(F_score_glass_shuffled_7)\n",
        "print(F_score_glass_shuffled_8)\n",
        "print(F_score_glass_shuffled_9)\n",
        "print(F_score_glass_shuffled_0)"
      ],
      "metadata": {
        "id": "-Y9_8qJuD8O1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9fa015f-baa1-4071-9ab6-8b3a1d5e390b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, None, 1.0]\n",
            "\n",
            "\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 0.9333333333333333, 1.0, 1.0, 0.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 0.9333333333333333, 1.0, 1.0, 0.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 0.9411764705882353, 1.0, 1.0, 0.0, 1.0]\n",
            "[1.0, 0.9411764705882353, 1.0, 1.0, 0.0, 1.0]\n",
            "[1.0, 0.9411764705882353, 1.0, 1.0, 0.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 0.6666666666666666, None, 0.8571428571428571]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "iris_training_one = shuffle(train_merge(iris_ten, iris_two, iris_three, iris_four, iris_five, iris_six, iris_seven, iris_eight, iris_nine), iris_class)\n",
        "iris_training_two = shuffle(train_merge(iris_ten, iris_one, iris_three, iris_four, iris_five, iris_six, iris_seven, iris_eight, iris_nine), iris_class)\n",
        "iris_training_three = shuffle(train_merge(iris_ten, iris_one, iris_two, iris_four, iris_five, iris_six, iris_seven, iris_eight, iris_nine), iris_class)\n",
        "iris_training_four = shuffle(train_merge(iris_ten, iris_one, iris_two, iris_three, iris_five, iris_six, iris_seven, iris_eight, iris_nine), iris_class)\n",
        "iris_training_five = shuffle(train_merge(iris_ten, iris_one, iris_two, iris_three, iris_four, iris_six, iris_seven, iris_eight, iris_nine), iris_class)\n",
        "iris_training_six = shuffle(train_merge(iris_ten, iris_one, iris_two, iris_three, iris_four, iris_five, iris_seven, iris_eight, iris_nine), iris_class)\n",
        "iris_training_seven = shuffle(train_merge(iris_ten, iris_one, iris_two, iris_three, iris_four, iris_five, iris_six, iris_eight, iris_nine), iris_class)\n",
        "iris_training_eight = shuffle(train_merge(iris_ten, iris_one, iris_two, iris_three, iris_four, iris_five, iris_six, iris_seven, iris_nine), iris_class)\n",
        "iris_training_nine = shuffle(train_merge(iris_ten, iris_one, iris_two, iris_three, iris_four, iris_five, iris_six, iris_seven, iris_eight), iris_class)\n",
        "iris_training_ten = shuffle(train_merge(iris_one, iris_two, iris_three, iris_four, iris_five, iris_six, iris_seven, iris_eight, iris_nine), iris_class)\n",
        " \n",
        "print(\"iris first training start\")\n",
        "result_iris_one = Class(iris_one, iris_training_one, iris_class)\n",
        "print(\"iris second training start\")\n",
        "result_iris_two = Class(iris_two, iris_training_two, iris_class)\n",
        "print(\"iris third training start\")\n",
        "result_iris_three = Class(iris_three, iris_training_three, iris_class)\n",
        "print(\"iris fourth training start\")\n",
        "result_iris_four = Class(iris_four, iris_training_four, iris_class)\n",
        "print(\"iris fifth training start\")\n",
        "result_iris_five = Class(iris_five, iris_training_five, iris_class)\n",
        "print(\"iris sixth training start\")\n",
        "result_iris_six = Class(iris_six, iris_training_six, iris_class)\n",
        "print(\"iris seventh training start\")\n",
        "result_iris_seven = Class(iris_seven, iris_training_seven, iris_class)\n",
        "print(\"iris eighth training start\")\n",
        "result_iris_eight = Class(iris_eight, iris_training_eight, iris_class)\n",
        "print(\"iris ninth training start\")\n",
        "result_iris_nine = Class(iris_nine, iris_training_nine, iris_class)\n",
        "print(\"iris tenth training start\")\n",
        "result_iris_ten = Class(iris_ten, iris_training_ten, iris_class)\n",
        " \n",
        "print(zero_loss_func(result_iris_one))\n",
        "print(zero_loss_func(result_iris_two))\n",
        "print(zero_loss_func(result_iris_three))\n",
        "print(zero_loss_func(result_iris_four))\n",
        "print(zero_loss_func(result_iris_five))\n",
        "print(zero_loss_func(result_iris_six))\n",
        "print(zero_loss_func(result_iris_seven))\n",
        "print(zero_loss_func(result_iris_eight))\n",
        "print(zero_loss_func(result_iris_nine))\n",
        "print(zero_loss_func(result_iris_ten))\n",
        "\n",
        "print(confusion_matrix_iris(result_iris_one))\n",
        "print(confusion_matrix_iris(result_iris_two))\n",
        "print(confusion_matrix_iris(result_iris_three))\n",
        "print(confusion_matrix_iris(result_iris_four))\n",
        "print(confusion_matrix_iris(result_iris_five))\n",
        "print(confusion_matrix_iris(result_iris_six))\n",
        "print(confusion_matrix_iris(result_iris_seven))\n",
        "print(confusion_matrix_iris(result_iris_eight))\n",
        "print(confusion_matrix_iris(result_iris_nine))\n",
        "print(confusion_matrix_iris(result_iris_ten))"
      ],
      "metadata": {
        "id": "CtxRQE1VAhuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9e4f20-0ccc-40c7-fdd2-6ebbd462c1b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iris first training start\n",
            "iris second training start\n",
            "iris third training start\n",
            "iris fourth training start\n",
            "iris fifth training start\n",
            "iris sixth training start\n",
            "iris seventh training start\n",
            "iris eighth training start\n",
            "iris ninth training start\n",
            "iris tenth training start\n",
            "[15, 0]\n",
            "[15, 0]\n",
            "[15, 0]\n",
            "[15, 0]\n",
            "[15, 0]\n",
            "[15, 0]\n",
            "[15, 0]\n",
            "[15, 0]\n",
            "[15, 0]\n",
            "[15, 0]\n",
            "[[5, 0, 0], [0, 5, 0], [0, 0, 5]]\n",
            "[[5, 0, 0], [0, 5, 0], [0, 0, 5]]\n",
            "[[5, 0, 0], [0, 5, 0], [0, 0, 5]]\n",
            "[[5, 0, 0], [0, 5, 0], [0, 0, 5]]\n",
            "[[5, 0, 0], [0, 5, 0], [0, 0, 5]]\n",
            "[[5, 0, 0], [0, 5, 0], [0, 0, 5]]\n",
            "[[5, 0, 0], [0, 5, 0], [0, 0, 5]]\n",
            "[[5, 0, 0], [0, 5, 0], [0, 0, 5]]\n",
            "[[5, 0, 0], [0, 5, 0], [0, 0, 5]]\n",
            "[[5, 0, 0], [0, 5, 0], [0, 0, 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "soybean_training_one = shuffle(train_merge(soybean_ten, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine), soybean_class)\n",
        "soybean_training_two = shuffle(train_merge(soybean_ten, soybean_one, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine), soybean_class)\n",
        "soybean_training_three = shuffle(train_merge(soybean_ten, soybean_one, soybean_two, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine), soybean_class)\n",
        "soybean_training_four = shuffle(train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine), soybean_class)\n",
        "soybean_training_five = shuffle(train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_six, soybean_seven, soybean_eight, soybean_nine), soybean_class)\n",
        "soybean_training_six = shuffle(train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_seven, soybean_eight, soybean_nine), soybean_class)\n",
        "soybean_training_seven = shuffle(train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_eight, soybean_nine), soybean_class)\n",
        "soybean_training_eight = shuffle(train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_nine), soybean_class)\n",
        "soybean_training_nine = shuffle(train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight), soybean_class)\n",
        "soybean_training_ten = shuffle(train_merge(soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine), soybean_class)\n",
        " \n",
        "print(\"soybean first training start\")\n",
        "result_soybean_one = Class(soybean_one, soybean_training_one, soybean_class)\n",
        "print(\"soybean second training start\")\n",
        "result_soybean_two = Class(soybean_two, soybean_training_two, soybean_class)\n",
        "print(\"soybean third training start\")\n",
        "result_soybean_three = Class(soybean_three, soybean_training_three, soybean_class)\n",
        "print(\"soybean fourth training start\")\n",
        "result_soybean_four = Class(soybean_four, soybean_training_four, soybean_class)\n",
        "print(\"soybean fifth training start\")\n",
        "result_soybean_five = Class(soybean_five, soybean_training_five, soybean_class)\n",
        "print(\"soybean sixth training start\")\n",
        "result_soybean_six = Class(soybean_six, soybean_training_six, soybean_class)\n",
        "print(\"soybean seventh training start\")\n",
        "result_soybean_seven = Class(soybean_seven, soybean_training_seven, soybean_class)\n",
        "print(\"soybean eighth training start\")\n",
        "result_soybean_eight = Class(soybean_eight, soybean_training_eight, soybean_class)\n",
        "print(\"soybean ninth training start\")\n",
        "result_soybean_nine = Class(soybean_nine, soybean_training_nine, soybean_class)\n",
        "print(\"soybean tenth training start\")\n",
        "result_soybean_ten = Class(soybean_ten, soybean_training_ten, soybean_class)\n",
        "\n",
        "print(confusion_matrix_soybean(result_soybean_one))\n",
        "print(confusion_matrix_soybean(result_soybean_two))\n",
        "print(confusion_matrix_soybean(result_soybean_three))\n",
        "print(confusion_matrix_soybean(result_soybean_four))\n",
        "print(confusion_matrix_soybean(result_soybean_five))\n",
        "print(confusion_matrix_soybean(result_soybean_six))\n",
        "print(confusion_matrix_soybean(result_soybean_seven))\n",
        "print(confusion_matrix_soybean(result_soybean_eight))\n",
        "print(confusion_matrix_soybean(result_soybean_nine))\n",
        "print(confusion_matrix_soybean(result_soybean_ten))\n",
        "\n",
        "print(zero_loss_func(result_soybean_one))\n",
        "print(zero_loss_func(result_soybean_two))\n",
        "print(zero_loss_func(result_soybean_three))\n",
        "print(zero_loss_func(result_soybean_four))\n",
        "print(zero_loss_func(result_soybean_five))\n",
        "print(zero_loss_func(result_soybean_six))\n",
        "print(zero_loss_func(result_soybean_seven))\n",
        "print(zero_loss_func(result_soybean_eight))\n",
        "print(zero_loss_func(result_soybean_nine))\n",
        "print(zero_loss_func(result_soybean_ten))"
      ],
      "metadata": {
        "id": "Zq9XUaI4Am0r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "859ac245-aec1-471a-ec07-2094550ec0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "soybean first training start\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e11c6ae356f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"soybean first training start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mresult_soybean_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoybean_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoybean_training_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoybean_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"soybean second training start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mresult_soybean_two\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoybean_two\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoybean_training_two\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoybean_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Class' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "vote_training_one = shuffle(train_merge(vote_ten, vote_two, vote_three, vote_four, vote_five, vote_six, vote_seven, vote_eight, vote_nine), vote_class)\n",
        "vote_training_two = shuffle(train_merge(vote_ten, vote_one, vote_three, vote_four, vote_five, vote_six, vote_seven, vote_eight, vote_nine), vote_class)\n",
        "vote_training_three = shuffle(train_merge(vote_ten, vote_one, vote_two, vote_four, vote_five, vote_six, vote_seven, vote_eight, vote_nine), vote_class)\n",
        "vote_training_four = shuffle(train_merge(vote_ten, vote_one, vote_two, vote_three, vote_five, vote_six, vote_seven, vote_eight, vote_nine), vote_class)\n",
        "vote_training_five = shuffle(train_merge(vote_ten, vote_one, vote_two, vote_three, vote_four, vote_six, vote_seven, vote_eight, vote_nine), vote_class)\n",
        "vote_training_six = shuffle(train_merge(vote_ten, vote_one, vote_two, vote_three, vote_four, vote_five, vote_seven, vote_eight, vote_nine), vote_class)\n",
        "vote_training_seven = shuffle(train_merge(vote_ten, vote_one, vote_two, vote_three, vote_four, vote_five, vote_six, vote_eight, vote_nine), vote_class)\n",
        "vote_training_eight = shuffle(train_merge(vote_ten, vote_one, vote_two, vote_three, vote_four, vote_five, vote_six, vote_seven, vote_nine), vote_class)\n",
        "vote_training_nine = shuffle(train_merge(vote_ten, vote_one, vote_two, vote_three, vote_four, vote_five, vote_six, vote_seven, vote_eight), vote_class)\n",
        "vote_training_ten = shuffle(train_merge(vote_one, vote_two, vote_three, vote_four, vote_five, vote_six, vote_seven, vote_eight, vote_nine), vote_class)\n",
        " \n",
        "print(\"vote first training start\")\n",
        "result_vote_one = Class(vote_one, vote_training_one, vote_class)\n",
        "print(\"vote second training start\")\n",
        "result_vote_two = Class(vote_two, vote_training_two, vote_class)\n",
        "print(\"vote third training start\")\n",
        "result_vote_three = Class(vote_three, vote_training_three, vote_class)\n",
        "print(\"vote fourth training start\")\n",
        "result_vote_four = Class(vote_four, vote_training_four, vote_class)\n",
        "print(\"vote fifth training start\")\n",
        "result_vote_five = Class(vote_five, vote_training_five, vote_class)\n",
        "print(\"vote sixth training start\")\n",
        "result_vote_six = Class(vote_six, vote_training_six, vote_class)\n",
        "print(\"vote seventh training start\")\n",
        "result_vote_seven = Class(vote_seven, vote_training_seven, vote_class)\n",
        "print(\"vote eighth training start\")\n",
        "result_vote_eight = Class(vote_eight, vote_training_eight, vote_class)\n",
        "print(\"vote ninth training start\")\n",
        "result_vote_nine = Class(vote_nine, vote_training_nine, vote_class)\n",
        "print(\"vote tenth training start\")\n",
        "result_vote_ten = Class(vote_ten, vote_training_ten, vote_class)\n",
        " \n",
        "print(zero_loss_func(result_vote_one))\n",
        "print(zero_loss_func(result_vote_two))\n",
        "print(zero_loss_func(result_vote_three))\n",
        "print(zero_loss_func(result_vote_four))\n",
        "print(zero_loss_func(result_vote_five))\n",
        "print(zero_loss_func(result_vote_six))\n",
        "print(zero_loss_func(result_vote_seven))\n",
        "print(zero_loss_func(result_vote_eight))\n",
        "print(zero_loss_func(result_vote_nine))\n",
        "print(zero_loss_func(result_vote_ten))\n",
        "\n",
        "print(confusion_matrix_vote(result_vote_one))\n",
        "print(confusion_matrix_vote(result_vote_two))\n",
        "print(confusion_matrix_vote(result_vote_three))\n",
        "print(confusion_matrix_vote(result_vote_four))\n",
        "print(confusion_matrix_vote(result_vote_five))\n",
        "print(confusion_matrix_vote(result_vote_six))\n",
        "print(confusion_matrix_vote(result_vote_seven))\n",
        "print(confusion_matrix_vote(result_vote_eight))\n",
        "print(confusion_matrix_vote(result_vote_nine))\n",
        "print(confusion_matrix_vote(result_vote_ten))"
      ],
      "metadata": {
        "id": "uMTYgn0IAqRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "\"\"\"\n",
        "F-score = (TP)/(TP+0.5(FP+FN))\n",
        "\"\"\"\n",
        "def F_score_2x2(matrix):\n",
        "  TP_first = matrix[0][0]\n",
        "  FP_first = matrix[0][1]\n",
        "  FN_first = matrix[1][0]\n",
        "  TN_first = matrix[1][1]\n",
        "\n",
        "  TN_second = matrix[0][0]\n",
        "  FN_second = matrix[0][1]\n",
        "  FP_second = matrix[1][0]\n",
        "  TP_second = matrix[1][1]\n",
        "\n",
        "  F_score_first = (TP_first) / (TP_first + 0.5 * (FP_first + FN_first))\n",
        "  F_score_second = (TP_second) / (TP_second + 0.5 * (FP_second + FN_second))\n",
        "\n",
        "  F_score = [round(F_score_first, ndigits=4), round(F_score_second, ndigits=4)]\n",
        "  return F_score"
      ],
      "metadata": {
        "id": "H_58-nwXBRse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "\"\"\"\n",
        "F-score = (TP)/(TP+0.5(FP+FN))\n",
        "\"\"\"\n",
        "def F_score_3x3(matrix):\n",
        "  TP_1 = matrix[0][0]\n",
        "  TN_1 = matrix[1][1] + matrix[1][2] + matrix[2][1] + matrix[2][2] \n",
        "  FP_1 = matrix[0][1] + matrix[0][2]\n",
        "  FN_1 = matrix[1][0] + matrix[2][0]\n",
        "\n",
        "  TP_2 = matrix[1][1]\n",
        "  TN_2 = matrix[0][0] + matrix[0][2] + matrix[2][0] + matrix[2][2]\n",
        "  FP_2 = matrix[1][0] + matrix[1][2]\n",
        "  FN_2 = matrix[0][1] + matrix[2][1]\n",
        "\n",
        "  TP_3 = matrix[2][2]\n",
        "  TN_3 = matrix[0][0] + matrix[0][1] + matrix[1][0] + matrix[1][1]\n",
        "  FP_3 = matrix[2][0] + matrix[2][1]\n",
        "  FN_3 = matrix[0][2] + matrix[1][2]\n",
        "\n",
        "  F_score_1 = (TP_1) / (TP_1 + 0.5 * (FP_1 + FN_1))\n",
        "  F_score_2 = (TP_2) / (TP_2 + 0.5 * (FP_2 + FN_2))\n",
        "  F_score_3 = (TP_3) / (TP_3 + 0.5 * (FP_3 + FN_3))\n",
        "\n",
        "  F_score = [round(F_score_1, ndigits=4), round(F_score_2, ndigits=4), round(F_score_3, ndigits=4)]\n",
        "  return F_score"
      ],
      "metadata": {
        "id": "FK19tph1o3CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "\"\"\"\n",
        "F-score = (TP)/(TP+0.5(FP+FN))\n",
        "\"\"\"\n",
        "def F_score_4x4(matrix):\n",
        "  TP_1 = matrix[0][0]\n",
        "  TN_1 = matrix[1][1] + matrix[1][2] + matrix[1][3] + matrix[2][1] + matrix[2][2] + matrix[2][3] + matrix[3][1] + matrix[3][2] + matrix[3][3]\n",
        "  FP_1 = matrix[0][1] + matrix[0][2] + matrix[0][3]\n",
        "  FN_1 = matrix[1][0] + matrix[2][0] + matrix[3][0]\n",
        "\n",
        "  TP_2 = matrix[1][1]\n",
        "  TN_2 = matrix[0][0] + matrix[0][2] + matrix[0][3] + matrix[2][0] + matrix[2][2] + matrix[2][3] + matrix[3][0] + matrix[3][2] + matrix[3][3]\n",
        "  FP_2 = matrix[1][0] + matrix[1][2] + matrix[1][3]\n",
        "  FN_2 = matrix[0][1] + matrix[2][1] + matrix[3][1]\n",
        "\n",
        "  TP_3 = matrix[2][2]\n",
        "  TN_3 = matrix[0][0] + matrix[0][1] + matrix[0][3] + matrix[1][0] + matrix[1][1] + matrix[1][3] + matrix[3][0] + matrix[3][1] + matrix[3][2]\n",
        "  FP_3 = matrix[2][0] + matrix[2][1] + matrix[2][3]\n",
        "  FN_3 = matrix[0][2] + matrix[1][2] + matrix[3][2]\n",
        "\n",
        "  TP_4 = matrix[3][3]\n",
        "  TN_4 = matrix[0][0] + matrix[0][1] + matrix[0][2] + matrix[1][0] + matrix[1][1] + matrix[1][2] + matrix[2][0] + matrix[2][1] + matrix[2][2]\n",
        "  FP_4 = matrix[3][0] + matrix[3][1] + matrix[3][2]\n",
        "  FN_4 = matrix[0][3] + matrix[1][3] + matrix[2][3]\n",
        "\n",
        "  F_score_1 = (TP_1) / (TP_1 + 0.5 * (FP_1 + FN_1))\n",
        "  F_score_2 = (TP_2) / (TP_2 + 0.5 * (FP_2 + FN_2))\n",
        "  F_score_3 = (TP_3) / (TP_3 + 0.5 * (FP_3 + FN_3))\n",
        "  F_score_4 = (TP_4) / (TP_4 + 0.5 * (FP_4 + FN_4))\n",
        "\n",
        "  F_score = [round(F_score_1, ndigits=4), round(F_score_2, ndigits=4), round(F_score_3, ndigits=4), round(F_score_4, ndigits=4)]\n",
        "  return F_score"
      ],
      "metadata": {
        "id": "qCIHAvAqDPCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "F_score_breast_cancer_unshuffled_1 = F_score_2x2([[43, 0], [2, 24]])\n",
        "F_score_breast_cancer_unshuffled_2 = F_score_2x2([[43, 0], [2, 24]])\n",
        "F_score_breast_cancer_unshuffled_3 = F_score_2x2([[46, 0], [0, 24]])\n",
        "F_score_breast_cancer_unshuffled_4 = F_score_2x2([[46, 0], [0, 24]])\n",
        "F_score_breast_cancer_unshuffled_5 = F_score_2x2([[45, 1], [1, 23]])\n",
        "F_score_breast_cancer_unshuffled_6 = F_score_2x2([[46, 0], [0, 24]])\n",
        "F_score_breast_cancer_unshuffled_7 = F_score_2x2([[46, 0], [0, 24]])\n",
        "F_score_breast_cancer_unshuffled_8 = F_score_2x2([[45, 0], [1, 24]])\n",
        "F_score_breast_cancer_unshuffled_9 = F_score_2x2([[45, 0], [1, 24]])\n",
        "F_score_breast_cancer_unshuffled_10 = F_score_2x2([[46, 0], [0, 25]])\n",
        "\n",
        "F_score_breast_cancer_shuffled_1 = F_score_2x2([[43, 0], [2, 24]])\n",
        "F_score_breast_cancer_shuffled_2 = F_score_2x2([[43, 0], [2, 24]])\n",
        "F_score_breast_cancer_shuffled_3 = F_score_2x2([[45, 0], [1, 24]])\n",
        "F_score_breast_cancer_shuffled_4 = F_score_2x2([[46, 0], [0, 24]])\n",
        "F_score_breast_cancer_shuffled_5 = F_score_2x2([[45, 0], [1, 24]])\n",
        "F_score_breast_cancer_shuffled_6 = F_score_2x2([[46, 0], [0, 24]])\n",
        "F_score_breast_cancer_shuffled_7 = F_score_2x2([[46, 0], [0, 24]])\n",
        "F_score_breast_cancer_shuffled_8 = F_score_2x2([[43, 0], [3, 24]])\n",
        "F_score_breast_cancer_shuffled_9 = F_score_2x2([[44, 0], [2, 24]])\n",
        "F_score_breast_cancer_shuffled_10 = F_score_2x2([[46, 0], [0, 25]])"
      ],
      "metadata": {
        "id": "B_EtUbU2CnaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "F_score_vote_unshuffled_1 = F_score_2x2([[15, 0], [1, 26]])\n",
        "F_score_vote_unshuffled_2 = F_score_2x2([[14, 2], [2, 24]])\n",
        "F_score_vote_unshuffled_3 = F_score_2x2([[17, 0], [0, 26]])\n",
        "F_score_vote_unshuffled_4 = F_score_2x2([[17, 0], [0, 27]])\n",
        "F_score_vote_unshuffled_5 = F_score_2x2([[16, 0], [1, 27]])\n",
        "F_score_vote_unshuffled_6 = F_score_2x2([[17, 1], [0, 26]])\n",
        "F_score_vote_unshuffled_7 = F_score_2x2([[16, 0], [1, 27]])\n",
        "F_score_vote_unshuffled_8 = F_score_2x2([[16, 0], [1, 27]])\n",
        "F_score_vote_unshuffled_9 = F_score_2x2([[16, 3], [1, 24]])\n",
        "F_score_vote_unshuffled_0 = F_score_2x2([[17, 1], [0, 26]])\n",
        "\n",
        "F_score_vote_shuffled_1 = F_score_2x2([[15, 0], [1, 26]])\n",
        "F_score_vote_shuffled_2 = F_score_2x2([[16, 4], [0, 22]])\n",
        "F_score_vote_shuffled_3 = F_score_2x2([[17, 0], [0, 26]])\n",
        "F_score_vote_shuffled_4 = F_score_2x2([[16, 0], [1, 27]])\n",
        "F_score_vote_shuffled_5 = F_score_2x2([[16, 0], [1, 27]])\n",
        "F_score_vote_shuffled_6 = F_score_2x2([[17, 1], [0, 26]])\n",
        "F_score_vote_shuffled_7 = F_score_2x2([[16, 0], [1, 27]])\n",
        "F_score_vote_shuffled_8 = F_score_2x2([[16, 0], [1, 27]])\n",
        "F_score_vote_shuffled_9 = F_score_2x2([[16, 3], [1, 24]])\n",
        "F_score_vote_shuffled_0 = F_score_2x2([[17, 1], [0, 26]])"
      ],
      "metadata": {
        "id": "uOAVo4GjnPmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steven Ohms\n",
        "F_score_iris_unshuffled_1 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_unshuffled_2 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_unshuffled_3 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_unshuffled_4 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_unshuffled_5 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_unshuffled_6 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_unshuffled_7 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_unshuffled_8 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_unshuffled_9 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_unshuffled_0 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "\n",
        "F_score_iris_shuffled_1 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_shuffled_2 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_shuffled_3 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_shuffled_4 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_shuffled_5 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_shuffled_6 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_shuffled_7 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_shuffled_8 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_shuffled_9 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])\n",
        "F_score_iris_shuffled_0 = F_score_3x3([[5, 0, 0], [0, 5, 0], [0, 0, 5]])"
      ],
      "metadata": {
        "id": "gtbH7YRUBxCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gak Roppongi\n",
        "F_score_soybean_unshuffled_1 = F_score_4x4([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "F_score_soybean_unshuffled_2 = F_score_4x4([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "F_score_soybean_unshuffled_3 = F_score_4x4([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "F_score_soybean_unshuffled_4 = F_score_4x4([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 0], [0, 0, 1, 2]])\n",
        "F_score_soybean_unshuffled_5 = F_score_4x4([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 2]])\n",
        "F_score_soybean_unshuffled_6 = F_score_4x4([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 2]])\n",
        "F_score_soybean_unshuffled_7 = F_score_4x4([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 2]])\n",
        "F_score_soybean_unshuffled_8 = F_score_4x4([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 2]])\n",
        "F_score_soybean_unshuffled_9 = F_score_4x4([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 2]])\n",
        "F_score_soybean_unshuffled_0 = F_score_4x4([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 2]])\n",
        "\n",
        "F_score_soybean_shuffled_1 = F_score_4x4([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1]])\n",
        "F_score_soybean_shuffled_2 = F_score_4x4([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1]])\n",
        "F_score_soybean_shuffled_3 = F_score_4x4([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1]])\n",
        "F_score_soybean_shuffled_4 = F_score_4x4([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 2]])\n",
        "F_score_soybean_shuffled_5 = F_score_4x4([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 2]])\n",
        "F_score_soybean_shuffled_6 = F_score_4x4([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 2]])\n",
        "F_score_soybean_shuffled_7 = F_score_4x4([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 2]])\n",
        "F_score_soybean_shuffled_8 = F_score_4x4([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 0], [0, 0, 1, 2]])\n",
        "F_score_soybean_shuffled_9 = F_score_4x4([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 2]])\n",
        "F_score_soybean_shuffled_0 = F_score_4x4([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 2]])"
      ],
      "metadata": {
        "id": "OSHh3qmpDGsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pC-Z2dJCutbY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}