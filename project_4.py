# -*- coding: utf-8 -*-
"""Project 4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jHpgc1c6imtEooEKdhGf1deKbtbMfB2V
"""

# Project 2
#Overall for each part (coding, writing, and video segements) were equally distributed between the two of us Steven Ohms and Gak Roppongi
#A majority of the time we were working together in person, and constantly looked over/edited each others code
#Note whenever a cell says one of our names, we assume a majority of the work in the cell was done by that individual, but was still split between the two of us

import numpy as np
import pandas as pd
import math
import random

#Gak Roppongi
#importing the data sets, and deal with the missing values and one-hot encoding
#Breast cancer, Glass dataset, Soybean datasets are classification
#Abalone, Computer, Forest datasets are regression

breast_cancer_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', 
                                    names = ["radius", "texture", "perimeter", "area", "smoothness", "compactness", "concavity", "concave points", 
                                             "symmetry", "fractal dimension", "class"])
#"symmetry" may be label-encoded, but I'm not sure 
#the sum of number of instances of 1 ... 7 in "class" attribute is 699 = number of all instances
#therefore, "class" should be the categorical attribute
#no categorical value is missed
#There are 16 instances in Groups 1 to 6 that contain a single missing (i.e., unavailable) attribute value, now denoted by "?".  
#699 * 11

glass_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data', 
                            names = ["id", "RI", "Na", "Mg", "Al", "Si", "K", "Ca", "Ba",  "Fe", "type of glass"])
#the "type of glass" is label-encoded
#everything else is numerical attribute
#Number of Attributes: 10 (including an Id#) plus the class attribute -- all attributes are continuously valued
#the last attribute, "class", has label-encoded for 7 categories from 1~7
#no missing attributes values
#214 * 11 

soybean_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data', 
                              names = ["date", "plant-stand", "precip", "temp", "hail", "crop-hist", "area-damaged", 
                                       "severity", "seed-tmt", "germination", "plant-growth", "leaves", "leafspots-halo", 
                                       "leafspots-marg", "leadspot-size", "leaf-shread", "lead-malf", "lead-mild", "stem", 
                                       "lodging", "stem-cankers", "fruiting-bodies", "external-decay", "mycelium", "int-discolor", 
                                       "sclerotia", "fruit-pods", "fruit-spots", "seed", "mold-growth", "seed-discolor", "seed-size", "shriveling", "roots"])
#many of the attributes are string data type and label-encoded
#no missing attribute values
#all values have been normalized
#"roots" is the categorical value
#46 * 36

abalone_dataset = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data",
                              names = ["sex", "length", "diameter", "height", "whole weight", "shucked weight", "viscera weight", "shell weight", "rings"])

#no missing values
#ring is the class
#4177 instances
#one hot encode sex, then hamming distance
#8 features


computer_dataset = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/machine.data",
                               names = ["vendor name", "model name", "MYCT", "MMIN", "MMAX", "CACH", "CHMIN", "CHMAX", "PRP", "ERP"])
#no missing values
#PRP is the class here
#vendor name and model name are string data
#everything else is numerical data
#209 instances
#10 features
#remove vendor/model name

forest_dataset = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv",
                             names = ["X", "Y", "month", "day", "FFMC", "DMC", "DC", "ISI", "temp", "RH", "wind", "rain", "area"])
#no missing values
#517 instances
#12 features
#area - the burned area of the forest (in ha): 0.00 to 1090.84 
#(this output variable is very skewed towards 0.0, 
#thus it may make sense to model with the logarithm transform).
#area is the class here
#month and day are cyclical, so for day, distance between sunday and moday would be 1, or 6, but 1 is lower

breast_class = np.array([2, 4])
glass_class = np.array([1, 2, 3, 5, 6, 7])
soybean_class = np.array([1, 2, 3, 4])
#soybean_class = np.array(['D1', 'D2', 'D3', 'D4'])
abalone_class = np.array(["rings"])
computer_class = np.array(["PRP"])
forest_class = np.array(["area"])

#do not need model name or vendor name in the computer dataset
#row dataset does not contain any data, just attribute names
computer_dataset = computer_dataset.drop("vendor name", axis=1);
computer_dataset = computer_dataset.drop("model name", axis=1);

forest_dataset = forest_dataset.drop(index = 0)


soybean_dataset = soybean_dataset.reset_index(drop = True)

breast_cancer_dataset.sample(frac=1)
breast_cancer_dataset.sort_values(by=["class"], inplace=True)
glass_dataset.sort_values(by=["type of glass"], inplace=True)
soybean_dataset.sort_values(by=["roots"], inplace=True)
abalone_dataset.sort_values(by=["rings"], inplace=True)
computer_dataset.sort_values(by=["PRP"], inplace=True)
forest_dataset.sort_values(by=["area"], inplace=True)

#turns the abalone_dataset into something easier to read, and turns the days of the week and months into integers so distance can be calculated
breast_cancer_dataset = breast_cancer_dataset.replace(["?"], np.nan)

abalone_dataset["sex"] = abalone_dataset["sex"].replace(["M"], ["Male"])
abalone_dataset["sex"] = abalone_dataset["sex"].replace(["F"], ["Female"])
abalone_dataset["sex"] = abalone_dataset["sex"].replace(["I"], ["Infant"])

forest_dataset["month"] = forest_dataset["month"].replace(["jan"], [1])
forest_dataset["month"] = forest_dataset["month"].replace(["feb"], [2])
forest_dataset["month"] = forest_dataset["month"].replace(["mar"], [3])
forest_dataset["month"] = forest_dataset["month"].replace(["apr"], [4])
forest_dataset["month"] = forest_dataset["month"].replace(["may"], [5])
forest_dataset["month"] = forest_dataset["month"].replace(["jun"], [6])
forest_dataset["month"] = forest_dataset["month"].replace(["jul"], [7])
forest_dataset["month"] = forest_dataset["month"].replace(["aug"], [8])
forest_dataset["month"] = forest_dataset["month"].replace(["sep"], [9])
forest_dataset["month"] = forest_dataset["month"].replace(["oct"], [10])
forest_dataset["month"] = forest_dataset["month"].replace(["nov"], [11])
forest_dataset["month"] = forest_dataset["month"].replace(["dec"], [12])

forest_dataset["day"] = forest_dataset["day"].replace(["mon"], [1])
forest_dataset["day"] = forest_dataset["day"].replace(["tue"], [2])
forest_dataset["day"] = forest_dataset["day"].replace(["wed"], [3])
forest_dataset["day"] = forest_dataset["day"].replace(["thu"], [4])
forest_dataset["day"] = forest_dataset["day"].replace(["fri"], [5])
forest_dataset["day"] = forest_dataset["day"].replace(["sat"], [6])
forest_dataset["day"] = forest_dataset["day"].replace(["sun"], [7])

breast_cancer_dataset['concavity'] = breast_cancer_dataset['concavity'].astype(float)

computer_dataset["MYCT"] = computer_dataset["MYCT"].astype(float)
computer_dataset["MMIN"] = computer_dataset["MMIN"].astype(float)
computer_dataset["MMAX"] = computer_dataset["MMAX"].astype(float)
computer_dataset["CACH"] = computer_dataset["CACH"].astype(float)
computer_dataset["CHMIN"] = computer_dataset["CHMIN"].astype(float)
computer_dataset["CHMAX"] = computer_dataset["CHMAX"].astype(float)
computer_dataset["PRP"] = computer_dataset["PRP"].astype(float)
computer_dataset["ERP"] = computer_dataset["ERP"].astype(float)

forest_dataset = forest_dataset.astype(float)

#Steven Ohms
#this section of the code will turn all the vallues in the dataframes into numeric values
#all values are saved as str in the first place
#we will turn them into numeric

#glass, iris, and  soybean don't have the missing value

#this function does mean-filling of the dataset for the missing value
#df is the dataframe we manipulate
#attribute is the attribute in the dataframe we are manipulating

def mean_filling(df, attribute):
  mean = df[attribute].mean()
  df[attribute] = df[attribute].replace(np.nan, mean)
  return df

breast_cancer_dataset = mean_filling(breast_cancer_dataset, "concavity")

#Gak Roppongi
#this function implements the one-hot encoding
#df is the dataset we manipulate
#attribute is the attribute that we manipulate in str
#returns the one-hot encoded dataframe

def onehot(df, attribute):
  # Get one hot encoding of columns B
  one_hot = pd.get_dummies(df[attribute])
  # Drop column B as it is now encoded
  df = df.drop(attribute,axis = 1)
  # Join the encoded df
  df = df.join(one_hot)
  return df

abalone_dataset = onehot(abalone_dataset, "sex")
glass_dataset = onehot(glass_dataset, "type of glass")
soybean_dataset = onehot(soybean_dataset, "roots")
breast_cancer_dataset = onehot(breast_cancer_dataset, "class")

soybean_dataset.rename(columns = {'D1':1}, inplace = True)
soybean_dataset.rename(columns = {'D2':2}, inplace = True)
soybean_dataset.rename(columns = {'D3':3}, inplace = True)
soybean_dataset.rename(columns = {'D4':4}, inplace = True)

"""
Steven Ohms
this section of the code will normalize the dataset
the function will take df and attribute
df is the dataframe we are manipulating
attribute is the attribute we are normalizing
this is the z-score/standard score normalization
normalized value = Xi - Xmean / (standard deviation)
"""

breast_normalized_class = ["radius", "texture", "perimeter", "area", "smoothness", 
                           "compactness", "concavity", "concave points", "symmetry", "fractal dimension"]
glass_normalized_class = ["id", "RI", "Na", "Mg", "Al", "Si", "K", "Ca", "Ba", "Fe"]
abalone_normalized_class = ["length", "diameter", "height", "whole weight", "shucked weight", "viscera weight", "shell weight"]
computer_normalized_class = ["MYCT", "MMIN", "MMAX", "CACH", "CHMIN", "CHMAX", "PRP", "ERP"]
forest_normalized_class = ["X", "Y", "FFMC", "DMC", "DC", "ISI", "temp", "RH", "wind", "rain", "area"]

def normalize(df, attribute):
  column = df[attribute]
  column = (column - column.mean()) / column.std()
  df[attribute] = column
  return df


for a in breast_normalized_class:
  breast_cancer_dataset = normalize(breast_cancer_dataset, a)

for b in glass_normalized_class:
  glass_dataset = normalize(glass_dataset, b)

for c in abalone_normalized_class:
  abalone_dataset = normalize(abalone_dataset, c)

for d in computer_normalized_class:
  computer_dataset = normalize(computer_dataset, d)

for e in forest_normalized_class:
  forest_dataset = normalize(forest_dataset, e)

#merges all the portions together to create a testing set
 
def train_merge(one, two, three, four, five, six, seven, eight, nine):
  final_train = pd.merge(one, two, how="outer")
  final_train = pd.merge(final_train, three, how="outer")
  final_train = pd.merge(final_train, four, how="outer")
  final_train = pd.merge(final_train, five, how="outer")
  final_train = pd.merge(final_train, six, how="outer")
  final_train = pd.merge(final_train, seven, how="outer")
  final_train = pd.merge(final_train, eight, how="outer")
  final_train = pd.merge(final_train, nine, how="outer")
 
  return final_train

def set_split(dataset):
  i = 0
  j = 0
  HP_set = dataset.iloc[0:0]
  one = dataset.iloc[0:0]
  two = dataset.iloc[0:0]
  three = dataset.iloc[0:0]
  four = dataset.iloc[0:0]
  five = dataset.iloc[0:0]
  six = dataset.iloc[0:0]
  seven = dataset.iloc[0:0]
  eight = dataset.iloc[0:0]
  nine = dataset.iloc[0:0]
  ten = dataset.iloc[0:0]
  
  for i in range(dataset.shape[0]):
    if (i % 10 == 0):
      
      HP_set = pd.merge(HP_set, dataset.iloc[i:(i+1)], how="outer")
    else:
      h = j % 10  
      if h == 0:
        one = pd.merge(one, dataset.iloc[i:(i+1)], how="outer")
      if h == 1:
        two = pd.merge(two, dataset.iloc[i:(i+1)], how="outer")
      if h == 2:
        three = pd.merge(three, dataset.iloc[i:(i+1)], how="outer")
      if h == 3:
        four = pd.merge(four, dataset.iloc[i:(i+1)], how="outer")
      if h == 4:
        five = pd.merge(five, dataset.iloc[i:(i+1)], how="outer")
      if h == 5:
        six = pd.merge(six, dataset.iloc[i:(i+1)], how="outer")
      if h == 6:
        seven = pd.merge(seven, dataset.iloc[i:(i+1)], how="outer")
      if h == 7:
        eight = pd.merge(eight, dataset.iloc[i:(i+1)], how="outer")
      if h == 8:
        nine = pd.merge(nine, dataset.iloc[i:(i+1)], how="outer")
      if h == 9:
        ten = pd.merge(ten, dataset.iloc[i:(i+1)], how="outer")
      j = j + 1
 
  return HP_set, one, two, three, four, five, six, seven, eight, nine, ten

glass_HP, glass_1, glass_2, glass_3, glass_4, glass_5, glass_6, glass_7, glass_8, glass_9, glass_10 = set_split(glass_dataset)
soybean_HP, soybean_1, soybean_2, soybean_3, soybean_4, soybean_5, soybean_6, soybean_7, soybean_8, soybean_9, soybean_10 = set_split(soybean_dataset)
breast_HP, breast_1, breast_2, breast_3, breast_4, breast_5, breast_6, breast_7, breast_8, breast_9, breast_10 = set_split(breast_cancer_dataset)
forest_HP, forest_1, forest_2, forest_3, forest_4, forest_5, forest_6, forest_7, forest_8, forest_9, forest_10 = set_split(forest_dataset)
computer_HP, computer_1, computer_2, computer_3, computer_4, computer_5, computer_6, computer_7, computer_8, computer_9, computer_10 = set_split(computer_dataset)
abalone_HP, abalone_1, abalone_2, abalone_3, abalone_4, abalone_5, abalone_6, abalone_7, abalone_8, abalone_9, abalone_10 = set_split(abalone_dataset)

breast_training_1 = train_merge(breast_10, breast_2, breast_3, breast_4, breast_5, breast_6, breast_7, breast_8, breast_9)
breast_training_2 = train_merge(breast_10, breast_1, breast_3, breast_4, breast_5, breast_6, breast_7, breast_8, breast_9)
breast_training_3 = train_merge(breast_10, breast_1, breast_2, breast_4, breast_5, breast_6, breast_7, breast_8, breast_9)
breast_training_4 = train_merge(breast_10, breast_1, breast_2, breast_3, breast_5, breast_6, breast_7, breast_8, breast_9)
breast_training_5 = train_merge(breast_10, breast_1, breast_2, breast_3, breast_4, breast_6, breast_7, breast_8, breast_9)
breast_training_6 = train_merge(breast_10, breast_1, breast_2, breast_3, breast_4, breast_5, breast_7, breast_8, breast_9)
breast_training_7 = train_merge(breast_10, breast_1, breast_2, breast_3, breast_4, breast_5, breast_6, breast_8, breast_9)
breast_training_8 = train_merge(breast_10, breast_1, breast_2, breast_3, breast_4, breast_5, breast_6, breast_7, breast_9)
breast_training_9 = train_merge(breast_10, breast_1, breast_2, breast_3, breast_4, breast_5, breast_6, breast_7, breast_8)
breast_training_10 = train_merge(breast_1, breast_2, breast_3, breast_4, breast_5, breast_6, breast_7, breast_8, breast_9)

soybean_training_1 = train_merge(soybean_10, soybean_2, soybean_3, soybean_4, soybean_5, soybean_6, soybean_7, soybean_8, soybean_9)
soybean_training_2 = train_merge(soybean_10, soybean_1, soybean_3, soybean_4, soybean_5, soybean_6, soybean_7, soybean_8, soybean_9)
soybean_training_3 = train_merge(soybean_10, soybean_1, soybean_2, soybean_4, soybean_5, soybean_6, soybean_7, soybean_8, soybean_9)
soybean_training_4 = train_merge(soybean_10, soybean_1, soybean_2, soybean_3, soybean_5, soybean_6, soybean_7, soybean_8, soybean_9)
soybean_training_5 = train_merge(soybean_10, soybean_1, soybean_2, soybean_3, soybean_4, soybean_6, soybean_7, soybean_8, soybean_9)
soybean_training_6 = train_merge(soybean_10, soybean_1, soybean_2, soybean_3, soybean_4, soybean_5, soybean_7, soybean_8, soybean_9)
soybean_training_7 = train_merge(soybean_10, soybean_1, soybean_2, soybean_3, soybean_4, soybean_5, soybean_6, soybean_8, soybean_9)
soybean_training_8 = train_merge(soybean_10, soybean_1, soybean_2, soybean_3, soybean_4, soybean_5, soybean_6, soybean_7, soybean_9)
soybean_training_9 = train_merge(soybean_10, soybean_1, soybean_2, soybean_3, soybean_4, soybean_5, soybean_6, soybean_7, soybean_8)
soybean_training_10 = train_merge(soybean_1, soybean_2, soybean_3, soybean_4, soybean_5, soybean_6, soybean_7, soybean_8, soybean_9)

glass_training_1 = train_merge(glass_10, glass_2, glass_3, glass_4, glass_5, glass_6, glass_7, glass_8, glass_9)
glass_training_2 = train_merge(glass_10, glass_1, glass_3, glass_4, glass_5, glass_6, glass_7, glass_8, glass_9)
glass_training_3 = train_merge(glass_10, glass_1, glass_2, glass_4, glass_5, glass_6, glass_7, glass_8, glass_9)
glass_training_4 = train_merge(glass_10, glass_1, glass_2, glass_3, glass_5, glass_6, glass_7, glass_8, glass_9)
glass_training_5 = train_merge(glass_10, glass_1, glass_2, glass_3, glass_4, glass_6, glass_7, glass_8, glass_9)
glass_training_6 = train_merge(glass_10, glass_1, glass_2, glass_3, glass_4, glass_5, glass_7, glass_8, glass_9)
glass_training_7 = train_merge(glass_10, glass_1, glass_2, glass_3, glass_4, glass_5, glass_6, glass_8, glass_9)
glass_training_8 = train_merge(glass_10, glass_1, glass_2, glass_3, glass_4, glass_5, glass_6, glass_7, glass_9)
glass_training_9 = train_merge(glass_10, glass_1, glass_2, glass_3, glass_4, glass_5, glass_6, glass_7, glass_8)
glass_training_10 = train_merge(glass_1, glass_2, glass_3, glass_4, glass_5, glass_6, glass_7, glass_8, glass_9)

abalone_training_1 = train_merge(abalone_10, abalone_2, abalone_3, abalone_4, abalone_5, abalone_6, abalone_7, abalone_8, abalone_9)
abalone_training_2 = train_merge(abalone_10, abalone_1, abalone_3, abalone_4, abalone_5, abalone_6, abalone_7, abalone_8, abalone_9)
abalone_training_3 = train_merge(abalone_10, abalone_1, abalone_2, abalone_4, abalone_5, abalone_6, abalone_7, abalone_8, abalone_9)
abalone_training_4 = train_merge(abalone_10, abalone_1, abalone_2, abalone_3, abalone_5, abalone_6, abalone_7, abalone_8, abalone_9)
abalone_training_5 = train_merge(abalone_10, abalone_1, abalone_2, abalone_3, abalone_4, abalone_6, abalone_7, abalone_8, abalone_9)
abalone_training_6 = train_merge(abalone_10, abalone_1, abalone_2, abalone_3, abalone_4, abalone_5, abalone_7, abalone_8, abalone_9)
abalone_training_7 = train_merge(abalone_10, abalone_1, abalone_2, abalone_3, abalone_4, abalone_5, abalone_6, abalone_8, abalone_9)
abalone_training_8 = train_merge(abalone_10, abalone_1, abalone_2, abalone_3, abalone_4, abalone_5, abalone_6, abalone_7, abalone_9)
abalone_training_9 = train_merge(abalone_10, abalone_1, abalone_2, abalone_3, abalone_4, abalone_5, abalone_6, abalone_7, abalone_8)
abalone_training_10 = train_merge(abalone_1, abalone_2, abalone_3, abalone_4, abalone_5, abalone_6, abalone_7, abalone_8, abalone_9)

computer_training_1 = train_merge(computer_10, computer_2, computer_3, computer_4, computer_5, computer_6, computer_7, computer_8, computer_9)
computer_training_2 = train_merge(computer_10, computer_1, computer_3, computer_4, computer_5, computer_6, computer_7, computer_8, computer_9)
computer_training_3 = train_merge(computer_10, computer_1, computer_2, computer_4, computer_5, computer_6, computer_7, computer_8, computer_9)
computer_training_4 = train_merge(computer_10, computer_1, computer_2, computer_3, computer_5, computer_6, computer_7, computer_8, computer_9)
computer_training_5 = train_merge(computer_10, computer_1, computer_2, computer_3, computer_4, computer_6, computer_7, computer_8, computer_9)
computer_training_6 = train_merge(computer_10, computer_1, computer_2, computer_3, computer_4, computer_5, computer_7, computer_8, computer_9)
computer_training_7 = train_merge(computer_10, computer_1, computer_2, computer_3, computer_4, computer_5, computer_6, computer_8, computer_9)
computer_training_8 = train_merge(computer_10, computer_1, computer_2, computer_3, computer_4, computer_5, computer_6, computer_7, computer_9)
computer_training_9 = train_merge(computer_10, computer_1, computer_2, computer_3, computer_4, computer_5, computer_6, computer_7, computer_8)
computer_training_10 = train_merge(computer_1, computer_2, computer_3, computer_4, computer_5, computer_6, computer_7, computer_8, computer_9)

forest_training_1 = train_merge(forest_10, forest_2, forest_3, forest_4, forest_5, forest_6, forest_7, forest_8, forest_9)
forest_training_2 = train_merge(forest_10, forest_1, forest_3, forest_4, forest_5, forest_6, forest_7, forest_8, forest_9)
forest_training_3 = train_merge(forest_10, forest_1, forest_2, forest_4, forest_5, forest_6, forest_7, forest_8, forest_9)
forest_training_4 = train_merge(forest_10, forest_1, forest_2, forest_3, forest_5, forest_6, forest_7, forest_8, forest_9)
forest_training_5 = train_merge(forest_10, forest_1, forest_2, forest_3, forest_4, forest_6, forest_7, forest_8, forest_9)
forest_training_6 = train_merge(forest_10, forest_1, forest_2, forest_3, forest_4, forest_5, forest_7, forest_8, forest_9)
forest_training_7 = train_merge(forest_10, forest_1, forest_2, forest_3, forest_4, forest_5, forest_6, forest_8, forest_9)
forest_training_8 = train_merge(forest_10, forest_1, forest_2, forest_3, forest_4, forest_5, forest_6, forest_7, forest_9)
forest_training_9 = train_merge(forest_10, forest_1, forest_2, forest_3, forest_4, forest_5, forest_6, forest_7, forest_8)
forest_training_10 = train_merge(forest_1, forest_2, forest_3, forest_4, forest_5, forest_6, forest_7, forest_8, forest_9)

"""
0-1 loss function
param: array of results. 
results are array of predicted and actual response of each instance
"""
def zero_loss_func(actual_matrix, pred_matrix):
  correct = 0
  incorrect = 0
  for i in range(len(actual_matrix)):
    if actual_matrix[i] == pred_matrix[i][0]:
      correct = correct+1
    elif actual_matrix[i] != pred_matrix[i][0]:
      incorrect = incorrect + 1
  
  return [correct, incorrect]

def cros_entropy_func_multivariate(actual_outputs, predicted_outputs, class_names):
  for i in range(len(class_names)):
    if actual_outputs == class_names[i]:
      cross_entropy =  math.log2(predicted_outputs[1][i])

  return cross_entropy

"""
takes the matrix as the argument
calculates the MSE out of the matrix
"""
def Mean_Squared_Error(matrix):
  MSE = 0
  n = len(matrix)
  for i in range(n):
    MSE += (matrix[i][0] - matrix[i][1])**2
  
  MSE = MSE/n
  return MSE

"""
takes the matrix as the argument
calculates the SSR out of the matrix
actual_outputs is the list of actual outputs
predicted_outputs is the list of predicted outputs
"""
def SSR(actual_outputs, predicted_outputs):
  SSR = 0
  for i in range(len(actual_outputs)):
    SSR += (actual_outputs[i] - predicted_outputs[i])**2

  return SSR

"""
takes the matrix as the argument
calculates the SR out of the matrix
actual_outputs is the list of actual outputs
predicted_outputs is the list of predicted outputs
"""
def sum_of_residual(actual_outputs, predicted_outputs):
  SR = 0
  for i in range(len(actual_outputs)):
    SR += (actual_outputs[i] - predicted_outputs[i])

  return SR

"""
this is for the hidden layers
we chose logistic regression
"""
def sigmoid_activation(input):
  try:
    sigmoid_activation_output = 1 / (1 + math.exp(- input))
  except OverflowError:
    sigmoid_activation_output = float('inf')
  return sigmoid_activation_output
 
#sigmoid_activation(5)
 
"""
for regression output
param: a number
"""
def linear_activation(input):
  return input
 
"""
for classification output
param: list of numbers, float
"""
def softmax_activation(input_list, class_name):
  current_max = 0
  current_max_pos = 0
  denom = 0
  softmax_output_list = [0 for k in range(len(input_list))]
  for j in range(len(input_list)):
    denom = denom + math.exp(input_list[j])
  for i in range(len(input_list)):
    current_test = math.exp(input_list[i])/denom
    softmax_output_list[i] = current_test
    if i == 0:
      current_max = current_test
      current_max_pos = i
      
    elif current_max < current_test:
      current_max = current_test
      current_max_pos = i
      
      #print("this is the softmax_output_list  ", class_name[current_max_pos], softmax_output_list, current_max_pos)
  return class_name[current_max_pos], softmax_output_list, current_max_pos
 

def cross_entropy_loss(actual_outputs, predicted_outputs, class_names):
  for i in range(len(class_names)):
    if actual_outputs == class_names[i]:
      cross_entropy =  -1 * math.log2(predicted_outputs[1][i])
  return cross_entropy


j = [1, 2, 3]
class_name = [1, 2, 3]
predicted_soft = softmax_activation(j, class_name)
print(predicted_soft)
#print(cross_entropy_loss(3, predicted_soft, class_name))

def binary_crossover(x_network, u_network, p_crossover, weight_length):
  #This block of code ensures that atleast one point gets randomly assigned
  rand_pos = random.randint(0, weight_length-1)
  #print(rand_pos)
  for i in range(len(x_network)):
    #print(rand_pos)
    if len(x_network[i][0]["weights"]) > rand_pos:
      x_network[i][0]["weights"][rand_pos] = u_network[i][0]["weights"][rand_pos]
      break
    rand_pos -= (len(x_network[i][0]["weights"]))
    
 
  #This block of code randomly performs crossover on the x_network
  for i in range(len(x_network)):
    for j in range(len(x_network[i][0]["weights"])):
      prob =  random.random()
      if prob < p_crossover: #Notes show that the if the generated probability is less than p_crossover, then we crossover
        x_network[i][0]["weights"][j] = u_network[i][0]["weights"][j]
 
  return x_network
 
def get_number_of_weights(network):
  length = 0
  for layer in network:
    for neuron in layer:
      length += len(neuron["weights"])
 
  return length

"""
this function takes the derivative of the error term and the weight.
then updates the weight according to the derivative.
iterates this step until it hits the maximum iteration or the calculated step size reaches the minimum step size
this function does this only for a 
"""
def gradient_descent(dftrain, network, layer_depth, step_size, neuron_index, neuron, actual_outputs, class_name, LR, max_ite, min_step, regression):
  #print("gradient_descent")
  new_network = network
  weights = neuron[0]["weights"] #store the weights list in the dummy variable "weights"
  bias = weights[len(weights)-1] #only the bias, float or int
  weights = weights[0:-1] #only the weights, list
  #print("layer depth", layer_depth)
  a = 0
  condition = True
  #while (a < max_ite == True) or (step_size > min_step == True):
  while (condition == True):
    a += 1

    if a > max_ite:
      condition = False

    #print(a, "th iteration")
    #print("max_ite", max_ite)
    outputs = forward_propagate(dftrain, network, class_name, regression)
    if regression == True:
      actual_outputs = outputs[0]
      predicted_outputs = outputs[1]
      #gradient descent for the bias
      #dSSR/dBias = -2 * sum(actual - preditct)
      #step_size = dSSR/dBias * LR
      #weights[j] = weights[j] - step_size
      #bias = 0.763774618976614, float or int
      #print("bias updating ...")
      dBias = -2 * sum_of_residual(actual_outputs, predicted_outputs) #<- takes the derivative
      ##print("Dbias is : ", dBias)
      step_size = dBias * LR #<- multiplies the derivative by the learning rate
      ##print("step_size is: ", step_size)
      bias = bias - step_size #<- update the bias
      ##print("updataed bias is: ", bias)
    if regression != True:
      
      actual_outputs = outputs[0]
      #print(actual_outputs)
      predicted_outputs = outputs[1]
      #print("THis is predicted", predicted_outputs)
      dBias = 0
      for f in range(len(actual_outputs)):
        for h in range(len(class_name)):
          if actual_outputs[f] == class_name[h]:
            actual_class_pos = h
        if actual_outputs[f] == predicted_outputs[f][0]:
          dBias += dBias + 1 - predicted_outputs[f][1][actual_class_pos]
        elif actual_outputs[f] != predicted_outputs[f][0]:
          dBias += dBias - predicted_outputs[f][1][actual_class_pos]

      ##print("dBias is: ", dBias)
      step_size = dBias * LR
      ##print("step size is: ", step_size)
      bias = bias - step_size
      ##print("updated bias is: ", bias)
    
    if step_size < min_step:
      condition = False
    #---------------------------------------------------------------------------
    #print("before:", network[layer_depth][neuron_index][0]["weights"][len(weights)-1])
    #print("step:", step_size)
    new_network[layer_depth][neuron_index][0]["weights"][len(weights)-1] = bias
    #print("after:", network[layer_depth][neuron_index][0]["weights"][len(weights)-1])
    #---------------------------------------------------------------------------

    prev_layer = layer_depth - 1
    for b in range(len(weights)):
      #print("weights updating ...")
      #gradient descent for the weights
      #dSSR/dW = -2 * sum(Xi * (actual - predict)) 
      #dSSR/dW = -2 * sum(Xi) * sum(SSR) <- SSR(actual and predicted)
      #step_size = dSSR/dW * LR
      #weights[j] = weights[j] - step_size
      #weights: ex:) [0.13436424411240122, 0.8474337369372327], list

      weight = weights[b]
      Xi = []
      predictor_list = dftrain.columns

      if layer_depth == 0: #this the case where the input is the actual input of the instance
        for c in range(dftrain.shape[0]):
          instance = dftrain.loc[c]
          predictor = predictor_list[b]
          predictor = instance[predictor]
          Xi.append(predictor)
      
      else: #this is where the input for the weight is the output from the previous layer
        layer = network[prev_layer]
        neuron = layer[b]
        Xi = neuron[1]["outputs"] #we want Xi to be a list of inputs to the neuron


      if regression == True:
        product = 0
        for d in range(len(Xi)):
          #print(Xi)
          product += -Xi[d] * (actual_outputs[d] - predicted_outputs[d])
        
        dWi = 2 * product #<- takes the derivative
        ##print("dWi is: ", dWi)
      #Xi is only an output from the single instance in the training dataset
        step_size = dWi * LR #<- multiplies the derivative by the learning rate
        ##print("step_size is: ", step_size)
        weight = weight - step_size #<- update the weight
        ##print("updated weight is: ", weight)

      if regression != True:
        product = 0
        for d in range(len(Xi)):
          for h in range(len(class_name)):
            if actual_outputs[d] == class_name[h]:
              actual_class_pos = h
          if actual_outputs[d] == predicted_outputs[d][0]:
            product += Xi[d] * (0 - predicted_outputs[d][1][actual_class_pos])
          elif actual_outputs[d] != predicted_outputs[d][0]:
            product += Xi[d] * (1 - predicted_outputs[d][1][actual_class_pos])

        step_size = product * LR
        weight = weight - step_size
        ##print("dWi is: ", product)
        ##print("step_size is: ", step_size)
        ##print("updated weight is: ", weight)
      if step_size < min_step:
        condition = False

      #---------------------------------------------------------------------------
      #print("length:", len(network[layer_depth][neuron_index][0]["weights"]))
      #print("i:", i)
      #print("before:", network[layer_depth][neuron_index][0]["weights"][b])
      #print("step:", step_size)
      ##print("before: ", network[layer_depth][neuron_index][0]["weights"][b])
      new_network[layer_depth][neuron_index][0]["weights"][b] = weight
      ##print("after:", network[layer_depth][neuron_index][0]["weights"][b])
      #---------------------------------------------------------------------------


    for e in range(len(network)):
      layer = network[e]
      for f in range(len(layer)):
        neuron = layer[f]
        neuron[1]["outputs"].clear()

  return new_network

"""
this funciton will perform the back propagation to update the weight for the training dataset

the frist loop iterates through the layers in the network
the second loop iterates through the neurons in the layer

"""
def back_propagation(dftrain, network, actual_outputs, LR, max_ite, min_step, class_name, regression):
  #print("back_propagation")
  for layers in reversed(range(len(network))): #goes through each layer in the network in the reversed order (from output to input)
    layer = network[layers]
    layer_depth = layers
    #print(layer_depth)
    for i in range(len(layer)):
      #print(neuron) <- ex) {'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}
      #call the gradient descent for each neuron
      neuron = layer[i]
      step_size = 100
      network = gradient_descent(dftrain, network, layer_depth, step_size, i, neuron, actual_outputs, class_name, LR, max_ite, min_step, regression)

  return network

"""
this funciton trains the network and returns the trained network

param: 
dftrain = training dataset, pandas.dataframe
network = initialized network, list
LR = learning rate for the G.D., float
max_ite = max number of iteration for G.D., int
min_step = minimum step for G.D., float
class_name = class name of the responsive variable, list
regression = whether the dataset is regression dataset or not, bool
"""
def training_network(structure, dftrain, dftest, LR, max_ite, min_step, class_name, regression=True):
  network = initialize_network(structure, dftrain, dftest, class_name, regression)
  actual_outputs = []

  #going through each instance in the training dataset and store the actual response value for each instance in the actual_outputs list
  for a in range(dftrain.shape[0]):
    #print(a,"th instance in training")
    
    if regression == True:
      instance = dftrain.loc[a][class_name]
      actual_outputs.append(instance[class_name])
      #the actual response variable for regression dataset is the response value

    if regression != True:
      for b in range(len(class_name)):
        #print("the cirrent row is: ", dftrain.loc[i])
        #print("the j value is: ", j)
        instance = dftrain.loc[a][class_name[b]]
        #print("the instance is: ", instance)
        
        if instance == 1 or instance == 1.0:
          actual_outputs.append(class_name[b])
          #print("is appended, :", actual_outputs)
          #the actual response variable for regression dataset is the class of the instance
 
  #this is to get the actual outputs for all instances in the dataframe
 
  if regression == True:
    network = back_propagation(dftrain, network, actual_outputs, LR, max_ite, min_step, class_name, regression)
    #class the back_prop func. for each instance in the training dataset and update the network
    pass
 
  elif regression != True:
    network = back_propagation(dftrain, network, actual_outputs, LR, max_ite, min_step, class_name, regression)
 
  return network

"""
param: hidden layer, learning_rate, max_ite, min_step, momentum
returns: result list of [actual response, predicted response]

H = the structure of hidden layers, len(H) is the number of layers, len[i] is number of nodes for each layer, list
LR = Learning Rate, float
max_ite = max iteration, int
min_step = minimum step size, float
alpha = momentum, float
dftest = the test data set, pd.DataFrame
dftrain = the training data set, pd.DataFrame
class_name = the list of strings of the name of columns that store the responsive variable of the data set, list
regression = True is the response value of the dataset is numerical, False otherwise, regression=True as default
"""

def initialize_network(structure, dftrain, class_name, regression=True):
  if structure[0] == 0:
    network = list() #<- initialize a network as list
    n_outputs = len(class_name)
    n_inputs = len(dftrain.columns) - n_outputs
    structure = [n_inputs, n_outputs]

  else:
    network = list() #<- initialize a network as list
    n_outputs = len(class_name)
    n_inputs = len(dftrain.columns) - n_outputs
    structure.insert(0, n_inputs) #<- insert the n_inputs in the beginning of the structure
    structure.append(n_outputs) #<- insert the n_inputs in the end of the structure

  #"random()" genrates random floating number b/w 0 and 1

  for i in range(1, len(structure)): #<- goes through the for loop for (number of layers - 1)
    weights = [] #<- initialize len(structure) number of hidden layer list

    for j in range(structure[i]): #<- goes though the for loop for number of nodes in hidden layers
      random_list = [] #<- initialize the list of initial weights for the i-th layer
      outputs_list = []

      for k in range(structure[i-1] + 1): #<- the # of weight coming into a node is (the number of nodes in the previous layer + 1)
        random_list.append(random.random()) #<- generates random number for each initial weight and add that in the "random_list"
      
      #weights_and_outputs = [{"weights": random_list}, {"outputs": outputs_list}]
      weights.append({"weights": random_list})
      #weights.append({"outputs": outputs_list})
      #weights.append(weights_and_outputs)
    
    network.append(weights)
  structure_len = len(structure)
    
  structure.pop(structure_len - 1)
  structure.pop(0)
  return network

def hyperparameter_tune_soybean_HP(structure, LR, max_ite, min_step):
  "------------------------------------------------------------------------"
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  soybean_trained_network1 = training_network(structure, soybean_training_1, soybean_HP, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results1 = forward_propagate(soybean_HP, soybean_trained_network1, soybean_class, regression=False)
 
  soybean_trained_network2 = training_network(structure, soybean_training_2, soybean_HP, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results2 = forward_propagate(soybean_HP, soybean_trained_network2, soybean_class, regression=False)
  
  soybean_trained_network3 = training_network(structure, soybean_training_3, soybean_HP, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results3 = forward_propagate(soybean_HP, soybean_trained_network3, soybean_class, regression=False)
  
  soybean_trained_network4 = training_network(structure, soybean_training_4, soybean_HP, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results4 = forward_propagate(soybean_HP, soybean_trained_network4, soybean_class, regression=False)
 
  soybean_trained_network5 = training_network(structure, soybean_training_5, soybean_HP, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results5 = forward_propagate(soybean_HP, soybean_trained_network5, soybean_class, regression=False)
 
  soybean_trained_network6 = training_network(structure, soybean_training_6, soybean_HP, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results6 = forward_propagate(soybean_HP, soybean_trained_network6, soybean_class, regression=False)
 
  soybean_trained_network7 = training_network(structure, soybean_training_7, soybean_HP, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results7 = forward_propagate(soybean_HP, soybean_trained_network7, soybean_class, regression=False)
 
  soybean_trained_network8 = training_network(structure, soybean_training_8, soybean_HP, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results8 = forward_propagate(soybean_HP, soybean_trained_network8, soybean_class, regression=False)
 
  soybean_trained_network9 = training_network(structure, soybean_training_9, soybean_HP, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results9 = forward_propagate(soybean_HP, soybean_trained_network9, soybean_class, regression=False)
 
  soybean_trained_network10 = training_network(structure, soybean_training_10, soybean_HP, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results10 = forward_propagate(soybean_HP, soybean_trained_network10, soybean_class, regression=False)
 
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  print("results for test one")
  print(zero_loss_func(soybean_results1[0], soybean_results1[1]))
  print("results for test two")
  print(zero_loss_func(soybean_results2[0], soybean_results2[1]))
  #print("results for test three")
  #print(zero_loss_func(soybean_results3[0], soybean_results3[1]))
  print("results for test four")
  print(zero_loss_func(soybean_results4[0], soybean_results4[1]))
  print("results for test five")
  print(zero_loss_func(soybean_results5[0], soybean_results5[1]))
  print("results for test six")
  print(zero_loss_func(soybean_results6[0], soybean_results6[1]))
  print("results for test seven")
  print(zero_loss_func(soybean_results7[0], soybean_results7[1]))
  print("results for test eight")
  print(zero_loss_func(soybean_results8[0], soybean_results8[1]))
  print("results for test nine")
  print(zero_loss_func(soybean_results9[0], soybean_results9[1]))
  print("results for test ten")
  print(zero_loss_func(soybean_results10[0], soybean_results10[1]))
  "------------------------------------------------------------------------"

def hyperparameter_tune_soybean(structure, LR, max_ite, min_step):
  "------------------------------------------------------------------------"
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  soybean_trained_network1 = training_network(structure, soybean_training_1, soybean_1, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results1 = forward_propagate(soybean_1, soybean_trained_network1, soybean_class, regression=False)
 
  soybean_trained_network2 = training_network(structure, soybean_training_2, soybean_2, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results2 = forward_propagate(soybean_2, soybean_trained_network2, soybean_class, regression=False)
 
  soybean_trained_network3 = training_network(structure, soybean_training_3, soybean_3, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results3 = forward_propagate(soybean_3, soybean_trained_network3, soybean_class, regression=False)
 
  soybean_trained_network4 = training_network(structure, soybean_training_4, soybean_4, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results4 = forward_propagate(soybean_4, soybean_trained_network4, soybean_class, regression=False)
 
  soybean_trained_network5 = training_network(structure, soybean_training_5, soybean_5, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results5 = forward_propagate(soybean_5, soybean_trained_network5, soybean_class, regression=False)
 
  soybean_trained_network6 = training_network(structure, soybean_training_6, soybean_6, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results6 = forward_propagate(soybean_6, soybean_trained_network6, soybean_class, regression=False)
 
  soybean_trained_network7 = training_network(structure, soybean_training_7, soybean_7, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results7 = forward_propagate(soybean_7, soybean_trained_network7, soybean_class, regression=False)
 
  soybean_trained_network8 = training_network(structure, soybean_training_8, soybean_8, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results8 = forward_propagate(soybean_8, soybean_trained_network8, soybean_class, regression=False)
 
  soybean_trained_network9 = training_network(structure, soybean_training_9, soybean_9, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results9 = forward_propagate(soybean_9, soybean_trained_network9, soybean_class, regression=False)
 
  soybean_trained_network10 = training_network(structure, soybean_training_10, soybean_10, LR, max_ite, min_step, soybean_class, regression=False)
  soybean_results10 = forward_propagate(soybean_10, soybean_trained_network10, soybean_class, regression=False)
 
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  print("results for test one")
  print(zero_loss_func(soybean_results1[0], soybean_results1[1]))
  print("results for test two")
  print(zero_loss_func(soybean_results2[0], soybean_results2[1]))
  print("results for test three")
  print(zero_loss_func(soybean_results3[0], soybean_results3[1]))
  print("results for test four")
  print(zero_loss_func(soybean_results4[0], soybean_results4[1]))
  print("results for test five")
  print(zero_loss_func(soybean_results5[0], soybean_results5[1]))
  print("results for test six")
  print(zero_loss_func(soybean_results6[0], soybean_results6[1]))
  print("results for test seven")
  print(zero_loss_func(soybean_results7[0], soybean_results7[1]))
  print("results for test eight")
  print(zero_loss_func(soybean_results8[0], soybean_results8[1]))
  print("results for test nine")
  print(zero_loss_func(soybean_results9[0], soybean_results9[1]))
  print("results for test ten")
  print(zero_loss_func(soybean_results10[0], soybean_results10[1]))
  "------------------------------------------------------------------------"

def hyperparameter_tune_forest(structure, LR, max_ite, min_step):
  "------------------------------------------------------------------------"
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  forest_trained_network1 = training_network(structure, forest_training_1, forest_1, LR, max_ite, min_step, forest_class, regression=True)
  forest_results1 = forward_propagate(forest_1, forest_trained_network1, forest_class, regression=True)
 
  forest_trained_network2 = training_network(structure, forest_training_2, forest_2, LR, max_ite, min_step, forest_class, regression=True)
  forest_results2 = forward_propagate(forest_2, forest_trained_network2, forest_class, regression=True)
 
  forest_trained_network3 = training_network(structure, forest_training_3, forest_3, LR, max_ite, min_step, forest_class, regression=True)
  forest_results3 = forward_propagate(forest_3, forest_trained_network3, forest_class, regression=True)
 
  forest_trained_network4 = training_network(structure, forest_training_4, forest_4, LR, max_ite, min_step, forest_class, regression=True)
  forest_results4 = forward_propagate(forest_4, forest_trained_network4, forest_class, regression=True)
 
  forest_trained_network5 = training_network(structure, forest_training_5, forest_5, LR, max_ite, min_step, forest_class, regression=True)
  forest_results5 = forward_propagate(forest_5, forest_trained_network5, forest_class, regression=True)
 
  forest_trained_network6 = training_network(structure, forest_training_6, forest_6, LR, max_ite, min_step, forest_class, regression=True)
  forest_results6 = forward_propagate(forest_6, forest_trained_network6, forest_class, regression=True)
 
  forest_trained_network7 = training_network(structure, forest_training_7, forest_7, LR, max_ite, min_step, forest_class, regression=True)
  forest_results7 = forward_propagate(forest_7, forest_trained_network7, forest_class, regression=True)
 
  forest_trained_network8 = training_network(structure, forest_training_8, forest_8, LR, max_ite, min_step, forest_class, regression=True)
  forest_results8 = forward_propagate(forest_8, forest_trained_network8, forest_class, regression=True)
 
  forest_trained_network9 = training_network(structure, forest_training_9, forest_9, LR, max_ite, min_step, forest_class, regression=True)
  forest_results9 = forward_propagate(forest_9, forest_trained_network9, forest_class, regression=True)
 
  forest_trained_network10 = training_network(structure, forest_training_10, forest_10, LR, max_ite, min_step, forest_class, regression=True)
  forest_results10 = forward_propagate(forest_10, forest_trained_network10, forest_class, regression=True)
 
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  print("results for test one")
  print(Mean_Squared_Error(forest_results1))
  print("results for test two")
  print(Mean_Squared_Error(forest_results2))
  print("results for test three")
  print(Mean_Squared_Error(forest_results3))
  print("results for test four")
  print(Mean_Squared_Error(forest_results4))
  print("results for test five")
  print(Mean_Squared_Error(forest_results5))
  print("results for test six")
  print(Mean_Squared_Error(forest_results6))
  print("results for test seven")
  print(Mean_Squared_Error(forest_results7))
  print("results for test eight")
  print(Mean_Squared_Error(forest_results8))
  print("results for test nine")
  print(Mean_Squared_Error(forest_results9))
  print("results for test ten")
  print(Mean_Squared_Error(forest_results10))
  "------------------------------------------------------------------------"

def hyperparameter_tune_computer(structure, LR, max_ite, min_step):
  "------------------------------------------------------------------------"
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  computer_trained_network1 = training_network(structure, computer_training_1, computer_1, LR, max_ite, min_step, computer_class, regression=True)
  computer_results1 = forward_propagate(computer_1, computer_trained_network1, computer_class, regression=True)
 
  computer_trained_network2 = training_network(structure, computer_training_2, computer_2, LR, max_ite, min_step, computer_class, regression=True)
  computer_results2 = forward_propagate(computer_2, computer_trained_network2, computer_class, regression=True)
 
  computer_trained_network3 = training_network(structure, computer_training_3, computer_3, LR, max_ite, min_step, computer_class, regression=True)
  computer_results3 = forward_propagate(computer_3, computer_trained_network3, computer_class, regression=True)
 
  computer_trained_network4 = training_network(structure, computer_training_4, computer_4, LR, max_ite, min_step, computer_class, regression=True)
  computer_results4 = forward_propagate(computer_4, computer_trained_network4, computer_class, regression=True)
 
  computer_trained_network5 = training_network(structure, computer_training_5, computer_5, LR, max_ite, min_step, computer_class, regression=True)
  computer_results5 = forward_propagate(computer_5, computer_trained_network5, computer_class, regression=True)
 
  computer_trained_network6 = training_network(structure, computer_training_6, computer_6, LR, max_ite, min_step, computer_class, regression=True)
  computer_results6 = forward_propagate(computer_6, computer_trained_network6, computer_class, regression=True)
 
  computer_trained_network7 = training_network(structure, computer_training_7, computer_7, LR, max_ite, min_step, computer_class, regression=True)
  computer_results7 = forward_propagate(computer_7, computer_trained_network7, computer_class, regression=True)
 
  computer_trained_network8 = training_network(structure, computer_training_8, computer_8, LR, max_ite, min_step, computer_class, regression=True)
  computer_results8 = forward_propagate(computer_8, computer_trained_network8, computer_class, regression=True)
 
  computer_trained_network9 = training_network(structure, computer_training_9, computer_9, LR, max_ite, min_step, computer_class, regression=True)
  computer_results9 = forward_propagate(computer_9, computer_trained_network9, computer_class, regression=True)
 
  computer_trained_network10 = training_network(structure, computer_training_10, computer_10, LR, max_ite, min_step, computer_class, regression=True)
  computer_results10 = forward_propagate(computer_10, computer_trained_network10, computer_class, regression=True)
 
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  print("results for test one")
  print(Mean_Squared_Error(computer_results1))
  print("results for test two")
  print(Mean_Squared_Error(computer_results2))
  print("results for test three")
  print(Mean_Squared_Error(computer_results3))
  print("results for test four")
  print(Mean_Squared_Error(computer_results4))
  print("results for test five")
  print(Mean_Squared_Error(computer_results5))
  print("results for test six")
  print(Mean_Squared_Error(computer_results6))
  print("results for test seven")
  print(Mean_Squared_Error(computer_results7))
  print("results for test eight")
  print(Mean_Squared_Error(computer_results8))
  print("results for test nine")
  print(Mean_Squared_Error(computer_results9))
  print("results for test ten")
  print(Mean_Squared_Error(computer_results10))
  "------------------------------------------------------------------------"

def hyperparameter_tune_breast(structure, LR, max_ite, min_step):
  "------------------------------------------------------------------------"
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  breast_trained_network1 = training_network(structure, breast_training_1, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results1 = forward_propagate(breast_HP, breast_trained_network1, breast_class, regression=False)
 
  breast_trained_network2 = training_network(structure, breast_training_2, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results2 = forward_propagate(breast_HP, breast_trained_network2, breast_class, regression=False)
 
  breast_trained_network3 = training_network(structure, breast_training_3, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results3 = forward_propagate(breast_HP, breast_trained_network3, breast_class, regression=False)
 
  breast_trained_network4 = training_network(structure, breast_training_4, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results4 = forward_propagate(breast_HP, breast_trained_network4, breast_class, regression=False)
 
  breast_trained_network5 = training_network(structure, breast_training_5, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results5 = forward_propagate(breast_HP, breast_trained_network5, breast_class, regression=False)
 
  breast_trained_network6 = training_network(structure, breast_training_6, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results6 = forward_propagate(breast_HP, breast_trained_network6, breast_class, regression=False)
 
  breast_trained_network7 = training_network(structure, breast_training_7, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results7 = forward_propagate(breast_HP, breast_trained_network7, breast_class, regression=False)
 
  breast_trained_network8 = training_network(structure, breast_training_8, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results8 = forward_propagate(breast_HP, breast_trained_network8, breast_class, regression=False)
 
  breast_trained_network9 = training_network(structure, breast_training_9, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results9 = forward_propagate(breast_HP, breast_trained_network9, breast_class, regression=False)
 
  breast_trained_network10 = training_network(structure, breast_training_10, breast_HP, LR, max_ite, min_step, breast_class, regression=False)
  breast_results10 = forward_propagate(breast_HP, breast_trained_network10, breast_class, regression=False)
 
  print("hyperparameters are structure = ", structure, " , LR = ", LR, " , max_ite = ", max_ite, " , min_step = ", min_step)
  print("results for test one")
  print(zero_loss_func(breast_results1[0], breast_results1[1]))
  print("results for test two")
  print(zero_loss_func(breast_results2[0], breast_results2[1]))
  print("results for test three")
  print(zero_loss_func(breast_results3[0], breast_results3[1]))
  print("results for test four")
  print(zero_loss_func(breast_results4[0], breast_results4[1]))
  print("results for test five")
  print(zero_loss_func(breast_results5[0], breast_results5[1]))
  print("results for test six")
  print(zero_loss_func(breast_results6[0], breast_results6[1]))
  print("results for test seven")
  print(zero_loss_func(breast_results7[0], breast_results7[1]))
  print("results for test eight")
  print(zero_loss_func(breast_results8[0], breast_results8[1]))
  print("results for test nine")
  print(zero_loss_func(breast_results9[0], breast_results9[1]))
  print("results for test ten")
  print(zero_loss_func(breast_results10[0], breast_results10[1]))
  "------------------------------------------------------------------------"

def initialize_X_list(structure, dftrain, dftest, class_name, regression = True):
  X_list = []
  if structure[0] == 0:
    network = list() #<- initialize a network as list
    n_outputs = len(class_name)
    n_inputs = len(dftrain.columns) - n_outputs
    structure = [n_inputs, n_outputs]
 
  else:
    network = list() #<- initialize a network as list
    n_outputs = len(class_name)
    n_inputs = len(dftrain.columns) - n_outputs
    structure.insert(0, n_inputs) #<- insert the n_inputs in the beginning of the structure
    structure.append(n_outputs) #<- insert the n_inputs in the end of the structure
 
  #print(structure)
  for i in range(1, len(structure)): #<- goes through the for loop for (number of layers - 1)
    for j in range(structure[i]): #<- goes though the for loop for number of nodes in hidden layers
      random_list = [] #<- initialize the list of initial weights for the i-th layer
    
      for k in range(structure[i-1] + 1): #<- the # of weight coming into a node is (the number of nodes in the previous layer + 1)
        random_list.append(random.random()) #<- generates random number for each initial weight and add that in the "random_list"
 
      for l in random_list:
        X_list.append(l)
    
  structure_len = len(structure)
    
  structure.pop(structure_len - 1)
  structure.pop(0)
  return X_list
 
def list_to_network(X_list, structure, dftrain, dftest, class_name, regression = True):
  #print(X_list[0])
  if structure[0] == 0:
    network = list() #<- initialize a network as list
    n_outputs = len(class_name)
    n_inputs = len(dftrain.columns) - n_outputs
    structure = [n_inputs, n_outputs]
 
  else:
    network = list() #<- initialize a network as list
    n_outputs = len(class_name)
    n_inputs = len(dftrain.columns) - n_outputs
    structure.insert(0, n_inputs) #<- insert the n_inputs in the beginning of the structure
    structure.append(n_outputs) #<- insert the n_inputs in the end of the structure
    #print(structure)
  
  #"random()" genrates random floating number b/w 0 and 1
 
  for i in range(1, len(structure)): #<- goes through the for loop for (number of layers - 1)
    weights = [] #<- initialize len(structure) number of hidden layer list
    
    for j in range(structure[i]): #<- goes though the for loop for number of nodes in hidden layers
      random_list = [] #<- initialize the list of initial weights for the i-th layer
      outputs_list = []
 
      for k in range(structure[i-1] + 1): #<- the # of weight coming into a node is (the number of nodes in the previous layer + 1)
        #print(X_list[0])
        random_list.append(X_list[0]) #<- generates random number for each initial weight and add that in the "random_list"
        X_list.pop(0)
      
      #weights.append({"weights": random_list})
      #weights.append({"outputs": outputs_list})
      weights.append({"weights": random_list})
      #print(weights)
    
    network.append(weights)
  structure_len = len(structure)
    
  structure.pop(structure_len - 1)
  structure.pop(0)
  return network
 
 
 
def U_vector_calc(X1, X2, X3, scaling_factor):
  Uj = []
  for i in range(len(X1)):
    Uj.append(X1[i] + (scaling_factor * (X2[i] - X3[i])))
  
  return Uj
 
 
 
X1 = initialize_X_list([5], glass_training_1, glass_1, glass_class, regression = False )
print(X1)
X2 = initialize_X_list([5], glass_training_1, glass_1, glass_class, regression = False )
print(X2)
X3 = initialize_X_list([5], glass_training_1, glass_1, glass_class, regression = False )
print(X3)
U = U_vector_calc(X1, X2, X3, .5)
print(U)
 
print(list_to_network(U, [5], glass_training_1, glass_1, glass_class, regression = False))
X = (initialize_network([5], glass_training_1, glass_class, regression = False))
print(X)

"""
Forward propagate input to a network output
network: initialized network, the list of dictionaries of lists of weights
instance: each instance in the training data set
class_name: a list of column name that stores the response variable in the data set, list
  
this funciton intakes the network (lsit of weights) and each instance in the data set
to run the entire network to obtian the predicted result

this will return the output for each instance in the dataset, and the network it used
return: 
outputs = [actual_outputs, predicted_outputs], list
actual_outputs = list of actual output for each instance in the training dataset, len(actual_outputs) = number of instances in the dataset, list
predicted_outputs = list of predicted output for each instance in the training dataset, len(actual_outputs) = number of instances in the dataset, list
"""
def forward_propagate(dftrain, network, class_name, regression=True):
  actual = None
  actual_outputs = []
  predicted_outputs = []
  result = []

  for i in range(dftrain.shape[0]):
    instance = dftrain.loc[i]  

    #assigns the actual response varibale to "actual"
    if regression == True:
      actual = float(instance[class_name])
    
    elif regression != True:
      for column in class_name:
        if instance[column] == 1 or instance[column] == 1.0:
          actual = column
          

    for column in class_name: #<- this goes through each column in the instance to drop the response variable column
      instance = instance.drop(labels = column)

    instance = instance.tolist() #<- convert instance (type(inputs) = pandas.series) into list
    
    inputs = instance #<- the inputs are the attribute values in the instance.
    

    for layer in network: #<- this goes through each list of 'weights' dictionaries in each layer.

      new_inputs = [] #<- this stores the input that are calculated at a layer. This becomes the input to the next layer.

      for neuron in layer: #<- this goes through each 'weights' dictionary in each layer
        activation = 0
        #weights = neuron[0]["weights"] #<- stores the weights coming in to each neuron in a list
        weights = neuron["weights"]
        
        sum = 0

        for i in range(len(inputs)): #<- this goes through each weight in a neuron

          sum += weights[i] * inputs[i] #<- multiplies each weight and each input


        bias = weights[len(inputs)]

        sum += bias #<- then, adds the bias to the product of weights and inputs

        activation = sigmoid_activation(sum) #<- call sigmoid activation function and get the output of the neuron

        #neuron[1]["outputs"].append(activation)
        new_inputs.append(activation)
      
      inputs = new_inputs #<- update the inputs list with new_inputs list

    
    if regression==True:
      inputs = float(linear_activation(inputs[0]))



    elif regression!=True:
      inputs = softmax_activation(inputs, class_name)
      

    #"inputs" is the final input will be the input for the layer after the final output layer = this input is the output of the entire network
    #actual is the read response variable for each instance
    preidcted = inputs
    actual_outputs.append(actual)
    predicted_outputs.append(preidcted)

  result = [actual_outputs, predicted_outputs]
  return result

def differential_evolution_mutation(structure, dftrain, dftest, class_name, scaling_factor, regression = True):
  X1 = initialize_X_list(structure, dftrain, dftest, class_name, regression)
  X2 = initialize_X_list(structure, dftrain, dftest, class_name, regression)
  X3 = initialize_X_list(structure, dftrain, dftest, class_name, regression)
  print("U vector creation (mutation)")
  U = U_vector_calc(X1, X2, X3, scaling_factor)
 
  Uj = list_to_network(U, structure, dftrain, dftest, class_name, regression)
  print(Uj)
  return Uj
 
# def differential_evolution_crossover(structure, dftest, class_name, x_network, u_network, p_crossover, weight_length, regression = True):
#   Uj = binary_crossover(x_network, u_network, p_crossover, weight_length)
 
# u_network = differential_evolution_mutation([5], glass_training_1, glass_1, glass_class, .5, regression = False)
# X = (initialize_network([5], glass_training_1, glass_class, regression = False))
 
 
def differential_evolution(structure, dftest, dftrain, class_name, p_crossover, scaling_factor, num_of_ite, regression = True):
  X_best = initialize_network(structure, dftrain, class_name, regression)
  print("Original X best set of weights")
  print(X_best)
  weight_length = get_number_of_weights(X_best)
  X_best_result_list = forward_propagate(dftest, X_best, class_name, regression)
  
  if regression == False:
    X_best_result = zero_loss_func(X_best_result_list[0], X_best_result_list[1])
  if regression == True:
    X_best_result = DE_Mean_Squared_Error(X_best_result_list, len(X_best_result_list[0]))
    print(X_best_result)
  for i in range(num_of_ite):
    Uj = differential_evolution_mutation(structure, dftrain, dftest, class_name, scaling_factor, regression)
    Uj_cross = binary_crossover(X_best, Uj, p_crossover, weight_length)
    print("crossoverver between X_best and newly created U_vector (Crossover)")
    print(Uj_cross)
    Uj_cross_result_list = forward_propagate(dftest, Uj_cross, class_name, regression)
    
    if regression == False:
      Uj_cross_best_result = zero_loss_func(Uj_cross_result_list[0], Uj_cross_result_list[1])
      
      # print(Uj_cross_best_result)
      if Uj_cross_best_result[0] > X_best_result[0]:
        X_best_result = Uj_cross_best_result
        X_best = Uj_cross

    if regression == True:
      Uj_cross_best_result = DE_Mean_Squared_Error(Uj_cross_result_list, len(Uj_cross_result_list[0]))
      print(Uj_cross_best_result)
      if Uj_cross_best_result < X_best_result:
        X_best_result = Uj_cross_best_result
        X_best = Uj_cross
      print("updated X_best (selection)")
      print(X_best)
      print(X_best_result)
  print(X_best)
  print(X_best_result)
  return X_best_result
    


def DE_Mean_Squared_Error(result_list, length):
  MSE = 0
  
  for i in range(length):
    MSE += (result_list[0][i] -result_list[1][i])**2
  
  MSE = MSE/length
  return MSE

#VIDEO CODE

differential_evolution([3],computer_HP,computer_training_4,computer_class, .1, 2, 100, regression = True)

print("two layer results")
differential_evolution([5, 5],computer_HP,computer_training_4,computer_class, .1, 2, 100, regression = True)

print("two layer results")
differential_evolution([5, 5],soybean_HP,soybean_training_4,soybean_class, .1, 2, 100, regression = False)

print("10XCV computer layer = 2")
hyperparameter_tune_computer_HP_DE([5], .1, 100, 2, 1)

"""
this function is the main funciton for the PSO
this calls necessary functions for the algorithm to complete

1. initialize parameters and population (Xi and Vi)
2. forward_propagate to get the results for all particles
3. evaluate the fitness of Xi at t-time
4. update Xi and Vi
5. repeat 2, 3, and 4 until the convergence
6. return gBest
7. perform forward_propagate for the test on gBest

args:
dftrain = the training dataset, pd.dataframe
dftest = the test dataset, pd.dataframe
population = the number of initial population, int
structure = the structure of the network, array
"""

def PSO(dftrain, dftest, population, structure, class_name, max_ite, w, c1, c2, regression=True):
  particles = [] #<- this is the array that stores the particles (networks)
  v_list = []
  output_dict = [] #<- this stores the outputs of the particles
  xBest_network = [] #<- stores the xBest networks
  gBest = None #<- initialize the global best, stores the result of gBest
  gBest_network = None #<- stores the actual gBest network

  """
  the parameter are initialized below with random.random() for now, and c1/c2 with 1.492 as recommended for now.
  """


  """
  this initializes network times the size of population using initialize_network() func. 
  the randomly initialized networks are stored in a list called particles.
  it also initialize the velocity for each particle. velocity can be initialized using the initialize_network 
  becaue each weight in the network should have a different velocit. Thus, size of v_i(t) == size of x_i(t)
  then, it runs the forward_prop for each particle to get the first result of the particle.
  """
  for i in range(population):
    #initializing the network for each particle, and append it to the list "particles"
    particle = initialize_network(structure, dftrain, class_name, regression)
    particles.append(particle)
    xBest_network.append(particle)

    #initializing the velocity for each particle, and appned it to the lis "v_list"
    Vi = initialize_network(structure, dftrain, class_name, regression)
    v_list.append(Vi)

    #runs the forward_prop to get the first result for each particle
    #output = forward_propagate(dftrain, particle, class_name, regression)
    #output_dict.append({"output": output[1], "xBest": output[1]})

  """-------------------------------------------------------------------------- networks and velocity initializations are okay."""
  
  condition = True
  t = 1
  while (condition == True): #<- this loop goes through the particle swarm optimization
    #print("condition =", condition, "at", t)

    """
    the condition whether the program breaks the while loop or not is updated here
    break while loop when:
    max_ite < t etc...
    """
    #---------------------------------------------------------------------------
    if max_ite < t: #<- if t-generation exceeds the max_ite,
      condition = False #<- update the condition and stops the while loop
      #print("condition =", condition, "at", t)
      break
    #---------------------------------------------------------------------------

    """
    evaluate the particle (forward_prop)
    """
    #---------------------------------------------------------------------------
    #runs the evaluation and particle update for the number of particles
    for i in range(len(particles)):
      #only for the first time
      #runs the forward_prop to get the first result for each particle
      if t == 1:
        if regression == True:
          output = forward_propagate(dftrain, particles[i], class_name, regression)
          output = SSR(output[0], output[1])
          output_dict.append({"output": output, "xBest": output})

        elif regression != True:
          output = forward_propagate(dftrain, particles[i], class_name, regression)
          output = zero_loss_func(output[0], output[1]) #<- obtain the number of correct and incorrect guesses
          output = 1 / output[0] #<- the output is the reciprocal of the number of correct guesses
          output_dict.append({"output": output, "xBest": output})
        
        """-------------------------------------------------------------------- getting the evaluation of the initial particles is okay. storing the first output as the xBest is okay."""
      #runs the evaluation (forward_prop) for when 1 <= t 
      #because we need to update xBest each time
      elif t != 1:
        print("xBest evaluation Before:", output_dict[i]["xBest"])
        print("xBest network Before:", xBest_network[i])
        if regression == True:
          output = forward_propagate(dftrain, particles[i], class_name, regression)
          output = SSR(output[0], output[1])

          if output < output_dict[i]["xBest"]: #<- if the current output is better than the current xBest,
            output_dict[i]["output"] = output #<- update the current output
            output_dict[i]["xBest"] = output #<- update the xBest
            xBest_network[i] = particles[i]

          else:
            output_dict[i]["output"] = output

        elif regression != True:
          output = forward_propagate(dftrain, particles[i], class_name, regression)
          output = zero_loss_func(output[0], output[1])
          output = 1 / output[0] #<- output is the reciprocal of the number of correct guess so that a better result can have a smaller value
    
          if output < output_dict[i]["xBest"]: #<- if the current output is better than the current xBest,
            output_dict[i]["output"] = output #<- update the current output
            output_dict[i]["xBest"] = output #<- update the xBest
            xBest_network[i] = particles[i]

          else:
            output_dict[i]["output"] = output
        print("xBest evaluation After:", output_dict[i]["xBest"])
        print("xBest network After:", xBest_network[i])
        print(" ")
    """------------------------------------------------------------------------ getting the evaluation in the each generation is okay. storing the current evaluation and updating the best evaluation is okay."""
    #---------------------------------------------------------------------------

    """
    gBest update
    """
    #---------------------------------------------------------------------------
    for i in range(len(output_dict)): #<- to update the gBest
      xBest = output_dict[i]["xBest"] #<- get the xBest for each particle
      if gBest == None: #<- if the gBest is None (only for the first time),
        gBest = xBest #<- update the performance of gBest,
        gBest_network = xBest_network[i] #<- and update the gBest_network

      elif xBest < gBest: #<- compare xBest of each particle and gBest
        gBest = xBest #<- then update the global best,
        gBest_network = xBest_network[i] #<- and update the gBest_network
      #output = forward_propagate(dftrain, gBest_network, class_name, regression)
      #output = SSR(output[0], output[1])
    #---------------------------------------------------------------------------
    print("gBest evaluation:", gBest)
    print("gBest network:", gBest_network)
    """------------------------------------------------------------------------ gBest value is correct updated. the network is NOT properly updated"""


    """
    update x_i and v_i
    """
    #---------------------------------------------------------------------------
    for i in range(len(particles)):
      r1 = random.random() #<- randomly generate r1 for each particle
      r2 = random.random() #<- randomly generate r2 for each particle

      #Vi(t+1) = w * Vi(t) + c1 * r1 * (xBesti(t) - Xi(t)) + c2 * r2 * (gBesti(t) - Xi(t))
      #Xi(t+1) = Xi(t) + Vi(t+1)

      print("current velocity:", v_list[i])
      print("current position:", particles[i])
      for (layer_in_network, layer_in_velocity, layer_in_xBest, layer_in_gBest) in zip(particles[i], v_list[i], xBest_network[i], gBest_network):
        for (neuron_in_network, neuron_in_velocity, neuron_in_xBest, neuron_in_gBest) in zip(layer_in_network, layer_in_velocity, layer_in_xBest, layer_in_gBest):
          for (weight_in_network, weight_in_velocity, weight_in_xBest, weight_in_gBest) in zip(neuron_in_network["weights"], neuron_in_velocity["weights"], neuron_in_xBest["weights"], neuron_in_gBest["weights"]):
            Vi = weight_in_velocity
            Xi = weight_in_network
            xBest = weight_in_xBest
            gBest = weight_in_gBest

            Vi = (w * Vi) + (c1 * r1 * (xBest - Xi)) + (c2 * r2 * (gBest - Xi))
            Xi = Xi + Vi
            
            weight_in_network = Xi
            weight_in_velocity = Vi
      print("updated velocity:", v_list[i])
      print("updated position:", particles[i])
      print(" ")
    t += 1 #<- keeps track with the t-generation     
  #---------------------------------------------------------------------------
  return gBest_network

#video code for PSO functionality

PSO_network_regression1 = PSO(computer_training_1, computer_1, 5, [2,2], computer_class, 5, 0.5, 1.492, 1.492, True)
PSO_network_regression2 = PSO(computer_training_2, computer_2, 5, [2,2], computer_class, 5, 0.5, 1.492, 1.492, True)
PSO_network_regression3 = PSO(computer_training_3, computer_3, 5, [2,2], computer_class, 5, 0.5, 1.492, 1.492, True)
PSO_network_regression4 = PSO(computer_training_4, computer_4, 5, [2,2], computer_class, 5, 0.5, 1.492, 1.492, True)
PSO_network_regression5 = PSO(computer_training_5, computer_5, 5, [2,2], computer_class, 5, 0.5, 1.492, 1.492, True)
PSO_network_regression6 = PSO(computer_training_6, computer_6, 5, [2,2], computer_class, 5, 0.5, 1.492, 1.492, True)
PSO_network_regression7 = PSO(computer_training_7, computer_7, 5, [2,2], computer_class, 5, 0.5, 1.492, 1.492, True)
PSO_network_regression8 = PSO(computer_training_8, computer_8, 5, [2,2], computer_class, 5, 0.5, 1.492, 1.492, True)
PSO_network_regression9 = PSO(computer_training_9, computer_9, 5, [2,2], computer_class, 5, 0.5, 1.492, 1.492, True)
PSO_network_regression0 = PSO(computer_training_10, computer_10, 5, [2,2], computer_class, 5, 0.5, 1.492, 1.492, True)

PSO_result_regression1 = forward_propagate(computer_1, PSO_network_regression1, computer_class, True)
PSO_result_regression2 = forward_propagate(computer_2, PSO_network_regression2, computer_class, True)
PSO_result_regression3 = forward_propagate(computer_3, PSO_network_regression3, computer_class, True)
PSO_result_regression4 = forward_propagate(computer_4, PSO_network_regression4, computer_class, True)
PSO_result_regression5 = forward_propagate(computer_5, PSO_network_regression5, computer_class, True)
PSO_result_regression6 = forward_propagate(computer_6, PSO_network_regression6, computer_class, True)
PSO_result_regression7 = forward_propagate(computer_7, PSO_network_regression7, computer_class, True)
PSO_result_regression8 = forward_propagate(computer_8, PSO_network_regression8, computer_class, True)
PSO_result_regression9 = forward_propagate(computer_9, PSO_network_regression9, computer_class, True)
PSO_result_regression0 = forward_propagate(computer_10, PSO_network_regression0, computer_class, True)

PSO_network_classification1 = PSO(soybean_training_1, soybean_1, 5, [2,2], soybean_class, 5, 0.5, 1.492, 1.492, False)
PSO_network_classification2 = PSO(soybean_training_2, soybean_2, 5, [2,2], soybean_class, 5, 0.5, 1.492, 1.492, False)
PSO_network_classification3 = PSO(soybean_training_3, soybean_3, 5, [2,2], soybean_class, 5, 0.5, 1.492, 1.492, False)
PSO_network_classification4 = PSO(soybean_training_4, soybean_4, 5, [2,2], soybean_class, 5, 0.5, 1.492, 1.492, False)
PSO_network_classification5 = PSO(soybean_training_5, soybean_5, 5, [2,2], soybean_class, 5, 0.5, 1.492, 1.492, False)
PSO_network_classification6 = PSO(soybean_training_6, soybean_6, 5, [2,2], soybean_class, 5, 0.5, 1.492, 1.492, False)
PSO_network_classification7 = PSO(soybean_training_7, soybean_7, 5, [2,2], soybean_class, 5, 0.5, 1.492, 1.492, False)
PSO_network_classification8 = PSO(soybean_training_8, soybean_8, 5, [2,2], soybean_class, 5, 0.5, 1.492, 1.492, False)
PSO_network_classification9 = PSO(soybean_training_9, soybean_9, 5, [2,2], soybean_class, 5, 0.5, 1.492, 1.492, False)
PSO_network_classification0 = PSO(soybean_training_10, soybean_10, 5, [2,2], soybean_class, 5, 0.5, 1.492, 1.492, False)

PSO_result_classification1 = forward_propagate(soybean_1, PSO_network_classification1, soybean_class, False)
PSO_result_classification2 = forward_propagate(soybean_2, PSO_network_classification2, soybean_class, False)
PSO_result_classification3 = forward_propagate(soybean_3, PSO_network_classification3, soybean_class, False)
PSO_result_classification4 = forward_propagate(soybean_4, PSO_network_classification4, soybean_class, False)
PSO_result_classification5 = forward_propagate(soybean_5, PSO_network_classification5, soybean_class, False)
PSO_result_classification6 = forward_propagate(soybean_6, PSO_network_classification6, soybean_class, False)
PSO_result_classification7 = forward_propagate(soybean_7, PSO_network_classification7, soybean_class, False)
PSO_result_classification8 = forward_propagate(soybean_8, PSO_network_classification8, soybean_class, False)
PSO_result_classification9 = forward_propagate(soybean_9, PSO_network_classification9, soybean_class, False)
PSO_result_classification0 = forward_propagate(soybean_10, PSO_network_classification0, soybean_class, False)

PSO_result_regression1 = SSR(PSO_result_regression1[0],PSO_result_regression1[1])
PSO_result_regression2 = SSR(PSO_result_regression2[0],PSO_result_regression2[1])
PSO_result_regression3 = SSR(PSO_result_regression3[0],PSO_result_regression3[1])
PSO_result_regression4 = SSR(PSO_result_regression4[0],PSO_result_regression4[1])
PSO_result_regression5 = SSR(PSO_result_regression5[0],PSO_result_regression5[1])
PSO_result_regression6 = SSR(PSO_result_regression6[0],PSO_result_regression6[1])
PSO_result_regression7 = SSR(PSO_result_regression7[0],PSO_result_regression7[1])
PSO_result_regression8 = SSR(PSO_result_regression8[0],PSO_result_regression8[1])
PSO_result_regression9 = SSR(PSO_result_regression9[0],PSO_result_regression9[1])
PSO_result_regression0 = SSR(PSO_result_regression0[0],PSO_result_regression0[1])

PSO_result_classification1 = zero_loss_func(PSO_result_classification1[0],PSO_result_classification1[1])
PSO_result_classification2 = zero_loss_func(PSO_result_classification2[0],PSO_result_classification2[1])
PSO_result_classification3 = zero_loss_func(PSO_result_classification3[0],PSO_result_classification3[1])
PSO_result_classification4 = zero_loss_func(PSO_result_classification4[0],PSO_result_classification4[1])
PSO_result_classification5 = zero_loss_func(PSO_result_classification5[0],PSO_result_classification5[1])
PSO_result_classification6 = zero_loss_func(PSO_result_classification6[0],PSO_result_classification6[1])
PSO_result_classification7 = zero_loss_func(PSO_result_classification7[0],PSO_result_classification7[1])
PSO_result_classification8 = zero_loss_func(PSO_result_classification8[0],PSO_result_classification8[1])
PSO_result_classification9 = zero_loss_func(PSO_result_classification9[0],PSO_result_classification9[1])
PSO_result_classification0 = zero_loss_func(PSO_result_classification0[0],PSO_result_classification0[1])

PSO_regression_mean = (PSO_result_regression1+PSO_result_regression2+PSO_result_regression3+PSO_result_regression4+PSO_result_regression5+PSO_result_regression6+PSO_result_regression7+PSO_result_regression8+PSO_result_regression9+PSO_result_regression0)/10
print("regression:",PSO_regression_mean)

numerator = PSO_result_classification1[0] + PSO_result_classification2[0] + PSO_result_classification3[0] + PSO_result_classification4[0] + PSO_result_classification5[0] + PSO_result_classification6[0] + PSO_result_classification7[0] + PSO_result_classification8[0] + PSO_result_classification9[0] + PSO_result_classification0[0]
denominator = PSO_result_classification1[1] + PSO_result_classification2[1] + PSO_result_classification3[1] + PSO_result_classification4[1] + PSO_result_classification5[1] + PSO_result_classification6[1] + PSO_result_classification7[1] + PSO_result_classification8[1] + PSO_result_classification9[1] + PSO_result_classification0[1]
print("classification:",numerator / (numerator + denominator))

# import matplotlib.pyplot as plt
import numpy as np
import random
import pandas as pd
 
class GeneticAlgorithm:
 
    def __init__(self, Neural_Network, nbrGenerations, populationSize, elitismSize, tournamentPoolSize, mutationRate,crossoverRate,test):
        self.NeuralNetwork = Neural_Network
        self.nbrGenerations = nbrGenerations
        self.populationSize = populationSize
        self.elitismSize = elitismSize
        self.tournamentPoolSize = tournamentPoolSize
        self.mutationRate = mutationRate
        self.crossoverRate = crossoverRate
        self.test=test
        self.temp_population = Population(self.populationSize,
                                          self.NeuralNetwork)  # temp population is The Initial Population
        self.generationCounter = 0
        self.mean_squared_errors = []
        if not test:
            for i in range(self.nbrGenerations):
                if not self.NeuralNetwork.classification:
                  print(f'Generation Number : {self.generationCounter}  , mean Squared Error : {self.temp_population.fittest.fitness}')
                else:
                  print(f'Generation Number : {self.generationCounter}  , cross entropy : {self.temp_population.fittest.fitness}')
                self.mean_squared_errors.append(self.temp_population.fittest.fitness)
                self.temp_population = self.reproduction(self.temp_population)
                self.generationCounter += 1
            # Update The Weights and The Biases
                self.NeuralNetwork.weights = self.temp_population.fittest.weights
                self.NeuralNetwork.biases = self.temp_population.fittest.biases# plt.plot(np.arange(generationCounter), mean_squared_errors)
        else:
            self.mean_squared_errors.append(self.temp_population.fittest.fitness)
            self.temp_population = self.reproduction(self.temp_population)
            # self.temp_population = self.reproduction(self.temp_population)
            self.generationCounter += 1
            # Update The Weights and The Biases
            self.NeuralNetwork.weights = self.temp_population.fittest.weights
            self.NeuralNetwork.biases = self.temp_population.fittest.biases
        if not self.NeuralNetwork.classification:
          print(f'Generation Number : {self.generationCounter}  , mean Squared Error : {self.temp_population.fittest.fitness}')
        else:
          print(f'Generation Number : {self.generationCounter}  , cross entropy : {self.temp_population.fittest.fitness}')
        
    def reproduction(self, population):
        temp_chromosomes = []
        temp_chromosomes[:self.elitismSize] = population.getNFittestChromosome(self.elitismSize)
 
        if not self.test:
          for i in range(int((self.populationSize-self.elitismSize)/2)):
            parent1 = self.tournamentSelection(population)
            parent2 = self.tournamentSelection(population)
 
            child1, child2 = self.NPointCrossOver(parent1, parent2,self.crossoverRate)
 
            child1 = self.Mutation(child1)
            child2 = self.Mutation(child2)
 
            temp_chromosomes.append(child1)
            temp_chromosomes.append(child2)
        else:
            print(f"elitism fittest fitness: {temp_chromosomes[0].fitness}")
            parent1 = self.tournamentSelection(population)
            print("chromosome 1 fitness: ", parent1.fitness)
            parent2 = self.tournamentSelection(population)
            print("chromosome 2 fitness: ", parent2.fitness)
 
            child1, child2 = self.NPointCrossOver(parent1, parent2,self.crossoverRate)
            print("chromosome 1 fitness: ", child1.fitness)
            print("chromosome 2 fitness: ", child2.fitness)
 
            child1 = self.Mutation(child1)
            print("chromosome 1 fitness after mutation: ", child1.fitness)
            child2 = self.Mutation(child2)
            print("chromosome 1 fitness after mutation: ", child2.fitness)
 
            temp_chromosomes.append(child1)
            temp_chromosomes.append(child2)        
        new_pop = Population(self.populationSize, self.NeuralNetwork, chromosomes=temp_chromosomes)
 
        return new_pop
 
    def tournamentSelection(self, population):
        tournamentPool = []
        for i in range(self.tournamentPoolSize):
            index = random.randint(0, self.populationSize - 1)
            tournamentPool.append(population.chromosomes[index])
            if self.test:
              print(f"tournament pool selection ({i}): {population.chromosomes[index].fitness}")
        tournamentPool.sort(key=lambda x: x.fitness)
        return tournamentPool[0]
 
    def Mutation(self, child):
        if random.random() < self.mutationRate:
            flat_weights = self.Flatten(child.weights)
            flat_biases = self.Flatten(child.biases)
            randint = random.randint(0, len(flat_weights) - 1)
            flat_weights[randint] = np.random.randn()
            temp_weights = self.Unflatten2(flat_weights, self.NeuralNetwork.layers)
            child.weights = temp_weights
            randint = random.randint(0, len(flat_biases) - 1)
            flat_biases[randint] = np.random.randn()
            temp_biases = self.Unflatten2(flat_biases, self.NeuralNetwork.layers,True)
            child.biases = temp_biases
            child.fitness = child.calculateFitness()
        return child
 
    def NPointCrossOver(self, parent1, parent2,n):
        flat_weights_parent1 = self.Flatten(parent1.weights)
        flat_weights_parent2 = self.Flatten(parent2.weights)
        flat_parent1_biases = self.Flatten(parent1.biases)
        flat_parent2_biases = self.Flatten(parent2.biases)
        flat_weights_child1,flat_weights_child2 = self.Crossover(flat_weights_parent1,flat_weights_parent2)
        flat_biases_child1,flat_biases_child2 = self.Crossover(flat_parent1_biases,flat_parent2_biases)
        for i in range(n-1):
            flat_weights_child1,flat_weights_child2 = self.Crossover(flat_weights_child1,flat_weights_child2)
            flat_biases_child1,flat_biases_child2 = self.Crossover(flat_biases_child1,flat_biases_child2)
        child1_Weights = self.Unflatten2(flat_weights_child1, self.NeuralNetwork.layers)
        child2_Weights = self.Unflatten2(flat_weights_child2, self.NeuralNetwork.layers)
        child1_biases = self.Unflatten2(flat_biases_child1, self.NeuralNetwork.layers,True)
        child2_biases = self.Unflatten2(flat_biases_child2, self.NeuralNetwork.layers,True)
        child1 = Chromosome(NeuralNetwork=self.NeuralNetwork, weights=child1_Weights, biases=child1_biases)
        child2 = Chromosome(NeuralNetwork=self.NeuralNetwork, weights=child2_Weights, biases=child2_biases)
        return child1,child2
 
    def Crossover(self, chromosome1, chromosome2):
        try:
            split = np.random.randint(0, len(chromosome1) - 1)
            temp_gene1 = np.array(chromosome1[:split].tolist() + chromosome2[split:].tolist()) 
            temp_gene2 = np.array(chromosome2[:split].tolist() + chromosome1[split:].tolist())
            return temp_gene1, temp_gene2
        except:
            print("returning unchanged chromosomes")
            return chromosome1, chromosome2
 
    def Flatten(self, weights):
        return np.concatenate([a.flatten() for a in weights])
 
    def Unflatten(self, flattened, shapes):
        newarray = []
        index = 0
        for shape in shapes:
            size = np.product(shape)
            newarray.append(flattened[index: index + size].reshape(shape))
            index += size
        return newarray
    def Unflatten2(self, flattened, shapes, biases=None):
        newarray = []
        index = 0
        for i in range(1,len(shapes)):
            shape = (shapes[i], shapes[i-1])
            if biases is not None:
                shape = (shapes[i], 1)
            size = np.product(shape)
            newarray.append(flattened[index: index + size].reshape(shape))
            index += size
        return newarray

class NeuralNetwork:
 
    def __init__(self, x, y, layers, classification):
 
        self.x_train = x.T
        self.y_train = y.reshape(1, y.shape[0])
        if layers[len(layers) - 1] > 1:
            self.y_train = np.zeros((len(y), layers[len(layers) - 1]))
            self.y_train[list(range(len(y))), y.astype(int).tolist()] = 1
        self.layers = layers
        self.weights = []
        self.biases = []
        self.classification = classification
 
        for i in range(1, len(self.layers)):
            self.weights.append(np.random.randn(self.layers[i], self.layers[i - 1]))
            self.biases.append(np.random.randn(self.layers[i], 1))
 
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
 
    def softmax(self, vector):
        e = np.exp(vector.astype(float))
        return e / e.sum(axis=0)
 
    def mean_Squared_Error(self, y_hat):
        return (1 / self.y_train.shape[1]) * np.sum((self.y_train - y_hat) ** 2)
 
    def accuracy(self, y_hat):
        return np.count_nonzero(np.abs(self.y_train - y_hat) < .5) / y_hat.shape[1]
 
    def cross_Entropy(self, y_hat):
        if self.layers[len(self.layers) - 1] > 1:
            return (-1 / self.y_train.shape[0]) * np.sum(self.y_train * np.log(y_hat).transpose())
        else:
            return (-1 / self.y_train.shape[0]) * np.sum(
                self.y_train * np.log(y_hat) + (1 - self.y_train) * np.log(1 - y_hat))
 
    def feedforward(self, weights=None, biases=None):
        if weights == None and biases == None:
            weights, biases = self.weights, self.biases
        self.activations = {'A0': self.x_train}
        for i in range(1, len(self.layers)):
            z = np.dot(weights[i - 1], self.activations['A' + str(i - 1)]) + biases[i - 1]
            if i == (len(self.layers) - 1):
                if self.classification:
                    if self.layers[i] > 1:
                        self.activations['A' + str(i)] = self.softmax(z)
                    else:
                        self.activations['A' + str(i)] = self.sigmoid(z)
                else:
                    self.activations['A' + str(i)] = z
            else:
                self.activations['A' + str(i)] = self.sigmoid(z.astype(float))
        return self.activations['A' + str(len(self.layers) - 1)]

class Chromosome:
    def __init__(self, NeuralNetwork, weights=None, biases=None):
        self.NeuralNetwork = NeuralNetwork
        self.weights = weights if weights != None else [
            np.random.randn(self.NeuralNetwork.layers[i], self.NeuralNetwork.layers[i - 1]) for i in
            range(1, len(self.NeuralNetwork.layers))]
        self.biases = biases if biases != None else [np.random.randn(self.NeuralNetwork.layers[i], 1) for i in
                                                     range(1, len(self.NeuralNetwork.layers))]
        self.fitness = self.calculateFitness()
 
    def calculateFitness(self, ):
        y_hat = self.NeuralNetwork.feedforward(weights=self.weights, biases=self.biases)
        classification=self.NeuralNetwork.classification
        if classification:
            fitness = self.NeuralNetwork.cross_Entropy(y_hat)
        else:
            fitness = self.NeuralNetwork.mean_Squared_Error(y_hat)
        return fitness

class Population:
    def __init__(self, populationSize, NeuralNetwork, chromosomes=None):
        self.populationSize = populationSize
        self.NeuralNetwork = NeuralNetwork
        self.chromosomes = chromosomes if chromosomes != None else [Chromosome(NeuralNetwork=self.NeuralNetwork) for i
                                                                    in range(self.populationSize)]
        self.fittest = self.getFittestChromosome()
 
    def getFittestChromosome(self):
        if None in self.chromosomes:
            print('List Contains a None')
        self.chromosomes.sort(key=lambda x: x.fitness)
        return self.chromosomes[0]
 
    def getNFittestChromosome(self, n):
        self.chromosomes.sort(key=lambda x: x.fitness)
        return self.chromosomes[:n]

class DataHandler:
 
    def __int__(self):
        pass
 
    def bc_data(self):
        df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', delimiter=",", header=None)
        df = df.drop(labels=0, axis=1)
        # search for rows with ? that need to be removed
        # They are all in column 5 so that makes it simpler
        badRows = []
        for i in range(df.shape[0]):
            if df.iat[i, 5] == "?":
                # print(i,5)
                badRows.append(i)
        # remove the rows with missing data
        df = df.drop(labels=badRows)
 
        # make the values integers because they aren't for some reason
        df = df.apply(pd.to_numeric)
        df = df.reset_index(drop=True)
        df = df.set_axis(list(range(df.shape[1])), axis=1, inplace=False)
        df = self.normalizer(df)
        classes = df[df.shape[1] - 1].unique()
        for i in range(len(classes)):
            df.loc[df[df.shape[1] - 1] == classes[i], df.shape[1] - 1] = i
        return df
 
    def ff_data(self):
        df = pd.read_table("https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv", header=None, sep=",")
 
        # Drops the column names
        df = df.drop(labels=0, axis=0)
        df = df.reset_index(drop=True)
        for i in range(df.shape[0]):
            # replace month with an integer
            if df.iat[i, 2] == "jan":
                df.iat[i, 2] = 1
            elif df.iat[i, 2] == "feb":
                df.iat[i, 2] = 2
            elif df.iat[i, 2] == "mar":
                df.iat[i, 2] = 3
            elif df.iat[i, 2] == "apr":
                df.iat[i, 2] = 4
            elif df.iat[i, 2] == "may":
                df.iat[i, 2] = 5
            elif df.iat[i, 2] == "jun":
                df.iat[i, 2] = 6
            elif df.iat[i, 2] == "jul":
                df.iat[i, 2] = 7
            elif df.iat[i, 2] == "aug":
                df.iat[i, 2] = 8
            elif df.iat[i, 2] == "sep":
                df.iat[i, 2] = 9
            elif df.iat[i, 2] == "oct":
                df.iat[i, 2] = 10
            elif df.iat[i, 2] == "nov":
                df.iat[i, 2] = 11
            elif df.iat[i, 2] == "dec":
                df.iat[i, 2] = 12
 
            # replace day with an integer
            if df.iat[i, 3] == "sun":
                df.iat[i, 3] = 1
            elif df.iat[i, 3] == "mon":
                df.iat[i, 3] = 2
            elif df.iat[i, 3] == "tue":
                df.iat[i, 3] = 3
            elif df.iat[i, 3] == "wed":
                df.iat[i, 3] = 4
            elif df.iat[i, 3] == "thu":
                df.iat[i, 3] = 5
            elif df.iat[i, 3] == "fri":
                df.iat[i, 3] = 6
            elif df.iat[i, 3] == "sat":
                df.iat[i, 3] = 7
 
        df = df.apply(pd.to_numeric)
        df = self.normalizer2(df)
        return df
 
    def abalone_data(self):
        abalone = pd.read_table("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data", delimiter=',', header=None)
        categories = abalone[0].unique()
        for i in range(len(categories)):
            abalone.iloc[abalone[0] == categories[i], 0] = i
        abalone = self.normalizer(abalone)
        return abalone
 
    def soybean_data(self):
        df = pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data', delimiter=",", header=None)
        # remove the date column
        df = df.drop(labels=0, axis=1)
        # remove the "D" so classes are integers
        for i in range(df.shape[0]):
            df.iat[i, 34] = int(df.iat[i, 34][1])
        badCols = []
        for i in range(df.shape[1]):
            if len(df.iloc[:, i].unique()) == 1:  # not an empty column
                badCols.append(df.columns[i])
        df = df.drop(badCols, axis=1)
 
        attr_num = df.shape
        soybean = df.set_axis(list(range(attr_num[1])), axis=1, inplace=False)
        for i in range(soybean[soybean.shape[1] - 1].nunique()):
            soybean.loc[soybean[soybean.shape[1] - 1] == (i + 1), soybean.shape[1] - 1] = i
        soybean = self.normalizer(soybean)
        return soybean
 
    def machine_data(self):
        df = pd.read_table('machine.data', delimiter=',', header=None)
        df = df.drop([0, 1], axis=1)
        attr_num = df.shape
        machine = df.set_axis(list(range(attr_num[1])), axis=1, inplace=False)
        machine = machine.drop(7, axis=1)
        machine = self.normalizer2(machine)
        return machine
 
    def glass_data(self):
        df = pd.read_csv('glass.data', delimiter=",", header=None)
        # remove the ids column since we don't need it
        df = df.drop(labels=0, axis=1)
        categories = df.iloc[:, -1].unique()
        for i in range(len(categories)):
            df.iloc[df.iloc[:, -1] == categories[i],-1] = i
        df = df.set_axis(list(range(df.shape[1])), axis=1, inplace=False)
        df = self.normalizer(df)
        return df
 
    def normalizer(self, df):
        # for each column...
        for i in range(df.shape[1]-1):
            df[i]=(df[i]-df[i].mean())/df[i].std()
        return df
    def normalizer2(self, df):
        # for each column...
        for i in range(df.shape[1]):
            df[i]=(df[i]-df[i].mean())/df[i].std()
        return df
    def testTrainingSplitRegression(self,data):
        dim = data.shape
        data = data.sort_values(dim[1] - 1)
        fold = [[]] * 10
        for i in range(10):
            fold[i] = data.iloc[list(range(i, dim[0], 10))]
        return fold
 
    def testTrainingSplitClass(self,data):
        dim = data.shape
        fold = [[]] * 10
        N = dim[0] / 10
        for i in range(10):
            try:
                fold[i] = data.groupby(dim[1] - 1, group_keys=False).apply(
                    lambda x: x.sample(int(np.rint(N * len(x) / len(data)))))
            except:
                fold[i]=data
            data = data.drop(fold[i].index)
        return fold

import numpy as np
import pandas as pd
dataHandler=DataHandler()
df = dataHandler.soybean_data()
fitness=[]
accuracy=[]
folds=dataHandler.testTrainingSplitClass(data=df)
for i in range(1):
    train = pd.concat(folds[:i] + folds[i + 1:], axis=0)
    df=train.to_numpy()
    x=df[:,:-1]
    y=df[:,-1]
    nodes=(x.shape[1] ,16,16,4)
    Neural_Network = NeuralNetwork(x , y , layers=nodes,classification=True)
    print(f"fold:{i}")
    Genetic_Algorithm = GeneticAlgorithm(Neural_Network = Neural_Network , nbrGenerations = 10,
                                         populationSize = 100, elitismSize = 4, tournamentPoolSize = 5, mutationRate = .5,crossoverRate=10,test=True)
    fittest=Genetic_Algorithm.temp_population.fittest
    df = folds[i].to_numpy()
    x = df[:, :-1]
    y = df[:, -1]
    fittest.NeuralNetwork=NeuralNetwork(x,y,layers=nodes,classification=True)
    y_hat = fittest.NeuralNetwork.feedforward(fittest.weights, fittest.biases)
    accuracy.append(np.count_nonzero(np.argmax(y_hat, axis=0) == y) / len(y))
    fitness.append(fittest.calculateFitness())
print("cross entropy: ",fitness)
print("accuracy: ", accuracy)

df = dataHandler.ff_data()
fitness=[]
accuracy=[]
folds=dataHandler.testTrainingSplitRegression(data=df)
for i in range(1):
    train = pd.concat(folds[:i] + folds[i + 1:], axis=0)
    df=train.to_numpy()
    x=df[:,:-1]
    y=df[:,-1]
    nodes=(x.shape[1] ,16,16,1)
    Neural_Network = NeuralNetwork(x , y , layers=nodes,classification=False)
    print(f"fold:{i}")
    Genetic_Algorithm = GeneticAlgorithm(Neural_Network = Neural_Network , nbrGenerations = 10,
                                         populationSize = 100, elitismSize = 4, tournamentPoolSize = 5, mutationRate = .5,crossoverRate=10,test=False)
    fittest=Genetic_Algorithm.temp_population.fittest
    df = folds[i].to_numpy()
    x = df[:, :-1]
    y = df[:, -1]
    fittest.NeuralNetwork=NeuralNetwork(x,y,layers=nodes,classification=False)
    fitness.append(fittest.calculateFitness())
print("mean squared error: ",fitness)

import numpy as np
import pandas as pd
dataHandler=DataHandler()
df = dataHandler.soybean_data()
fitness=[]
accuracy=[]
folds=dataHandler.testTrainingSplitClass(data=df)
# folds=MLP.testTrainingSplitRegression(self=MLP,data=df)
for i in range(1):
    train = pd.concat(folds[:i] + folds[i + 1:], axis=0)
    df=train.to_numpy()
    x=df[:,:-1]
    y=df[:,-1]
    nodes=(x.shape[1] ,4)
    Neural_Network = NeuralNetwork(x , y , layers=nodes,classification=True)
    print(f"fold:{i}")
    Genetic_Algorithm = GeneticAlgorithm(Neural_Network = Neural_Network , nbrGenerations = 10,
                                         populationSize = 100, elitismSize = 4, tournamentPoolSize = 5, mutationRate = .5,crossoverRate=10,test=False)
    fittest=Genetic_Algorithm.temp_population.fittest
    df = folds[i].to_numpy()
    x = df[:, :-1]
    y = df[:, -1]
    fittest.NeuralNetwork=NeuralNetwork(x,y,layers=nodes,classification=True)
    y_hat = fittest.NeuralNetwork.feedforward(fittest.weights, fittest.biases)
    accuracy.append(np.count_nonzero(np.argmax(y_hat, axis=0) == y) / len(y))
    #accuracy.append(np.count_nonzero(y_hat.round()==y)/len(y))
    fitness.append(fittest.calculateFitness())
print("average cross entropy: ",np.sum(fitness)/10)
print("average accuracy: ", np.sum(accuracy)/10)
print("")

import numpy as np
import pandas as pd
dataHandler=DataHandler()
df = dataHandler.soybean_data()
fitness=[]
accuracy=[]
folds=dataHandler.testTrainingSplitClass(data=df)
for i in range(10):
    train = pd.concat(folds[:i] + folds[i + 1:], axis=0)
    df=train.to_numpy()
    x=df[:,:-1]
    y=df[:,-1]
    nodes=(x.shape[1] ,16,16,4)
    Neural_Network = NeuralNetwork(x , y , layers=nodes,classification=True)
    print(f"fold:{i}")
    Genetic_Algorithm = GeneticAlgorithm(Neural_Network = Neural_Network , nbrGenerations = 10,
                                         populationSize = 100, elitismSize = 4, tournamentPoolSize = 5, mutationRate = .5,crossoverRate=10,test=False)
    fittest=Genetic_Algorithm.temp_population.fittest
    y_hat = fittest.NeuralNetwork.feedforward(fittest.weights, fittest.biases)
    accuracy.append(np.count_nonzero(np.argmax(y_hat, axis=0) == y) / len(y))    
    df = folds[i].to_numpy()
    x = df[:, :-1]
    y = df[:, -1]
    Genetic_Algorithm.NeuralNetwork=NeuralNetwork(x,y,layers=nodes,classification=True)
    fittest=Genetic_Algorithm.temp_population.fittest
    fittest.NeuralNetwork=NeuralNetwork(x,y,layers=nodes,classification=True)    
    print("fittest cross entropy test data: ",fittest.calculateFitness())
    y_hat = fittest.NeuralNetwork.feedforward(fittest.weights, fittest.biases)
    accuracy.append(np.count_nonzero(np.argmax(y_hat, axis=0) == y) / len(y))
    fitness.append(fittest.calculateFitness())
print("cross entropy: ",fitness)
print("accuracy: ", accuracy)