# -*- coding: utf-8 -*-
"""CSCI 447 Project 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EjqDkBnOmF_m-UcYzAFRPfuk4Bb3MjVm
"""

# Project 2
#Overall for each part (coding, writing, and video segements) were equally distributed between the two of us Steven Ohms and Gak Roppongi
#A majority of the time we were working together in person, and constantly looked over/edited each others code
#Note whenever a cell says one of our names, we assume a majority of the work in the cell was done by that individual, but was still split between the two of us

import numpy as np
import pandas as pd
import math
import random
import matplotlib.pyplot as plt

#Gak Roppongi
#importing the data sets, and deal with the missing values and one-hot encoding
#Breast cancer, Glass dataset, Soybean datasets are classification
#Abalone, Computer, Forest datasets are regression

breast_cancer_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', 
                                    names = ["radius", "texture", "perimeter", "area", "smoothness", "compactness", "concavity", "concave points", 
                                             "symmetry", "fractal dimension", "class"])
#"symmetry" may be label-encoded, but I'm not sure 
#the sum of number of instances of 1 ... 7 in "class" attribute is 699 = number of all instances
#therefore, "class" should be the categorical attribute
#no categorical value is missed
#There are 16 instances in Groups 1 to 6 that contain a single missing (i.e., unavailable) attribute value, now denoted by "?".  
#699 * 11

glass_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data', 
                            names = ["id", "RI", "Na", "Mg", "Al", "Si", "K", "Ca", "Ba",  "Fe", "type of glass"])
#the "type of glass" is label-encoded
#everything else is numerical attribute
#Number of Attributes: 10 (including an Id#) plus the class attribute -- all attributes are continuously valued
#the last attribute, "class", has label-encoded for 7 categories from 1~7
#no missing attributes values
#214 * 11 

soybean_dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data', 
                              names = ["date", "plant-stand", "precip", "temp", "hail", "crop-hist", "area-damaged", 
                                       "severity", "seed-tmt", "germination", "plant-growth", "leaves", "leafspots-halo", 
                                       "leafspots-marg", "leadspot-size", "leaf-shread", "lead-malf", "lead-mild", "stem", 
                                       "lodging", "stem-cankers", "fruiting-bodies", "external-decay", "mycelium", "int-discolor", 
                                       "sclerotia", "fruit-pods", "fruit-spots", "seed", "mold-growth", "seed-discolor", "seed-size", "shriveling", "roots"])
#many of the attributes are string data type and label-encoded
#no missing attribute values
#all values have been normalized
#"roots" is the categorical value
#46 * 36

abalone_dataset = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data",
                              names = ["sex", "length", "diameter", "height", "whole weight", "shucked weight", "viscera weight", "shell weight", "rings"])

#no missing values
#ring is the class
#4177 instances
#one hot encode sex, then hamming distance
#8 features


computer_dataset = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/machine.data",
                               names = ["vendor name", "model name", "MYCT", "MMIN", "MMAX", "CACH", "CHMIN", "CHMAX", "PRP", "ERP"])
#no missing values
#PRP is the class here
#vendor name and model name are string data
#everything else is numerical data
#209 instances
#10 features
#remove vendor/model name

forest_dataset = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv",
                             names = ["X", "Y", "month", "day", "FFMC", "DMC", "DC", "ISI", "temp", "RH", "wind", "rain", "area"])
#no missing values
#517 instances
#12 features
#area - the burned area of the forest (in ha): 0.00 to 1090.84 
#(this output variable is very skewed towards 0.0, 
#thus it may make sense to model with the logarithm transform).
#area is the class here
#month and day are cyclical, so for day, distance between sunday and moday would be 1, or 6, but 1 is lower

breast_class = np.array([2, 4])
glass_class = np.array([1, 2, 3, 5, 6, 7])
soybean_class = np.array(['D1', 'D2', 'D3', 'D4'])
abalone_class = "rings"
computer_class = "PRP"
forest_class = "area"

#do not need model name or vendor name in the computer dataset
#row dataset does not contain any data, just attribute names
computer_dataset = computer_dataset.drop("vendor name", axis=1);
computer_dataset = computer_dataset.drop("model name", axis=1);

forest_dataset = forest_dataset.drop(index = 0)


soybean_dataset = soybean_dataset.reset_index(drop = True)

breast_cancer_dataset.sample(frac=1)
breast_cancer_dataset.sort_values(by=["class"], inplace=True)
glass_dataset.sort_values(by=["type of glass"], inplace=True)
soybean_dataset.sort_values(by=["roots"], inplace=True)
abalone_dataset.sort_values(by=["rings"], inplace=True)
computer_dataset.sort_values(by=["PRP"], inplace=True)
forest_dataset.sort_values(by=["area"], inplace=True)

#turns the abalone_dataset into something easier to read, and turns the days of the week and months into integers so distance can be calculated
abalone_dataset["sex"] = abalone_dataset["sex"].replace(["M"], ["Male"])
abalone_dataset["sex"] = abalone_dataset["sex"].replace(["F"], ["Female"])
abalone_dataset["sex"] = abalone_dataset["sex"].replace(["I"], ["Infant"])

forest_dataset["month"] = forest_dataset["month"].replace(["jan"], [1])
forest_dataset["month"] = forest_dataset["month"].replace(["feb"], [2])
forest_dataset["month"] = forest_dataset["month"].replace(["mar"], [3])
forest_dataset["month"] = forest_dataset["month"].replace(["apr"], [4])
forest_dataset["month"] = forest_dataset["month"].replace(["may"], [5])
forest_dataset["month"] = forest_dataset["month"].replace(["jun"], [6])
forest_dataset["month"] = forest_dataset["month"].replace(["jul"], [7])
forest_dataset["month"] = forest_dataset["month"].replace(["aug"], [8])
forest_dataset["month"] = forest_dataset["month"].replace(["sep"], [9])
forest_dataset["month"] = forest_dataset["month"].replace(["oct"], [10])
forest_dataset["month"] = forest_dataset["month"].replace(["nov"], [11])
forest_dataset["month"] = forest_dataset["month"].replace(["dec"], [12])

forest_dataset["day"] = forest_dataset["day"].replace(["mon"], [1])
forest_dataset["day"] = forest_dataset["day"].replace(["tue"], [2])
forest_dataset["day"] = forest_dataset["day"].replace(["wed"], [3])
forest_dataset["day"] = forest_dataset["day"].replace(["thu"], [4])
forest_dataset["day"] = forest_dataset["day"].replace(["fri"], [5])
forest_dataset["day"] = forest_dataset["day"].replace(["sat"], [6])
forest_dataset["day"] = forest_dataset["day"].replace(["sun"], [7])

forest_dataset = forest_dataset.astype(float)

#Steven Ohms
#this section of the code will turn all the vallues in the dataframes into numeric values
#all values are saved as str in the first place
#we will turn them into numeric

breast_cancer_dataset = breast_cancer_dataset.replace(["?"], np.nan)
breast_cancer_dataset['concavity'] = breast_cancer_dataset['concavity'].astype(float)


#glass, iris, and  soybean don't have the missing value

#this function does mean-filling of the dataset for the missing value
#df is the dataframe we manipulate
#attribute is the attribute in the dataframe we are manipulating

def mean_filling(df, attribute):
  mean = df[attribute].mean()
  df[attribute] = df[attribute].replace(np.nan, mean)
  return df

breast_cancer_dataset = mean_filling(breast_cancer_dataset, "concavity")

#Gak Roppongi
#this function implements the one-hot encoding
#df is the dataset we manipulate
#attribute is the attribute that we manipulate in str
#returns the one-hot encoded dataframe

def onehot(df, attribute):
  # Get one hot encoding of columns B
  one_hot = pd.get_dummies(df[attribute])
  # Drop column B as it is now encoded
  df = df.drop(attribute,axis = 1)
  # Join the encoded df
  df = df.join(one_hot)
  return df

abalone_dataset = onehot(abalone_dataset, "sex")

glass_dataset = onehot(glass_dataset, "type of glass")
soybean_dataset = onehot(soybean_dataset, "roots")
breast_cancer_dataset = onehot(breast_cancer_dataset, "class")

#merges all the portions together to create a testing set
 
def train_merge(one, two, three, four, five, six, seven, eight, nine):
  final_train = pd.merge(one, two, how="outer")
  final_train = pd.merge(final_train, three, how="outer")
  final_train = pd.merge(final_train, four, how="outer")
  final_train = pd.merge(final_train, five, how="outer")
  final_train = pd.merge(final_train, six, how="outer")
  final_train = pd.merge(final_train, seven, how="outer")
  final_train = pd.merge(final_train, eight, how="outer")
  final_train = pd.merge(final_train, nine, how="outer")
 
  return final_train

def zero_loss_func(result):
  correct = 0
  incorrect = 0
  for i in range(len(result)):
    if result[i][0] == result[i][1]:
      correct = correct+1
    elif result[i][0] != result[i][1]:
      incorrect = incorrect + 1
  
  return [correct, incorrect]

"""
takes the matrix as the argument
calculates the MSE out of the matrix returned from the KNN on regression
"""
def Mean_Squared_Error(matrix):
  MSE = 0
  n = len(matrix)
  for i in range(n):
    MSE = MSE + (matrix[i][0] - matrix[i][1])**2
  
  MSE = MSE/n
  return MSE

def set_split(dataset):
  i = 0
  j = 0
  HP_set = dataset.iloc[0:0]
  one = dataset.iloc[0:0]
  two = dataset.iloc[0:0]
  three = dataset.iloc[0:0]
  four = dataset.iloc[0:0]
  five = dataset.iloc[0:0]
  six = dataset.iloc[0:0]
  seven = dataset.iloc[0:0]
  eight = dataset.iloc[0:0]
  nine = dataset.iloc[0:0]
  ten = dataset.iloc[0:0]
  
  for i in range(dataset.shape[0]):
    if (i % 10 == 0):
      
      HP_set = pd.merge(HP_set, dataset.iloc[i:(i+1)], how="outer")
    else:
      h = j % 10
      if h == 0:
        one = pd.merge(one, dataset.iloc[i:(i+1)], how="outer")
      if h == 1:
        two = pd.merge(two, dataset.iloc[i:(i+1)], how="outer")
      if h == 2:
        three = pd.merge(three, dataset.iloc[i:(i+1)], how="outer")
      if h == 3:
        four = pd.merge(four, dataset.iloc[i:(i+1)], how="outer")
      if h == 4:
        five = pd.merge(five, dataset.iloc[i:(i+1)], how="outer")
      if h == 5:
        six = pd.merge(six, dataset.iloc[i:(i+1)], how="outer")
      if h == 6:
        seven = pd.merge(seven, dataset.iloc[i:(i+1)], how="outer")
      if h == 7:
        eight = pd.merge(eight, dataset.iloc[i:(i+1)], how="outer")
      if h == 8:
        nine = pd.merge(nine, dataset.iloc[i:(i+1)], how="outer")
      if h == 9:
        ten = pd.merge(ten, dataset.iloc[i:(i+1)], how="outer")
      j = j + 1
 
  return HP_set, one, two, three, four, five, six, seven, eight, nine, ten

glass_HP_set, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine, glass_ten = set_split(glass_dataset)
soybean_HP_set, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine, soybean_ten = set_split(soybean_dataset)
breast_HP_set, breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine, breast_ten = set_split(breast_cancer_dataset)
forest_HP_set, forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine, forest_ten = set_split(forest_dataset)
computer_HP_set, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine, computer_ten = set_split(computer_dataset)
abalone_HP_set, abalone_one, abalone_two, abalone_three, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_eight, abalone_nine, abalone_ten = set_split(abalone_dataset)

#For Thw Video
computer_training_one = train_merge(computer_ten, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_two = train_merge(computer_ten, computer_one, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_three = train_merge(computer_ten, computer_one, computer_two, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_four = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_five = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_six = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_seven, computer_eight, computer_nine)
computer_training_seven = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_eight, computer_nine)
computer_training_eight = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_nine)
computer_training_nine = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight)
computer_training_ten = train_merge(computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)

soybean_training_one = train_merge(soybean_ten, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_two = train_merge(soybean_ten, soybean_one, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_three = train_merge(soybean_ten, soybean_one, soybean_two, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_four = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_five = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_six = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_seven, soybean_eight, soybean_nine)
soybean_training_seven = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_eight, soybean_nine)
soybean_training_eight = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_nine)
soybean_training_nine = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight)
soybean_training_ten = train_merge(soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)

def euclidean_distance(x, y, class_name, column_headers, regression=False):
  
  if regression == False:
    #this drops the class columns for the sample test dataset
    #following four lines should be commented out when the actual experiment
    for j in class_name:
      if y[j] == 1.0 or y[j] == 1:
        predict = j
      x = x.drop(j)
      y = y.drop(j)

  elif regression == True:
    predict = y[class_name]
    x = x.drop(class_name)
    y = y.drop(class_name)

  #x = x.drop("student-yes")
  #x = x.drop("student-no")
  #y = y.drop("student-yes")
  #y = y.drop("student-no")
  #print("x:", x)
  #print("y:", y)

  delta = 0
  #print("---------------------------------------------------------------------")

  
  for i in range(x.shape[0]):
    #month and day are 
    if column_headers[i] == "month":
      cyclical_distance_list = [(x.iloc[i] - y.iloc[i]), (12 - (x.iloc[i] - y.iloc[i]))]
      d = np.argmin(cyclical_distance_list)
      d = d**2
      d = math.sqrt(d)
      delta = d + delta
    elif column_headers[i] == "day":
      cyclical_distance_list = [(x.iloc[i] - y.iloc[i]), (7 - (x.iloc[i] - y.iloc[i]))]
      d = np.argmin(cyclical_distance_list)
      d = d**2
      d = math.sqrt(d)
      delta = d + delta
    else:
      d = x.iloc[i] - y.iloc[i]
      d = d**2
      d = math.sqrt(d)
      delta = d + delta
      
    #print(x.iloc[i], "-", y.iloc[i])
    #print(d)
  #print("sum:", delta)

  #print(delta, predict)
  #print("---------------------------------------------------------------------")
  return delta, predict

#euclidean_distance(x, y, class_name, column_headers, regression=False)
video_computer_headers = list(computer_dataset.columns.values)
video_euclidean_distance1 = euclidean_distance(computer_dataset.iloc[0], computer_dataset.iloc[20], computer_class, video_computer_headers, regression = True)
video_euclidean_distance2 = euclidean_distance(computer_dataset.iloc[0], computer_dataset.iloc[40], computer_class, video_computer_headers, regression = True)
video_euclidean_distance3 = euclidean_distance(computer_dataset.iloc[0], computer_dataset.iloc[60], computer_class, video_computer_headers, regression = True)
video_quary_list = [video_euclidean_distance1[0], video_euclidean_distance2[0], video_euclidean_distance3[0]]
print(video_quary_list)

"""
Gaussian Kernel (Radial Basis Function)
For regression, apply a Gaussian (radial basis function) kernel to make your prediction.
You will need to tune the bandwidth σ for the Gaussian kernel.

A radial basis function is a scalar function that depends on the distance to some point, called the center point, c. 
One popular radial basis function is the Gaussian kernel 
φ(x; c) = exp(-||x – c||2 / (2 σ2)), which uses the squared distance from a vector x to the center c to assign a weight.

The RBF kernel function for two points X₁ and X₂ computes the similarity or how close they are to each other.
when two points are more similar, the kernel value will be closer to 1. and when two points are more different, the kernel value will be closer to 0.
smaller sigma will consider that only the extremely closer points are similar. 
The size of sigma correspoinds to the width which the model will consider whether the two points are similar or not

find the centroids in the k-means
use the centroids as the reduced training dataset to perform KNN
calculate the distance between the centroids and the instances from the test dataset
store the distance between the the centroids and the instance in an array or list or so
weight the distance based how similar they are and sum up all weights*distances to get the predicted value

weight = exp(- (d(a, b)^2) / sigma^2)
w is the weight, d(a,b) is distance between a and b. 
σ is a parameter we set. The query point is the point we are trying to estimate, 
so we take the distance of one of the K-nearest points and give its weight

sigma values we use = [0.1, 1, 100]
the larger the sigma value, the more range of points are classified as similar points
the smaller the sigma value, the fewer range of points are classified as similar points
"""

def gaussian_kernel(query, sigma):
  weight_list = []
  for i in query:
    power = (-1 * (i)**2) / (sigma**2)
    w = math.exp(power)
    weight_list.append(w)

  return weight_list

video_gaussian_kernel = gaussian_kernel(video_quary_list, 1000)
print(video_gaussian_kernel)

"""
KNN
specify the hyperparameter "k" identified as the number of neighbors to Xi

KNN() identifies the class that the majority of the "k" neighbors of Xi belongs to

1. calculate the distance between Xi and Xj <- distance function
2. define "k" closest Xj's as the neighbors of Xi
3. Xi will belong to a class which the majority of its neighbors belong to
"""

def KNN(dftest, dftrain, class_name, k, regression=False, sigma=None):
  column_headers = list(dftrain.columns.values)
  matrix = []
  
  for i in range(dftest.shape[0]): #<- the first loop should go through the test set to get the distance between a point in the test set and every points in the training set
    predict = None
    actual = None
    delta_dict = {} #<- this dictionary stores the delta between two points and the class of y
    delta_list = [] #this stores the actual delta value
    predicted_class = [] #<- this list stores the class name of each neighbor
    
    if regression == False:
      for j in class_name:
        if dftest.iloc[i][j] == 1.0 or dftest.iloc[i][j] == 1: #<- this determines which class the instance from test set belongs to
          actual = j #<- assigns the class of the test set to the variable "actual"

      for m in range(len(dftrain)):
        delta, neighbor_class = euclidean_distance(dftest.iloc[i], dftrain.iloc[m], class_name, column_headers, regression=False) #<- pass the two pandas series. it returns the distance and the class of which the dftrain.loc[l] belongs to
        delta_list.append(delta)
        delta_dict[delta] = neighbor_class #<- adds the delta as key for the neighbor_class as value in the dictionary
        #neighbor_class in the regression is the numerical response value. i.e. delta_dict = {distance : the numerical value of the centroid}
      delta_list = sorted(delta_list) #<- sorting the delta_list (contains only the delta value) in ascending order
      
      for n in range(k): #<- this k is the hyperparameter "k" value
        STR = delta_dict[delta_list[n]]
        predicted_class.append(STR) #<- the class name of the nearst neighbors are stored in the list

      print(predicted_class)
      predict = max(set(predicted_class), key=predicted_class.count)
      result = [actual, predict] #<- result array contains the actual class of a certain instance from the test set and the predicted class
      matrix.append(result)

    elif regression == True: #<- diverges to the gaussian kernel function
      actual = dftest.iloc[i][class_name]
      for j in range(len(dftrain)): #<- this goes through the centroids
        delta, neighbor_class = euclidean_distance(dftest.iloc[i], dftrain.iloc[j], class_name, column_headers, regression=True)
        #calculate the distance between each centroid and instance from test set
        delta_list.append(delta) #<- delta_list stores the dist b/w centroids and the instance
        delta_dict[delta] = neighbor_class #<- delta_dict stores the dist b/w centroids and the instance as key and the value of centroid as value
      
      delta_list = sorted(delta_list)

      for m in range(k):
        STR = delta_dict[delta_list[m]]
        predicted_class.append(STR)

      print(predicted_class)
      weight_list = gaussian_kernel(predicted_class, sigma) #<- calls the gaussian_kernel function to get the weight_list
      predict = 0
      weight_sum = 0

      for l in range(len(weight_list)):
        predict = predict + weight_list[l] * dftrain.iloc[l][class_name]
        weight_sum = weight_sum + weight_list[l]
      
      predict = predict / weight_sum
      """
      we might see the preidcted value of nan here. 
      this is bevause that the data point and each centroid are too far away
      that the e is raised to the extremely large negative value,
      which will ultimately returns the weight of 0.
      Therefore, the predicted value is (W1 * number + W2 * number ... Wn * number) / (sum of weight),
      which is 0/0. This will not return error because the numerator is 0. However, it return nan.
      """
      result = [actual, predict]
      matrix.append(result)

  return matrix #<- matrix contains the actual and the predicted class of each instance

#KNN(dftest, dftrain, class_name, k, regression=False, sigma=None)
#[actual, predict]
print(computer_dataset.iloc[0:1])
computer_video_point_result = KNN(computer_dataset.iloc[0:1], computer_training_one, computer_class, 3, regression = True, sigma = 1000)
print(computer_video_point_result)

print(soybean_dataset.iloc[0:1])
soybean_video_point_result = KNN(soybean_dataset.iloc[0:1], soybean_training_one, soybean_class, 3, regression = False)
print(soybean_video_point_result)

"""
edited-KNN
this function will remove any noise data from the dataset
1. for each instance Xi in dftrain, it calculates the distance between Xi and Xj
2. get the class of Xj
3. put the delta(Xi, Xj) as a key and class of Xj as the value in the delta_dict
4. put the delta(Xi, Xj) in the delta_list
5. sort delta_list in ascending order
6. pick up the first K elements in the delta_list
7. put the class of the first K elements in the delta_list in predicted_class
8. get the most frequent element in predicted_class
9. if the class of Xi is different from the most frequet element in predicted_class, remove Xi from the dataset
10. if same, pass
11. compare the edited df and the original df
12. if the len(edited df) == len(original df), return the original df and finish the code
13. if not, assign edited df to original df and continue the loop
14. continue the process above until there is no change

epsilon is the largest distance two points can be apart only for the regression data set
"""
def edited_clustering_KNN(dftrain, class_name, k=None, epsilon=None):
  column_headers = list(dftrain.columns.values)
  dftrain_edited = dftrain #the dftrain with removed points are 
  dftrain_original = dftrain #update or return this df after the removal of points
  convergence = False

  while (convergence == False):
    try:
      for i in range(dftrain_edited.shape[0]): #<- the first loop should go through the test set to get the distance between a point in the test set and every points in the training set
        class_Xi = None
        class_Xj = None
        delta_dict = {} #<- this dictionary stores the delta between two points and the class of y
        predicted_class = [] #<- this list stores the class name of each neighbor
          
        if epsilon == None:
          for j in class_name:
            if dftrain_edited.loc[i][j] == 1.0 or dftrain_edited.loc[i][j] == 1: #<- this determines which class the instance from test set belongs to
              class_Xi = j #<- assigns the class of the test set to the variable "actual"

          for m in range(len(dftrain_edited)):
            if (i != m):
              delta, neighbor_class = euclidean_distance(dftrain_edited.loc[i], dftrain_edited.loc[m], class_name, column_headers, regression=False) #<- pass the two pandas series. it returns the distance and the class of which the dftrain.loc[l] belongs to
              delta_dict[delta] = neighbor_class #<- adds the delta as key for the neighbor_class as value in the dictionary
              #return entire dataset showing distance between point and dataspoint
            delta_list = sorted(delta_dict.keys()) #<- sorting the delta_list (contains only the delta value) in ascending order

          for n in range(k): #<- this k is the hyperparameter "k" value
            predicted_class.append(delta_dict[delta_list[n]]) #<- the class name of the nearst neighbors are stored in the list

          class_Xj = max(set(predicted_class), key=predicted_class.count)
            
          if (class_Xi != class_Xj):
            dftrain_original = dftrain_edited
            dftrain_edited = dftrain_edited.drop(dftrain_edited.index[i])
            dftrain_edited = dftrain_edited.reset_index(drop = True)
          if (len(dftrain_edited) == len(dftrain_original)):
            convergence = True

        elif epsilon != None:
          delta = 0
          class_Xi = dftrain_edited[class_name]
          for j in range(len(dftrain_edited)):
            if (i != j):
              delta, response = euclidean_distance(dftrain_edited.loc[i], dftrain_edited.loc[j], class_name, column_headers, regression=True)
              
            if delta >= epsilon: #<- if the distance between two points are greater than or equal to epsilon threshold, remove the point
              dftrain_edited = dftrain_edited.drop(dftrain_edited.index[i])
              dftrain_edited = dftrain_edited.reset_index(drop = True)
            
          if (len(dftrain_edited) == len(dftrain_original)):
            convergence = True
    except KeyError:
      break

  return dftrain_original #<- matrix contains the actual and the predicted class of each instance

print("Before------------------------------------")
print(soybean_training_one)
result_soybean_video_edited = edited_clustering_KNN(soybean_training_one, soybean_class, 9)
print("After-------------------------------------")
print(result_soybean_video_edited)

#This is the most recent one I've been working on, I'll clear up the other k-means once this one is completed

def k_means(data_set, k, class_name, counter_break,kernel):
  clusters = [] #<- this stores centroids and the points associated with the centroid
  column_headers = list(data_set.columns.values)
  counter = 0
  for i in range(k):
    centroid = data_set.sample() #<- this randomly assigns k numbers of mean points as initial points
    clusters.append({"cen":centroid,"points":data_set.iloc[0:0]}) #<- 

  p2c = {}#dictionary where for each index we store the corresponding centroid that it belongs to

  while(True):
    counter = counter + 1
    changed = False
    #Goes trhough the training set and assigns each point to the nearest centroid
    for i in range(data_set.shape[0]):
      if i == 0:
        pointseries = data_set.loc[i]
      else:
        pointseries = data_set.loc[i]
      
      minDist = float("inf")
      j_best = -1
      for j in range(k): #calculates the distance between the point and each centroid
        if i == 0:
          dist, dontusethisvariable = euclidean_distance(clusters[j]["cen"].squeeze(), pointseries, class_name, column_headers, regression=kernel)
          #print(dist)
          #print(clusters[j]["cen"])
          if j_best == -1 or dist < minDist:
            minDist = dist
            j_best = j
        if i != 0:
          if math.isnan(clusters[j]["cen"][column_headers[0]]) == True:
            pass
          else:
            dist, dontusethisvariable = euclidean_distance(clusters[j]["cen"].squeeze(), pointseries, class_name, column_headers, regression=kernel)
            #print(dist)
            #print(clusters[j]["cen"])
            if j_best == -1 or dist < minDist:
              minDist = dist
              j_best = j
              #print(j_best)

      if i in p2c: #check to see if the point is already in the p2c dictionary
        j_old = p2c[i]
        
      if i not in p2c or j_old != j_best: #updates the centroids cooresponding points to p2c, if the new calculated best centroid is different from the old clalculated best for each centroid then we update the value
        p2c[i] = j_best
        changed = True #since the centroids points changed, we set this to true so we go through another loop

    if changed == False:#breaks the statement if the code has not changed
      break
    if counter > counter_break:
      #print("broke by counter")
      break
    
    print(p2c)
    for m in range(k): #recalculates the centroid means
      cluster_points = data_set.iloc[0:0]
      for n in range(data_set.shape[0]):
        if p2c[n] == m:
          if n == 0:
            cluster_points = pd.merge(cluster_points, data_set.loc[:0], how = "outer")
          else:
            cluster_points = pd.merge(cluster_points, data_set.iloc[(n-1):n], how = "outer")
      clusters[m]["cen"] = cluster_points.mean()


  
  k_means_centroids = data_set.loc[0:k-1]
  k_means_centroids = k_means_centroids.astype(float)

  for q in range(k): # updates the centroid and turns into a dataframe so we can iterate the centroids
    for r in range(data_set.shape[1]):
      value = clusters[q]["cen"].at[column_headers[r]]
      k_means_centroids.at[q, column_headers[r]] = value


  for s in range(k): #if a centroid has value nan, then that means that this point is not a centroid, therefore it gets removed from the centroid dataframe
    if math.isnan(k_means_centroids.at[s, column_headers[0]]) == True:
      k_means_centroids = k_means_centroids.drop(s)
  #dftest, dftrain, class_name, k, regression=False, sigma=None
  if kernel == False: #if categorical, updates the centroid class so that it is equal to its nearest neighbor
    class_result = KNN(k_means_centroids, data_set, class_name, 1, regression = False, sigma = None)
    for i in range(k_means_centroids.shape[0]):
      for j in range(k_means_centroids.shape[1]):
        if column_headers[j] in class_name:
          if column_headers[j] == class_result[i][1]:
            k_means_centroids.iloc[i][column_headers[j]] = 1
          else:
            k_means_centroids.iloc[i][column_headers[j]] = 0
  
  return k_means_centroids
# Need to do, First need is a better comparison method from old_clusters to new_clusters
# Second need is to deal with the NaN issue (probably arises when points is zero), or when classes gets messed up(doesn't like decimals)
# Third need is to return cluster centroids as a dataframe to be able to run KNN over
#k_means_centroid_df = k_means(soybean_training_one, 15, soybean_class, 15, kernel = False)
#k_means_centroid_df
#print(k_mean_centroid_df)

#(data_set, k, class_name, counter_break,kernel):

video_k_means_assignment = k_means(soybean_training_one, 5, soybean_class, kernel = False, counter_break = 5)
print(video_k_means_assignment)

computer_training_one = train_merge(computer_ten, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_two = train_merge(computer_ten, computer_one, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_three = train_merge(computer_ten, computer_one, computer_two, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_four = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_five = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_six = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_seven, computer_eight, computer_nine)
computer_training_seven = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_eight, computer_nine)
computer_training_eight = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_nine)
computer_training_nine = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight)
computer_training_ten = train_merge(computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)

soybean_training_one = train_merge(soybean_ten, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_two = train_merge(soybean_ten, soybean_one, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_three = train_merge(soybean_ten, soybean_one, soybean_two, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_four = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_five = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_six = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_seven, soybean_eight, soybean_nine)
soybean_training_seven = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_eight, soybean_nine)
soybean_training_eight = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_nine)
soybean_training_nine = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight)
soybean_training_ten = train_merge(soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)

###FOR THE VIDEO

k_list = [3]
sigma_list = [10000]
count_break_list = [3]
e = 10
computer_training_one = train_merge(computer_ten, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_two = train_merge(computer_ten, computer_one, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_three = train_merge(computer_ten, computer_one, computer_two, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_four = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_five = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_six = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_seven, computer_eight, computer_nine)
computer_training_seven = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_eight, computer_nine)
computer_training_eight = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_nine)
computer_training_nine = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight)
computer_training_ten = train_merge(computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
 
 
for s in sigma_list:
  for c in count_break_list:
    print("----------------------------------------------------------------------------------------------------------------------------")
    for k in k_list:
      print("this is the KNN result for sigma =", s, ",and for epsilon =", e)
      print("this is the -KNN result for k =", k)
      #print("abalone first training start")
      result_computer_one = KNN(computer_one, computer_training_one, computer_class, k, regression=True, sigma=s)
      #print("abalone second training start")
      result_computer_two = KNN(computer_two, computer_training_two, computer_class, k, regression=True, sigma=s)
      #print("abalone third training start")
      result_computer_three = KNN(computer_three, computer_training_three, computer_class, k, regression=True, sigma=s)
      #print("abalone fourth training start")
      result_computer_four = KNN(computer_four, computer_training_four, computer_class, k, regression=True, sigma=s)
      #print("abalone fifth training start")
      result_computer_five = KNN(computer_five, computer_training_five, computer_class, k, regression=True, sigma=s)
      #print("abalone sixth training start")
      result_computer_six = KNN(computer_six, computer_training_six, computer_class, k, regression=True, sigma=s)
      #print("abalone seventh training start")
      result_computer_seven = KNN(computer_seven, computer_training_seven, computer_class, k, regression=True, sigma=s)
      #print("abalone eighth training start")
      result_computer_eight = KNN(computer_eight, computer_training_eight, computer_class, k, regression=True, sigma=s)
      #print("abalone ninth training start")
      result_computer_nine = KNN(computer_nine, computer_training_nine, computer_class, k, regression=True, sigma=s)
      #print("abalone tenth training start")
      result_computer_ten = KNN(computer_ten, computer_training_ten, computer_class, k, regression=True, sigma=s)
 
      print(result_computer_one)
      print(result_computer_two)
      print(result_computer_three)
      print(result_computer_four)
      print(result_computer_five)
      print(result_computer_six)
      print(result_computer_seven)
      print(result_computer_eight)
      print(result_computer_nine)
      print(result_computer_ten)
   
      print(Mean_Squared_Error(result_computer_one))
      print(Mean_Squared_Error(result_computer_two))
      print(Mean_Squared_Error(result_computer_three))
      print(Mean_Squared_Error(result_computer_four))
      print(Mean_Squared_Error(result_computer_five))
      print(Mean_Squared_Error(result_computer_six))
      print(Mean_Squared_Error(result_computer_seven))
      print(Mean_Squared_Error(result_computer_eight))
      print(Mean_Squared_Error(result_computer_nine))
      print(Mean_Squared_Error(result_computer_ten))

      print("----------------------------------------------------------------------------------------------------------------------------")
      print("this is the edited-KNN result for sigma =", s, ",and for epsilon =", e)
      print("this is the edited-KNN result for k =", k)
      #print("abalone first training start")
      result_computer_one = KNN(computer_one, edited_clustering_KNN(computer_training_one, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
      #print("abalone second training start")
      result_computer_two = KNN(computer_two, edited_clustering_KNN(computer_training_two, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
      #print("abalone third training start")
      result_computer_three = KNN(computer_three, edited_clustering_KNN(computer_training_three, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
      #print("abalone fourth training start")
      result_computer_four = KNN(computer_four, edited_clustering_KNN(computer_training_four, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
      #print("abalone fifth training start")
      result_computer_five = KNN(computer_five, edited_clustering_KNN(computer_training_five, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
      #print("abalone sixth training start")
      result_computer_six = KNN(computer_six, edited_clustering_KNN(computer_training_six, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
      #print("abalone seventh training start")
      result_computer_seven = KNN(computer_seven, edited_clustering_KNN(computer_training_seven, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
      #print("abalone eighth training start")
      result_computer_eight = KNN(computer_eight, edited_clustering_KNN(computer_training_eight, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
      #print("abalone ninth training start")
      result_computer_nine = KNN(computer_nine, edited_clustering_KNN(computer_training_nine, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
      #print("abalone tenth training start")
      result_computer_ten = KNN(computer_ten, edited_clustering_KNN(computer_training_ten, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
 
      print(result_computer_one)
      print(result_computer_two)
      print(result_computer_three)
      print(result_computer_four)
      print(result_computer_five)
      print(result_computer_six)
      print(result_computer_seven)
      print(result_computer_eight)
      print(result_computer_nine)
      print(result_computer_ten)
   
      print(Mean_Squared_Error(result_computer_one))
      print(Mean_Squared_Error(result_computer_two))
      print(Mean_Squared_Error(result_computer_three))
      print(Mean_Squared_Error(result_computer_four))
      print(Mean_Squared_Error(result_computer_five))
      print(Mean_Squared_Error(result_computer_six))
      print(Mean_Squared_Error(result_computer_seven))
      print(Mean_Squared_Error(result_computer_eight))
      print(Mean_Squared_Error(result_computer_nine))
      print(Mean_Squared_Error(result_computer_ten))
      print("----------------------------------------------------------------------------------------------------------------------------")
      #soybean_training_two, 15, soybean_class, kernel = False (dftest, dftrain, class_name, k, regression=False, sigma=None)
      print("this is the Kmeans result for s =", s, " and counter break = ", c )
      print("computer first training start")
      result_computer_one = KNN(computer_one, k_means(computer_training_one, k, computer_class, c, kernel = True), computer_class, 1, regression=True, sigma=s)
      print("computer second training start")
      result_computer_two = KNN(computer_two, k_means(computer_training_two, k, computer_class, c, kernel = True), computer_class, 1, regression=True, sigma=s)
      print("computer third training start")
      result_computer_three = KNN(computer_three, k_means(computer_training_three, k, computer_class,  c, kernel = True), computer_class, 1, regression=True, sigma=s)
      print("computer fourth training start")
      result_computer_four = KNN(computer_four, k_means(computer_training_four, k, computer_class,  c, kernel = True), computer_class, 1, regression=True, sigma=s)
      print("computer fifth training start")
      result_computer_five = KNN(computer_five, k_means(computer_training_five, k, computer_class,  c, kernel = True),computer_class, 1, regression=True, sigma=s)
      print("computer sixth computer_HP_set start")
      result_computer_six = KNN(computer_six, k_means(computer_training_six, k, computer_class,  c, kernel = True),computer_class, 1, regression=True, sigma=s)
      print("computer seventh training start")
      result_computer_seven = KNN(computer_seven, k_means(computer_training_seven, k, computer_class,  c, kernel = True),computer_class, 1, regression=True, sigma=s)
      print("computer eighth training start")
      result_computer_eight = KNN(computer_eight, k_means(computer_training_eight, k, computer_class,  c, kernel = True),computer_class, 1, regression=True, sigma=s)
      print("computer ninth training start")
      result_computer_nine = KNN(computer_nine, k_means(computer_training_nine, k, computer_class,  c, kernel = True),computer_class, 1, regression=True, sigma=s)
      print("computer tenth training start")
      result_computer_ten = KNN(computer_ten, k_means(computer_training_ten, k, computer_class,  c, kernel = True),computer_class, 1, regression=True, sigma=s)
      
      print(result_computer_one)
      print(result_computer_two)
      print(result_computer_three)
      print(result_computer_four)
      print(result_computer_five)
      print(result_computer_six)
      print(result_computer_seven)
      print(result_computer_eight)
      print(result_computer_nine)
      print(result_computer_ten)
 
      print(Mean_Squared_Error(result_computer_one))
      print(Mean_Squared_Error(result_computer_two))
      print(Mean_Squared_Error(result_computer_three))
      print(Mean_Squared_Error(result_computer_four))
      print(Mean_Squared_Error(result_computer_five))
      print(Mean_Squared_Error(result_computer_six))
      print(Mean_Squared_Error(result_computer_seven))
      print(Mean_Squared_Error(result_computer_eight))
      print(Mean_Squared_Error(result_computer_nine))
      print(Mean_Squared_Error(result_computer_ten))

k_list = [3]
sigma_list = [10000]
count_break_list = [3]
e = 10
soybean_training_one = train_merge(soybean_ten, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_two = train_merge(soybean_ten, soybean_one, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_three = train_merge(soybean_ten, soybean_one, soybean_two, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_four = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_five = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_six = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_seven, soybean_eight, soybean_nine)
soybean_training_seven = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_eight, soybean_nine)
soybean_training_eight = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_nine)
soybean_training_nine = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight)
soybean_training_ten = train_merge(soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
 
 
for s in sigma_list:
  for c in count_break_list:
    print("----------------------------------------------------------------------------------------------------------------------------")
    for k in k_list:
      print("this is the KNN result for sigma =", s, ",and for epsilon =", e)
      print("this is the -KNN result for k =", k)
      #print("abalone first training start")
      result_soybean_one = KNN(soybean_one, soybean_training_one, soybean_class, k, regression=False, sigma=None)
      #print("abalone second training start")
      result_soybean_two = KNN(soybean_two, soybean_training_two, soybean_class, k, regression=False, sigma=None)
      #print("abalone third training start")
      result_soybean_three = KNN(soybean_three, soybean_training_three, soybean_class, k, regression=False, sigma=None)
      #print("abalone fourth training start")
      result_soybean_four = KNN(soybean_four, soybean_training_four, soybean_class, k, regression=False, sigma=None)
      #print("abalone fifth training start")
      result_soybean_five = KNN(soybean_five, soybean_training_five, soybean_class, k, regression=False, sigma=None)
      #print("abalone sixth training start")
      result_soybean_six = KNN(soybean_six, soybean_training_six, soybean_class, k, regression=False, sigma=None)
      #print("abalone seventh training start")
      result_soybean_seven = KNN(soybean_seven, soybean_training_seven, soybean_class, k, regression=False, sigma=None)
      #print("abalone eighth training start")
      result_soybean_eight = KNN(soybean_eight, soybean_training_eight, soybean_class, k, regression=False, sigma=None)
      #print("abalone ninth training start")
      result_soybean_nine = KNN(soybean_nine, soybean_training_nine, soybean_class, k, regression=False, sigma=None)
      #print("abalone tenth training start")
      result_soybean_ten = KNN(soybean_ten, soybean_training_ten, soybean_class, k, regression=False, sigma=None)
 
      print(result_soybean_one)
      print(result_soybean_two)
      print(result_soybean_three)
      print(result_soybean_four)
      print(result_soybean_five)
      print(result_soybean_six)
      print(result_soybean_seven)
      print(result_soybean_eight)
      print(result_soybean_nine)
      print(result_soybean_ten)
   
      print(zero_loss_func(result_soybean_one))
      print(zero_loss_func(result_soybean_two))
      print(zero_loss_func(result_soybean_three))
      print(zero_loss_func(result_soybean_four))
      print(zero_loss_func(result_soybean_five))
      print(zero_loss_func(result_soybean_six))
      print(zero_loss_func(result_soybean_seven))
      print(zero_loss_func(result_soybean_eight))
      print(zero_loss_func(result_soybean_nine))
      print(zero_loss_func(result_soybean_ten))
      print("----------------------------------------------------------------------------------------------------------------------------")
      print("this is the edited-KNN result for sigma =", s, ",and for epsilon =", e)
      print("this is the edited-KNN result for k =", k)
      #print("abalone first training start")
      result_soybean_one = KNN(soybean_one, edited_clustering_KNN(soybean_training_one, soybean_class, k), soybean_class, k, regression=False, sigma=None)
      #print("abalone second training start")
      result_soybean_two = KNN(soybean_two, edited_clustering_KNN(soybean_training_two, soybean_class, k), soybean_class, k, regression=False, sigma=s)
      #print("abalone third training start")
      result_soybean_three = KNN(soybean_three, edited_clustering_KNN(soybean_training_three, soybean_class, k), soybean_class, k, regression=False, sigma=None)
      #print("abalone fourth training start")
      result_soybean_four = KNN(soybean_four, edited_clustering_KNN(soybean_training_four, soybean_class, k), soybean_class, k, regression=False, sigma=None)
      #print("abalone fifth training start")
      result_soybean_five = KNN(soybean_five, edited_clustering_KNN(soybean_training_five, soybean_class, k), soybean_class, k, regression=False, sigma=None)
      #print("abalone sixth training start")
      result_soybean_six = KNN(soybean_six, edited_clustering_KNN(soybean_training_six, soybean_class, k), soybean_class, k, regression=False, sigma=None)
      #print("abalone seventh training start")
      result_soybean_seven = KNN(soybean_seven, edited_clustering_KNN(soybean_training_seven, soybean_class, k), soybean_class, k, regression=False, sigma=None)
      #print("abalone eighth training start")
      result_soybean_eight = KNN(soybean_eight, edited_clustering_KNN(soybean_training_eight, soybean_class, k), soybean_class, k, regression=False, sigma=None)
      #print("abalone ninth training start")
      result_soybean_nine = KNN(soybean_nine, edited_clustering_KNN(soybean_training_nine, soybean_class, k), soybean_class, k, regression=False, sigma=None)
      #print("abalone tenth training start")
      result_soybean_ten = KNN(soybean_ten, edited_clustering_KNN(soybean_training_ten, soybean_class, k), soybean_class, k, regression=False, sigma=None)
 
      print(result_soybean_one)
      print(result_soybean_two)
      print(result_soybean_three)
      print(result_soybean_four)
      print(result_soybean_five)
      print(result_soybean_six)
      print(result_soybean_seven)
      print(result_soybean_eight)
      print(result_soybean_nine)
      print(result_soybean_ten)
      print(zero_loss_func(result_soybean_one))
      print(zero_loss_func(result_soybean_two))
      print(zero_loss_func(result_soybean_three))
      print(zero_loss_func(result_soybean_four))
      print(zero_loss_func(result_soybean_five))
      print(zero_loss_func(result_soybean_six))
      print(zero_loss_func(result_soybean_seven))
      print(zero_loss_func(result_soybean_eight))
      print(zero_loss_func(result_soybean_nine))
      print(zero_loss_func(result_soybean_ten))      
      print("----------------------------------------------------------------------------------------------------------------------------")
      #soybean_training_two, 15, soybean_class, kernel = False (dftest, dftrain, class_name, k, regression=False, sigma=None)
      print("this is the Kmeans result for s =", s, " and counter break = ", c )
      print("soybean first training start")
      result_soybean_one = KNN(soybean_one, k_means(soybean_training_one, k, soybean_class, c, kernel = False), soybean_class, 1, regression=False, sigma=None)
      print("soybean second training start")
      result_soybean_two = KNN(soybean_two, k_means(soybean_training_two, k, soybean_class, c, kernel = False), soybean_class, 1, regression=False, sigma=None)
      print("soybean third training start")
      result_soybean_three = KNN(soybean_three, k_means(soybean_training_three, k, soybean_class,  c, kernel = False), soybean_class, 1, regression=False, sigma=None)
      print("soybean fourth training start")
      result_soybean_four = KNN(soybean_four, k_means(soybean_training_four, k, soybean_class,  c, kernel = False), soybean_class, 1, regression=False, sigma=None)
      print("soybean fifth training start")
      result_soybean_five = KNN(soybean_five, k_means(soybean_training_five, k, soybean_class,  c, kernel = False), soybean_class, 1, regression=False, sigma=None)
      print("soybean sixth soybean_HP_set start")
      result_soybean_six = KNN(soybean_six, k_means(soybean_training_six, k, soybean_class,  c, kernel = False), soybean_class, 1, regression=False, sigma=None)
      print("soybean seventh training start")
      result_soybean_seven = KNN(soybean_seven, k_means(soybean_training_seven, k, soybean_class,  c, kernel = False), soybean_class, 1, regression=False, sigma=None)
      print("soybean eighth training start")
      result_soybean_eight = KNN(soybean_eight, k_means(soybean_training_eight, k, soybean_class,  c, kernel = False), soybean_class, 1, regression=False, sigma=None)
      print("soybean ninth training start")
      result_soybean_nine = KNN(soybean_nine, k_means(soybean_training_nine, k, soybean_class,  c, kernel = False), soybean_class, 1, regression=False, sigma=None)
      print("soybean tenth training start")
      result_soybean_ten = KNN(soybean_ten, k_means(soybean_training_ten, k, soybean_class,  c, kernel = False), soybean_class, 1, regression=False, sigma=None)
      
      print(result_soybean_one)
      print(result_soybean_two)
      print(result_soybean_three)
      print(result_soybean_four)
      print(result_soybean_five)
      print(result_soybean_six)
      print(result_soybean_seven)
      print(result_soybean_eight)
      print(result_soybean_nine)
      print(result_soybean_ten)

      print(zero_loss_func(result_soybean_one))
      print(zero_loss_func(result_soybean_two))
      print(zero_loss_func(result_soybean_three))
      print(zero_loss_func(result_soybean_four))
      print(zero_loss_func(result_soybean_five))
      print(zero_loss_func(result_soybean_six))
      print(zero_loss_func(result_soybean_seven))
      print(zero_loss_func(result_soybean_eight))
      print(zero_loss_func(result_soybean_nine))
      print(zero_loss_func(result_soybean_ten))

k_list = [2,3,5,11,15]
sigma_list = [100]
count_break_list = [5]
forest_training_one = train_merge(forest_ten, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_two = train_merge(forest_ten, forest_one, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_three = train_merge(forest_ten, forest_one, forest_two, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_four = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_five = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_six = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_five, forest_seven, forest_eight, forest_nine)
forest_training_seven = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_eight, forest_nine)
forest_training_eight = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_nine)
forest_training_nine = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight)
forest_training_ten = train_merge(forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
 

for s in sigma_list:
  for c in count_break_list:
    print("----------------------------------------------------------------------------------------------------------------------------")
    print("this is the Kmeans result for s =", s, " and counter break = ", c )
    for k in k_list:
      print("----------------------------------------------------------------------------------------------------------------------------")
      #soybean_training_two, 15, soybean_class, kernel = False (dftest, dftrain, class_name, k, regression=False, sigma=None)
      print("this is the KNN result for k =", k)
      print("forest first training start")
      result_forest_one = KNN(forest_HP_set, k_means(forest_training_one, k, forest_class, c, kernel = True), forest_class, 1, regression=True, sigma=s)
      print("forest second training start")
      result_forest_two = KNN(forest_HP_set, k_means(forest_training_two, k, forest_class, c, kernel = True), forest_class, 1, regression=True, sigma=s)
      print("forest third training start")
      result_forest_three = KNN(forest_HP_set, k_means(forest_training_three, k, forest_class,  c, kernel = True), forest_class, 1, regression=True, sigma=s)
      print("forest fourth training start")
      result_forest_four = KNN(forest_HP_set, k_means(forest_training_four, k, forest_class,  c, kernel = True), forest_class, 1, regression=True, sigma=s)
      print("forest fifth training start")
      result_forest_five = KNN(forest_HP_set, k_means(forest_training_five, k, forest_class,  c, kernel = True),forest_class, 1, regression=True, sigma=s)
      print("forest sixth forest_HP_set start")
      result_forest_six = KNN(forest_HP_set, k_means(forest_training_six, k, forest_class,  c, kernel = True),forest_class, 1, regression=True, sigma=s)
      print("forest seventh training start")
      result_forest_seven = KNN(forest_HP_set, k_means(forest_training_seven, k, forest_class,  c, kernel = True),forest_class, 1, regression=True, sigma=s)
      print("forest eighth training start")
      result_forest_eight = KNN(forest_HP_set, k_means(forest_training_eight, k, forest_class,  c, kernel = True),forest_class, 1, regression=True, sigma=s)
      print("forest ninth training start")
      result_forest_nine = KNN(forest_HP_set, k_means(forest_training_nine, k, forest_class,  c, kernel = True),forest_class, 1, regression=True, sigma=s)
      print("forest tenth training start")
      result_forest_ten = KNN(forest_HP_set, k_means(forest_training_ten, k, forest_class,  c, kernel = True),forest_class, 1, regression=True, sigma=s)
      
      print(result_forest_one)
      print(result_forest_two)
      print(result_forest_three)
      print(result_forest_four)
      print(result_forest_five)
      print(result_forest_six)
      print(result_forest_seven)
      print(result_forest_eight)
      print(result_forest_nine)
      print(result_forest_ten)

      print(Mean_Squared_Error(result_forest_one))
      print(Mean_Squared_Error(result_forest_two))
      print(Mean_Squared_Error(result_forest_three))
      print(Mean_Squared_Error(result_forest_four))
      print(Mean_Squared_Error(result_forest_five))
      print(Mean_Squared_Error(result_forest_six))
      print(Mean_Squared_Error(result_forest_seven))
      print(Mean_Squared_Error(result_forest_eight))
      print(Mean_Squared_Error(result_forest_nine))
      print(Mean_Squared_Error(result_forest_ten))

k_list = [15]
sigma_list = [10000]
count_break_list = [3]
e = 10
computer_training_one = train_merge(computer_ten, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_two = train_merge(computer_ten, computer_one, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_three = train_merge(computer_ten, computer_one, computer_two, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_four = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_five = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_six = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_seven, computer_eight, computer_nine)
computer_training_seven = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_eight, computer_nine)
computer_training_eight = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_nine)
computer_training_nine = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight)
computer_training_ten = train_merge(computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
 
 
for s in sigma_list:
  for c in count_break_list:
    print("----------------------------------------------------------------------------------------------------------------------------")
    print("this is the Kmeans result for s =", s, " and counter break = ", c )
    for k in k_list:
      print("----------------------------------------------------------------------------------------------------------------------------")
      #soybean_training_two, 15, soybean_class, kernel = False (dftest, dftrain, class_name, k, regression=False, sigma=None)
      print("this is the KNN result for k =", k)
      print("computer first training start")
      result_computer_one = KNN(computer_one, k_means(computer_training_one, k, computer_class, c, kernel = True), computer_class, 1, regression=True, sigma=s)
      print("computer second training start")
      result_computer_two = KNN(computer_two, k_means(computer_training_two, k, computer_class, c, kernel = True), computer_class, 1, regression=True, sigma=s)
      print("computer third training start")
      result_computer_three = KNN(computer_three, k_means(computer_training_three, k, computer_class,  c, kernel = True), computer_class, 1, regression=True, sigma=s)
      print("computer fourth training start")
      result_computer_four = KNN(computer_four, k_means(computer_training_four, k, computer_class,  c, kernel = True), computer_class, 1, regression=True, sigma=s)
      print("computer fifth training start")
      result_computer_five = KNN(computer_five, k_means(computer_training_five, k, computer_class,  c, kernel = True),computer_class, 1, regression=True, sigma=s)
      print("computer sixth computer_HP_set start")
      result_computer_six = KNN(computer_six, k_means(computer_training_six, k, computer_class,  c, kernel = True),computer_class, 1, regression=True, sigma=s)
      print("computer seventh training start")
      result_computer_seven = KNN(computer_seven, k_means(computer_training_seven, k, computer_class,  c, kernel = True),computer_class, 1, regression=True, sigma=s)
      print("computer eighth training start")
      result_computer_eight = KNN(computer_eight, k_means(computer_training_eight, k, computer_class,  c, kernel = True),computer_class, 1, regression=True, sigma=s)
      print("computer ninth training start")
      result_computer_nine = KNN(computer_nine, k_means(computer_training_nine, k, computer_class,  c, kernel = True),computer_class, 1, regression=True, sigma=s)
      print("computer tenth training start")
      result_computer_ten = KNN(computer_ten, k_means(computer_training_ten, k, computer_class,  c, kernel = True),computer_class, 1, regression=True, sigma=s)
      
      print(result_computer_one)
      print(result_computer_two)
      print(result_computer_three)
      print(result_computer_four)
      print(result_computer_five)
      print(result_computer_six)
      print(result_computer_seven)
      print(result_computer_eight)
      print(result_computer_nine)
      print(result_computer_ten)
 
      print(Mean_Squared_Error(result_computer_one))
      print(Mean_Squared_Error(result_computer_two))
      print(Mean_Squared_Error(result_computer_three))
      print(Mean_Squared_Error(result_computer_four))
      print(Mean_Squared_Error(result_computer_five))
      print(Mean_Squared_Error(result_computer_six))
      print(Mean_Squared_Error(result_computer_seven))
      print(Mean_Squared_Error(result_computer_eight))
      print(Mean_Squared_Error(result_computer_nine))
      print(Mean_Squared_Error(result_computer_ten))

k_list = [3]
break_list = [3]
glass_training_one = train_merge(glass_ten, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_two = train_merge(glass_ten, glass_one, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_three = train_merge(glass_ten, glass_one, glass_two, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_four = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_five = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_six = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_seven, glass_eight, glass_nine)
glass_training_seven = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_eight, glass_nine)
glass_training_eight = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_nine)
glass_training_nine = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight)
glass_training_ten = train_merge(glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
#k_means(data_set, k, class_name, kernel):
for k in k_list:
  for c in break_list:
 
    #edited_clustering_KNN(dftrain, class_name, k=None, epsilon=None):
    print("----------------------------------------------------------------------------------------------------------------------------")
    print("this is the Kmeans result for k =", k)
    print("glass first training start")
    result_glass_one = KNN(glass_one, edited_clustering_KNN(glass_training_one, glass_class, k, epsilon = None),  glass_class, k, regression=False, sigma=None)
    print("glass second training start")
    result_glass_two = KNN(glass_two, edited_clustering_KNN(glass_training_two, glass_class, k, epsilon = None), glass_class, k, regression=False, sigma=None)
    print("glass third training start")
    result_glass_three = KNN(glass_three, edited_clustering_KNN(glass_training_three, glass_class, k, epsilon = None), glass_class, k, regression=False, sigma=None)
    print("glass fourth training start")
    result_glass_four = KNN(glass_four, edited_clustering_KNN(glass_training_four, glass_class, k, epsilon = None), glass_class, k, regression=False, sigma=None)
    print("glass fifth training start")
    result_glass_five = KNN(glass_five, edited_clustering_KNN(glass_training_five, glass_class, k, epsilon = None), glass_class, k, regression=False, sigma=None)
    print("glass sixth training start")
    result_glass_six = KNN(glass_six, edited_clustering_KNN(glass_training_six, glass_class, k, epsilon = None), glass_class, k, regression=False, sigma=None)
    print("glass seventh training start")
    result_glass_seven = KNN(glass_seven, edited_clustering_KNN(glass_training_seven, glass_class, k, epsilon = None), glass_class, k, regression=False, sigma=None)
    print("glass eighth training start")
    result_glass_eight = KNN(glass_eight, edited_clustering_KNN(glass_training_eight, glass_class, k, epsilon = None), glass_class, k, regression=False, sigma=None)
    print("glass ninth training start")
    result_glass_nine = KNN(glass_nine, edited_clustering_KNN(glass_training_nine, glass_class, k, epsilon = None), glass_class, k, regression=False, sigma=None)
    print("glass tenth training start")
    result_glass_ten = KNN(glass_ten, edited_clustering_KNN(glass_training_ten, glass_class, k, epsilon = None), glass_class, k, regression=False, sigma=None)
    print(result_glass_one)
    print(result_glass_two)
    print(result_glass_three)
    print(result_glass_four)
    print(result_glass_five)
    print(result_glass_six)
    print(result_glass_seven)
    print(result_glass_eight)
    print(result_glass_nine)
    print(result_glass_ten)
  
    print(zero_loss_func(result_glass_one))
    print(zero_loss_func(result_glass_two))
    print(zero_loss_func(result_glass_three))
    print(zero_loss_func(result_glass_four))
    print(zero_loss_func(result_glass_five))
    print(zero_loss_func(result_glass_six))
    print(zero_loss_func(result_glass_seven))
    print(zero_loss_func(result_glass_eight))
    print(zero_loss_func(result_glass_nine))
    print(zero_loss_func(result_glass_ten))

k_list = [1]
break_list = [3]
glass_training_one = train_merge(glass_ten, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_two = train_merge(glass_ten, glass_one, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_three = train_merge(glass_ten, glass_one, glass_two, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_four = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_five = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_six = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_seven, glass_eight, glass_nine)
glass_training_seven = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_eight, glass_nine)
glass_training_eight = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_nine)
glass_training_nine = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight)
glass_training_ten = train_merge(glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
#k_means(data_set, k, class_name, kernel):
for k in k_list:
  for c in break_list:
 
    #dftest, dftrain, class_name, k, regression=False, sigma=None
    print("----------------------------------------------------------------------------------------------------------------------------")
    print("this is the Kmeans result for k =", k)
    print("glass first training start")
    result_glass_one = KNN(glass_one, glass_training_one, glass_class, k, regression=False, sigma=None)
    print("glass second training start")
    result_glass_two = KNN(glass_two, glass_training_two, glass_class, k, regression=False, sigma=None)
    print("glass third training start")
    result_glass_three = KNN(glass_three, glass_training_three, glass_class, k, regression=False, sigma=None)
    print("glass fourth training start")
    result_glass_four = KNN(glass_four, glass_training_four, glass_class, k, regression=False, sigma=None)
    print("glass fifth training start")
    result_glass_five = KNN(glass_five, glass_training_five, glass_class, k, regression=False, sigma=None)
    print("glass sixth training start")
    result_glass_six = KNN(glass_six, glass_training_six, glass_class, k, regression=False, sigma=None)
    print("glass seventh training start")
    result_glass_seven = KNN(glass_seven, glass_training_seven, glass_class, k, regression=False, sigma=None)
    print("glass eighth training start")
    result_glass_eight = KNN(glass_eight, glass_training_eight, glass_class, k, regression=False, sigma=None)
    print("glass ninth training start")
    result_glass_nine = KNN(glass_nine, glass_training_nine, glass_class, k, regression=False, sigma=None)
    print("glass tenth training start")
    result_glass_ten = KNN(glass_ten, glass_training_ten, glass_class, k, regression=False, sigma=None)
    print(result_glass_one)
    print(result_glass_two)
    print(result_glass_three)
    print(result_glass_four)
    print(result_glass_five)
    print(result_glass_six)
    print(result_glass_seven)
    print(result_glass_eight)
    print(result_glass_nine)
    print(result_glass_ten)
  
    print(zero_loss_func(result_glass_one))
    print(zero_loss_func(result_glass_two))
    print(zero_loss_func(result_glass_three))
    print(zero_loss_func(result_glass_four))
    print(zero_loss_func(result_glass_five))
    print(zero_loss_func(result_glass_six))
    print(zero_loss_func(result_glass_seven))
    print(zero_loss_func(result_glass_eight))
    print(zero_loss_func(result_glass_nine))
    print(zero_loss_func(result_glass_ten))

centroid_df = k_means(soybean_training_one, 3, soybean_class, kernel = False)
print(centroid_df)
result_soybean_one = KNN(soybean_HP_set, centroid_df, soybean_class, 1, regression=False, sigma=None)

k_list = [3, 5, 7, 11, 15]
#data_set, k, class_name, kernel
breast_training_one = train_merge(breast_ten, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine)
breast_training_two = train_merge(breast_ten, breast_one, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine)
breast_training_three = train_merge(breast_ten, breast_one, breast_two, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine)
breast_training_four = train_merge(breast_ten, breast_one, breast_two, breast_three, breast_five, breast_six, breast_seven, breast_eight, breast_nine)
breast_training_five = train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_six, breast_seven, breast_eight, breast_nine)
breast_training_six = train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_five, breast_seven, breast_eight, breast_nine)
breast_training_seven = train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_eight, breast_nine)
breast_training_eight = train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_nine)
breast_training_nine = train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight)
breast_training_ten = train_merge(breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine)

for k in k_list:
  print("----------------------------------------------------------------------------------------------------------------------------")
  print("this is the edited KNN result for k =", k)
  print("breast first training start")
  result_breast_one = KNN(breast_HP_set, k_means(breast_training_one, breast_class, k, kernel = False), breast_class, 1, regression = False, sigma = None)
  print("breast second training start")
  result_breast_two = KNN(breast_HP_set, k_means(breast_training_two, breast_class, k, kernel = False), breast_class, 1, regression = False, sigma = None)
  print("breast third training start")
  result_breast_three = KNN(breast_HP_set, k_means(breast_training_three, breast_class, k, kernel = False), breast_class, 1, regression = False, sigma = None)
  print("breast fourth training start")
  result_breast_four = KNN(breast_HP_set, k_means(breast_training_four, breast_class, k, kernel = False), breast_class, 1, regression = False, sigma = None)
  print("breast fifth training start")
  result_breast_five = KNN(breast_HP_set, k_means(breast_training_five, breast_class, k, kernel = False), breast_class, 1, regression = False, sigma = None)
  print("breast sixth training start")
  result_breast_six = KNN(breast_HP_set, k_means(breast_training_six, breast_class, k, kernel = False), breast_class, 1, regression = False, sigma = None)
  print("breast seventh training start")
  result_breast_seven = KNN(breast_HP_set, k_means(breast_training_seven, breast_class, k, kernel = False), breast_class, 1, regression = False, sigma = None)
  print("breast eighth training start")
  result_breast_eight = KNN(breast_HP_set, k_means(breast_training_eight, breast_class, k, kernel = False), breast_class, 1, regression = False, sigma = None)
  print("breast ninth training start")
  result_breast_nine = KNN(breast_HP_set, k_means(breast_training_nine, breast_class, k, kernel = False), breast_class, 1, regression = False, sigma = None)
  print("breast tenth training start")
  result_breast_ten = KNN(breast_HP_set, k_means(breast_training_ten, breast_class, k, kernel = False), breast_class, 1, regression = False, sigma = None)
  print(result_breast_one)
  print(result_breast_two)
  print(result_breast_three)
  print(result_breast_four)
  print(result_breast_five)
  print(result_breast_six)
  print(result_breast_seven)
  print(result_breast_eight)
  print(result_breast_nine)
  print(result_breast_ten)

  print(zero_loss_func(result_breast_one))
  print(zero_loss_func(result_breast_two))
  print(zero_loss_func(result_breast_three))
  print(zero_loss_func(result_breast_four))
  print(zero_loss_func(result_breast_five))
  print(zero_loss_func(result_breast_six))
  print(zero_loss_func(result_breast_seven))
  print(zero_loss_func(result_breast_eight))
  print(zero_loss_func(result_breast_nine))
  print(zero_loss_func(result_breast_ten))
  #(actual, predicted)
  #(correct, incorrect)

k_list = [1, 3, 5, 11, 15]
sigma_list = [100, 500, 1000, 5000, 10000]
#Sigma of one returns nan
epsilon_list = [100, 1000]
s = 5000
e = 10
#sigma value of 0.1, 1, and 10 looked too small that many predicted values are nan
#sigma value of 5000 looked performed the best on the KNN
#epsilon value did not change the performance of the edited-KNN so we chose 10 out of [10, 100, 1000]
 
forest_training_one = train_merge(forest_ten, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_two = train_merge(forest_ten, forest_one, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_three = train_merge(forest_ten, forest_one, forest_two, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_four = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_five = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_six = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_five, forest_seven, forest_eight, forest_nine)
forest_training_seven = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_eight, forest_nine)
forest_training_eight = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_nine)
forest_training_nine = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight)
forest_training_ten = train_merge(forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
 
for s in sigma_list:
  for e in epsilon_list:
    print("----------------------------------------------------------------------------------------------------------------------------")
   
    for k in k_list:
      print("----------------------------------------------------------------------------------------------------------------------------")
      print("this is the edited-KNN result for sigma =", s, ",and for epsilon =", e)
      print("this is the edited-KNN result for k =", k)
      #print("abalone first training start")
      result_forest_one = KNN(forest_HP_set, edited_clustering_KNN(forest_training_one, forest_class, k, epsilon=e), forest_class, k, regression=True, sigma=s)
      #print("abalone second training start")
      result_forest_two = KNN(forest_HP_set, edited_clustering_KNN(forest_training_two, forest_class, k, epsilon=e), forest_class, k, regression=True, sigma=s)
      #print("abalone third training start")
      result_forest_three = KNN(forest_HP_set, edited_clustering_KNN(forest_training_three, forest_class, k, epsilon=e), forest_class, k, regression=True, sigma=s)
      #print("abalone fourth training start")
      result_forest_four = KNN(forest_HP_set, edited_clustering_KNN(forest_training_four, forest_class, k, epsilon=e), forest_class, k, regression=True, sigma=s)
      #print("abalone fifth training start")
      result_forest_five = KNN(forest_HP_set, edited_clustering_KNN(forest_training_five, forest_class, k, epsilon=e), forest_class, k, regression=True, sigma=s)
      #print("abalone sixth training start")
      result_forest_six = KNN(forest_HP_set, edited_clustering_KNN(forest_training_six, forest_class, k, epsilon=e), forest_class, k, regression=True, sigma=s)
      #print("abalone seventh training start")
      result_forest_seven = KNN(forest_HP_set, edited_clustering_KNN(forest_training_seven, forest_class, k, epsilon=e), forest_class, k, regression=True, sigma=s)
      #print("abalone eighth training start")
      result_forest_eight = KNN(forest_HP_set, edited_clustering_KNN(forest_training_eight, forest_class, k, epsilon=e), forest_class, k, regression=True, sigma=s)
      #print("abalone ninth training start")
      result_forest_nine = KNN(forest_HP_set, edited_clustering_KNN(forest_training_nine, forest_class, k, epsilon=e), forest_class, k, regression=True, sigma=s)
      #print("abalone tenth training start")
      result_forest_ten = KNN(forest_HP_set, edited_clustering_KNN(forest_training_ten, forest_class, k, epsilon=e), forest_class, k, regression=True, sigma=s)
 
      print(result_forest_one)
      print(result_forest_two)
      print(result_forest_three)
      print(result_forest_four)
      print(result_forest_five)
      print(result_forest_six)
      print(result_forest_seven)
      print(result_forest_eight)
      print(result_forest_nine)
      print(result_forest_ten)
   
      print(Mean_Squared_Error(result_forest_one))
      print(Mean_Squared_Error(result_forest_two))
      print(Mean_Squared_Error(result_forest_three))
      print(Mean_Squared_Error(result_forest_four))
      print(Mean_Squared_Error(result_forest_five))
      print(Mean_Squared_Error(result_forest_six))
      print(Mean_Squared_Error(result_forest_seven))
      print(Mean_Squared_Error(result_forest_eight))
      print(Mean_Squared_Error(result_forest_nine))
      print(Mean_Squared_Error(result_forest_ten))

k_list = [1, 3, 5, 7, 9, 11, 15]
 
soybean_training_one = train_merge(soybean_ten, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_two = train_merge(soybean_ten, soybean_one, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_three = train_merge(soybean_ten, soybean_one, soybean_two, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_four = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_five = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_six, soybean_seven, soybean_eight, soybean_nine)
soybean_training_six = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_seven, soybean_eight, soybean_nine)
soybean_training_seven = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_eight, soybean_nine)
soybean_training_eight = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_nine)
soybean_training_nine = train_merge(soybean_ten, soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight)
soybean_training_ten = train_merge(soybean_one, soybean_two, soybean_three, soybean_four, soybean_five, soybean_six, soybean_seven, soybean_eight, soybean_nine)

for k in k_list:
  #KNN only
  print("----------------------------------------------------------------------------------------------------------------------------")
  print("this is the KNN result for k =", k)
  print("soybean first training start")
  result_soybean_one = KNN(soybean_HP_set, soybean_training_one, soybean_class, k, kernel=False)
  print("soybean second training start")
  result_soybean_two = KNN(soybean_HP_set, soybean_training_two, soybean_class, k, kernel=False)
  print("soybean third training start")
  result_soybean_three = KNN(soybean_HP_set, soybean_training_three, soybean_class, k, kernel=False)
  print("soybean fourth training start")
  result_soybean_four = KNN(soybean_HP_set, soybean_training_four, soybean_class, k, kernel=False)
  print("soybean fifth training start")
  result_soybean_five = KNN(soybean_HP_set, soybean_training_five, soybean_class, k, kernel=False)
  print("soybean sixth training start")
  result_soybean_six = KNN(soybean_HP_set, soybean_training_six, soybean_class, k, kernel=False)
  print("soybean seventh training start")
  result_soybean_seven = KNN(soybean_HP_set, soybean_training_seven, soybean_class, k, kernel=False)
  print("soybean eighth training start")
  result_soybean_eight = KNN(soybean_HP_set, soybean_training_eight, soybean_class, k, kernel=False)
  print("soybean ninth training start")
  result_soybean_nine = KNN(soybean_HP_set, soybean_training_nine, soybean_class, k, kernel=False)
  print("soybean tenth training start")
  result_soybean_ten = KNN(soybean_HP_set, soybean_training_ten, soybean_class, k, kernel=False)
  print(result_soybean_one)
  print(result_soybean_two)
  print(result_soybean_three)
  print(result_soybean_four)
  print(result_soybean_five)
  print(result_soybean_six)
  print(result_soybean_seven)
  print(result_soybean_eight)
  print(result_soybean_nine)
  print(result_soybean_ten)
  
  print(zero_loss_func(result_soybean_one))
  print(zero_loss_func(result_soybean_two))
  print(zero_loss_func(result_soybean_three))
  print(zero_loss_func(result_soybean_four))
  print(zero_loss_func(result_soybean_five))
  print(zero_loss_func(result_soybean_six))
  print(zero_loss_func(result_soybean_seven))
  print(zero_loss_func(result_soybean_eight))
  print(zero_loss_func(result_soybean_nine))
  print(zero_loss_func(result_soybean_ten))
  
  #(correct, incorrect)

k_list = [5]
sigma_list = [1]
count_break_list = [3]
abalone_training_one = train_merge(abalone_ten, abalone_two, abalone_three, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_eight, abalone_nine)
abalone_training_two = train_merge(abalone_ten, abalone_one, abalone_three, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_eight, abalone_nine)
abalone_training_three = train_merge(abalone_ten, abalone_one, abalone_two, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_eight, abalone_nine)
abalone_training_four = train_merge(abalone_ten, abalone_one, abalone_two, abalone_three, abalone_five, abalone_six, abalone_seven, abalone_eight, abalone_nine)
abalone_training_five = train_merge(abalone_ten, abalone_one, abalone_two, abalone_three, abalone_four, abalone_six, abalone_seven, abalone_eight, abalone_nine)
abalone_training_six = train_merge(abalone_ten, abalone_one, abalone_two, abalone_three, abalone_four, abalone_five, abalone_seven, abalone_eight, abalone_nine)
abalone_training_seven = train_merge(abalone_ten, abalone_one, abalone_two, abalone_three, abalone_four, abalone_five, abalone_six, abalone_eight, abalone_nine)
abalone_training_eight = train_merge(abalone_ten, abalone_one, abalone_two, abalone_three, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_nine)
abalone_training_nine = train_merge(abalone_ten, abalone_one, abalone_two, abalone_three, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_eight)
abalone_training_ten = train_merge(abalone_one, abalone_two, abalone_three, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_eight, abalone_nine)
 
 
for s in sigma_list:
  for c in count_break_list:
    print("----------------------------------------------------------------------------------------------------------------------------")
    print("this is the Kmeans result for s =", s, " and counter break = ", c )
    for k in k_list:
      print("----------------------------------------------------------------------------------------------------------------------------")
      #soybean_training_two, 15, soybean_class, kernel = False (dftest, dftrain, class_name, k, regression=False, sigma=None)
      print("this is the KNN result for k =", k)
      print("abalone first training start")
      result_abalone_one = KNN(abalone_HP_set, k_means(abalone_training_one, k, abalone_class, c, kernel = True), abalone_class, 1, regression=True, sigma=s)
      print("abalone second training start")
      result_abalone_two = KNN(abalone_HP_set, k_means(abalone_training_two, k, abalone_class, c, kernel = True), abalone_class, 1, regression=True, sigma=s)
      print("abalone third training start")
      result_abalone_three = KNN(abalone_HP_set, k_means(abalone_training_three, k, abalone_class,  c, kernel = True), abalone_class, 1, regression=True, sigma=s)
      print("abalone fourth training start")
      result_abalone_four = KNN(abalone_HP_set, k_means(abalone_training_four, k, abalone_class,  c, kernel = True), abalone_class, 1, regression=True, sigma=s)
      print("abalone fifth training start")
      result_abalone_five = KNN(abalone_HP_set, k_means(abalone_training_five, k, abalone_class,  c, kernel = True),abalone_class, 1, regression=True, sigma=s)
      print("abalone sixth abalone_HP_set start")
      result_abalone_six = KNN(abalone_HP_set, k_means(abalone_training_six, k, abalone_class,  c, kernel = True),abalone_class, 1, regression=True, sigma=s)
      print("abalone seventh training start")
      result_abalone_seven = KNN(abalone_HP_set, k_means(abalone_training_seven, k, abalone_class,  c, kernel = True),abalone_class, 1, regression=True, sigma=s)
      print("abalone eighth training start")
      result_abalone_eight = KNN(abalone_HP_set, k_means(abalone_training_eight, k, abalone_class,  c, kernel = True),abalone_class, 1, regression=True, sigma=s)
      print("abalone ninth training start")
      result_abalone_nine = KNN(abalone_HP_set, k_means(abalone_training_nine, k, abalone_class,  c, kernel = True),abalone_class, 1, regression=True, sigma=s)
      print("abalone tenth training start")
      result_abalone_ten = KNN(abalone_HP_set, k_means(abalone_training_ten, k, abalone_class,  c, kernel = True),abalone_class, 1, regression=True, sigma=s)
      
      print(result_abalone_one)
      print(result_abalone_two)
      print(result_abalone_three)
      print(result_abalone_four)
      print(result_abalone_five)
      print(result_abalone_six)
      print(result_abalone_seven)
      print(result_abalone_eight)
      print(result_abalone_nine)
      print(result_abalone_ten)
 
      print(Mean_Squared_Error(result_abalone_one))
      print(Mean_Squared_Error(result_abalone_two))
      print(Mean_Squared_Error(result_abalone_three))
      print(Mean_Squared_Error(result_abalone_four))
      print(Mean_Squared_Error(result_abalone_five))
      print(Mean_Squared_Error(result_abalone_six))
      print(Mean_Squared_Error(result_abalone_seven))
      print(Mean_Squared_Error(result_abalone_eight))
      print(Mean_Squared_Error(result_abalone_nine))
      print(Mean_Squared_Error(result_abalone_ten))

k_list = [11]
 
breast_training_one = train_merge(breast_ten, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine)
breast_training_two = train_merge(breast_ten, breast_one, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine)
breast_training_three = train_merge(breast_ten, breast_one, breast_two, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine)
breast_training_four = train_merge(breast_ten, breast_one, breast_two, breast_three, breast_five, breast_six, breast_seven, breast_eight, breast_nine)
breast_training_five = train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_six, breast_seven, breast_eight, breast_nine)
breast_training_six = train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_five, breast_seven, breast_eight, breast_nine)
breast_training_seven = train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_eight, breast_nine)
breast_training_eight = train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_nine)
breast_training_nine = train_merge(breast_ten, breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight)
breast_training_ten = train_merge(breast_one, breast_two, breast_three, breast_four, breast_five, breast_six, breast_seven, breast_eight, breast_nine)
 
 
 
for k in k_list:
  print("----------------------------------------------------------------------------------------------------------------------------")
  print("this is the KNN result for k =", k)
  print("breast first training start")
  result_breast_one = KNN(breast_HP_set, edited_clustering_KNN(breast_training_one, breast_class, k), breast_class, k, regression = False, sigma = None)
  print(result_breast_one)
  print(zero_loss_func(result_breast_one))
  print("breast second training start")
  result_breast_two = KNN(breast_HP_set, edited_clustering_KNN(breast_training_two, breast_class, k), breast_class, k, regression = False, sigma = None)
  print(result_breast_two)
  print(zero_loss_func(result_breast_two))
  print("breast third training start")
  result_breast_three = KNN(breast_HP_set, edited_clustering_KNN(breast_training_three, breast_class, k), breast_class, k, regression = False, sigma = None)
  print(result_breast_three)
  print(zero_loss_func(result_breast_three))
  print("breast fourth training start")
  result_breast_four = KNN(breast_HP_set, edited_clustering_KNN(breast_training_four, breast_class, k), breast_class, k, regression = False, sigma = None)
  print(result_breast_four)
  print(zero_loss_func(result_breast_four))
  print("breast fifth training start")
  result_breast_five = KNN(breast_HP_set, edited_clustering_KNN(breast_training_five, breast_class, k), breast_class, k, regression = False, sigma = None)
  print(result_breast_five)
  print(zero_loss_func(result_breast_five))
  print("breast sixth training start")
  result_breast_six = KNN(breast_HP_set, edited_clustering_KNN(breast_training_six, breast_class, k), breast_class, k, regression = False, sigma = None)
  print(result_breast_six)
  print(zero_loss_func(result_breast_six))
  print("breast seventh training start")
  result_breast_seven = KNN(breast_HP_set, edited_clustering_KNN(breast_training_seven, breast_class, k), breast_class, k, regression = False, sigma = None)
  print(result_breast_seven)
  print(zero_loss_func(result_breast_seven))
  print("breast eighth training start")
  result_breast_eight = KNN(breast_HP_set, edited_clustering_KNN(breast_training_eight, breast_class, k), breast_class, k, regression = False, sigma = None)
  print(result_breast_eight)
  print(zero_loss_func(result_breast_eight))
  print("breast ninth training start")
  result_breast_nine = KNN(breast_HP_set, edited_clustering_KNN(breast_training_nine, breast_class, k), breast_class, k, regression = False, sigma = None)
  print(result_breast_nine)
  print(zero_loss_func(result_breast_nine))
  print("breast tenth training start")
  result_breast_ten = KNN(breast_HP_set, edited_clustering_KNN(breast_training_ten, breast_class, k), breast_class, k, regression = False, sigma = None)
  print(result_breast_one)
  print(result_breast_two)
  print(result_breast_three)
  print(result_breast_four)
  print(result_breast_five)
  print(result_breast_six)
  print(result_breast_seven)
  print(result_breast_eight)
  print(result_breast_nine)
  print(result_breast_ten)
 
  print(zero_loss_func(result_breast_one))
  print(zero_loss_func(result_breast_two))
  print(zero_loss_func(result_breast_three))
  print(zero_loss_func(result_breast_four))
  print(zero_loss_func(result_breast_five))
  print(zero_loss_func(result_breast_six))
  print(zero_loss_func(result_breast_seven))
  print(zero_loss_func(result_breast_eight))
  print(zero_loss_func(result_breast_nine))
  print(zero_loss_func(result_breast_ten))

k_list = [1, 3, 5, 11, 15]
 
glass_training_one = train_merge(glass_ten, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_two = train_merge(glass_ten, glass_one, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_three = train_merge(glass_ten, glass_one, glass_two, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_four = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_five, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_five = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_six, glass_seven, glass_eight, glass_nine)
glass_training_six = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_seven, glass_eight, glass_nine)
glass_training_seven = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_eight, glass_nine)
glass_training_eight = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_nine)
glass_training_nine = train_merge(glass_ten, glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight)
glass_training_ten = train_merge(glass_one, glass_two, glass_three, glass_four, glass_five, glass_six, glass_seven, glass_eight, glass_nine)

for k in k_list:
  print("----------------------------------------------------------------------------------------------------------------------------")
  print("this is the KNN result for k =", k)
  print("glass first training start")
  result_glass_one = KNN(glass_HP_set, glass_training_one, glass_class, k, kernel=False)
  print("glass second training start")
  result_glass_two = KNN(glass_HP_set, glass_training_two, glass_class, k, kernel=False)
  print("glass third training start")
  result_glass_three = KNN(glass_HP_set, glass_training_three, glass_class, k, kernel=False)
  print("glass fourth training start")
  result_glass_four = KNN(glass_HP_set, glass_training_four, glass_class, k, kernel=False)
  print("glass fifth training start")
  result_glass_five = KNN(glass_HP_set, glass_training_five, glass_class, k, kernel=False)
  print("glass sixth training start")
  result_glass_six = KNN(glass_HP_set, glass_training_six, glass_class, k, kernel=False)
  print("glass seventh training start")
  result_glass_seven = KNN(glass_HP_set, glass_training_seven, glass_class, k, kernel=False)
  print("glass eighth training start")
  result_glass_eight = KNN(glass_HP_set, glass_training_eight, glass_class, k, kernel=False)
  print("glass ninth training start")
  result_glass_nine = KNN(glass_HP_set, glass_training_nine, glass_class, k, kernel=False)
  print("glass tenth training start")
  result_glass_ten = KNN(glass_HP_set, glass_training_ten, glass_class, k, kernel=False)
  print(result_glass_one)
  print(result_glass_two)
  print(result_glass_three)
  print(result_glass_four)
  print(result_glass_five)
  print(result_glass_six)
  print(result_glass_seven)
  print(result_glass_eight)
  print(result_glass_nine)
  print(result_glass_ten)
  
  print(zero_loss_func(result_glass_one))
  print(zero_loss_func(result_glass_two))
  print(zero_loss_func(result_glass_three))
  print(zero_loss_func(result_glass_four))
  print(zero_loss_func(result_glass_five))
  print(zero_loss_func(result_glass_six))
  print(zero_loss_func(result_glass_seven))
  print(zero_loss_func(result_glass_eight))
  print(zero_loss_func(result_glass_nine))
  print(zero_loss_func(result_glass_ten))
  
  #(correct, incorrect)

k_list = [1, 3, 5, 11, 15]
sigma_list = [10000]
 
forest_training_one = train_merge(forest_ten, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_two = train_merge(forest_ten, forest_one, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_three = train_merge(forest_ten, forest_one, forest_two, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_four = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_five = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_six, forest_seven, forest_eight, forest_nine)
forest_training_six = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_five, forest_seven, forest_eight, forest_nine)
forest_training_seven = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_eight, forest_nine)
forest_training_eight = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_nine)
forest_training_nine = train_merge(forest_ten, forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight)
forest_training_ten = train_merge(forest_one, forest_two, forest_three, forest_four, forest_five, forest_six, forest_seven, forest_eight, forest_nine)
 


 #(dftest, dftrain, class_name, k, regression=False, sigma=None
for s in sigma_list:
  print("----------------------------------------------------------------------------------------------------------------------------")
  print("this is the KNN result for s =", s)
  for k in k_list:
    print("----------------------------------------------------------------------------------------------------------------------------")
    print("this is the KNN result for k =", k)
    print("forest first training start")
    result_forest_one = KNN(forest_HP_set, forest_training_one, forest_class, k, regression=True, sigma=s)
    print("forest second training start")
    result_forest_two = KNN(forest_HP_set, forest_training_two, forest_class, k, regression=True, sigma=s)
    print("forest third training start")
    result_forest_three = KNN(forest_HP_set, forest_training_three, forest_class, k, regression=True, sigma=s)
    print("forest fourth training start")
    result_forest_four = KNN(forest_HP_set, forest_training_four, forest_class, k, regression=True, sigma=s)
    print("forest fifth training start")
    result_forest_five = KNN(forest_HP_set, forest_training_five, forest_class, k, regression=True, sigma=s)
    print("forest sixth forest_HP_set start")
    result_forest_six = KNN(forest_HP_set, forest_training_six, forest_class, k, regression=True, sigma=s)
    print("forest seventh training start")
    result_forest_seven = KNN(forest_HP_set, forest_training_seven, forest_class, k, regression=True, sigma=s)
    print("forest eighth training start")
    result_forest_eight = KNN(forest_HP_set, forest_training_eight, forest_class, k, regression=True, sigma=s)
    print("forest ninth training start")
    result_forest_nine = KNN(forest_HP_set, forest_training_nine, forest_class, k, regression=True, sigma=s)
    print("forest tenth training start")
    result_forest_ten = KNN(forest_HP_set, forest_training_ten, forest_class, k, regression=True, sigma=s)
    print(result_forest_one)
    print(result_forest_two)
    print(result_forest_three)
    print(result_forest_four)
    print(result_forest_five)
    print(result_forest_six)
    print(result_forest_seven)
    print(result_forest_eight)
    print(result_forest_nine)
    print(result_forest_ten)
   
    print(Mean_Squared_Error(result_forest_one))
    print(Mean_Squared_Error(result_forest_two))
    print(Mean_Squared_Error(result_forest_three))
    print(Mean_Squared_Error(result_forest_four))
    print(Mean_Squared_Error(result_forest_five))
    print(Mean_Squared_Error(result_forest_six))
    print(Mean_Squared_Error(result_forest_seven))
    print(Mean_Squared_Error(result_forest_eight))
    print(Mean_Squared_Error(result_forest_nine))
    print(Mean_Squared_Error(result_forest_ten))

abalone_training_one = train_merge(abalone_ten, abalone_two, abalone_three, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_eight, abalone_nine)
abalone_training_two = train_merge(abalone_ten, abalone_one, abalone_three, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_eight, abalone_nine)
abalone_training_three = train_merge(abalone_ten, abalone_one, abalone_two, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_eight, abalone_nine)
abalone_training_four = train_merge(abalone_ten, abalone_one, abalone_two, abalone_three, abalone_five, abalone_six, abalone_seven, abalone_eight, abalone_nine)
abalone_training_five = train_merge(abalone_ten, abalone_one, abalone_two, abalone_three, abalone_four, abalone_six, abalone_seven, abalone_eight, abalone_nine)
abalone_training_six = train_merge(abalone_ten, abalone_one, abalone_two, abalone_three, abalone_four, abalone_five, abalone_seven, abalone_eight, abalone_nine)
abalone_training_seven = train_merge(abalone_ten, abalone_one, abalone_two, abalone_three, abalone_four, abalone_five, abalone_six, abalone_eight, abalone_nine)
abalone_training_eight = train_merge(abalone_ten, abalone_one, abalone_two, abalone_three, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_nine)
abalone_training_nine = train_merge(abalone_ten, abalone_one, abalone_two, abalone_three, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_eight)
abalone_training_ten = train_merge(abalone_one, abalone_two, abalone_three, abalone_four, abalone_five, abalone_six, abalone_seven, abalone_eight, abalone_nine)

k = 1
s = 1

result_abalone_one = KNN(abalone_one, abalone_training_one, abalone_class, k, regression=True, sigma=s)
print(Mean_Squared_Error(result_abalone_one))

result_abalone_two = KNN(abalone_two, abalone_training_two, abalone_class, k, regression=True, sigma=s)
print(Mean_Squared_Error(result_abalone_two))

result_abalone_three = KNN(abalone_three, abalone_training_three, abalone_class, k, regression=True, sigma=s)
print(Mean_Squared_Error(result_abalone_three))

result_abalone_four = KNN(abalone_four, abalone_training_four, abalone_class, k, regression=True, sigma=s)
print(Mean_Squared_Error(result_abalone_four))

result_abalone_five = KNN(abalone_five, abalone_training_five, abalone_class, k, regression=True, sigma=s)
print(Mean_Squared_Error(result_abalone_five))

result_abalone_six = KNN(abalone_six, abalone_training_six, abalone_class, k, regression=True, sigma=s)
print(Mean_Squared_Error(result_abalone_six))

result_abalone_seven = KNN(abalone_seven, abalone_training_seven, abalone_class, k, regression=True, sigma=s)
print(Mean_Squared_Error(result_abalone_seven))

result_abalone_eight = KNN(abalone_eight, abalone_training_eight, abalone_class, k, regression=True, sigma=s)
print(Mean_Squared_Error(result_abalone_eight))

result_abalone_nine = KNN(abalone_nine, abalone_training_nine, abalone_class, k, regression=True, sigma=s)
print(Mean_Squared_Error(result_abalone_nine))

result_abalone_ten = KNN(abalone_ten, abalone_training_ten, abalone_class, k, regression=True, sigma=s)
print(Mean_Squared_Error(result_abalone_ten))

k_list = [1, 3, 5, 11, 15]
sigma_list = [100, 500, 1000, 5000, 10000]
epsilon_list = [10]
s = 5000
e = 10
#sigma value of 0.1, 1, and 10 looked too small that many predicted values are nan
#sigma value of 5000 looked performed the best on the KNN
#epsilon value did not change the performance of the edited-KNN so we chose 10 out of [10, 100, 1000]
 
computer_training_one = train_merge(computer_ten, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_two = train_merge(computer_ten, computer_one, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_three = train_merge(computer_ten, computer_one, computer_two, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_four = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_five, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_five = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_six, computer_seven, computer_eight, computer_nine)
computer_training_six = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_seven, computer_eight, computer_nine)
computer_training_seven = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_eight, computer_nine)
computer_training_eight = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_nine)
computer_training_nine = train_merge(computer_ten, computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight)
computer_training_ten = train_merge(computer_one, computer_two, computer_three, computer_four, computer_five, computer_six, computer_seven, computer_eight, computer_nine)

for s in sigma_list:
  print("----------------------------------------------------------------------------------------------------------------------------")
  print("this is the edited-KNN result for sigma =", s, ",and for epsilon =", e)
  for k in k_list:
    print("----------------------------------------------------------------------------------------------------------------------------")
    print("this is the edited-KNN result for k =", k)
    #print("abalone first training start")
    result_computer_one = KNN(computer_HP_set, edited_clustering_KNN(computer_training_one, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
    #print("abalone second training start")
    result_computer_two = KNN(computer_HP_set, edited_clustering_KNN(computer_training_two, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
    #print("abalone third training start")
    result_computer_three = KNN(computer_HP_set, edited_clustering_KNN(computer_training_three, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
    #print("abalone fourth training start")
    result_computer_four = KNN(computer_HP_set, edited_clustering_KNN(computer_training_four, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
    #print("abalone fifth training start")
    result_computer_five = KNN(computer_HP_set, edited_clustering_KNN(computer_training_five, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
    #print("abalone sixth training start")
    result_computer_six = KNN(computer_HP_set, edited_clustering_KNN(computer_training_six, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
    #print("abalone seventh training start")
    result_computer_seven = KNN(computer_HP_set, edited_clustering_KNN(computer_training_seven, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
    #print("abalone eighth training start")
    result_computer_eight = KNN(computer_HP_set, edited_clustering_KNN(computer_training_eight, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
    #print("abalone ninth training start")
    result_computer_nine = KNN(computer_HP_set, edited_clustering_KNN(computer_training_nine, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)
    #print("abalone tenth training start")
    result_computer_ten = KNN(computer_HP_set, edited_clustering_KNN(computer_training_ten, computer_class, k, epsilon=e), computer_class, k, regression=True, sigma=s)

    print(result_computer_one)
    print(result_computer_two)
    print(result_computer_three)
    print(result_computer_four)
    print(result_computer_five)
    print(result_computer_six)
    print(result_computer_seven)
    print(result_computer_eight)
    print(result_computer_nine)
    print(result_computer_ten)
    
    print(Mean_Squared_Error(result_computer_one))
    print(Mean_Squared_Error(result_computer_two))
    print(Mean_Squared_Error(result_computer_three))
    print(Mean_Squared_Error(result_computer_four))
    print(Mean_Squared_Error(result_computer_five))
    print(Mean_Squared_Error(result_computer_six))
    print(Mean_Squared_Error(result_computer_seven))
    print(Mean_Squared_Error(result_computer_eight))
    print(Mean_Squared_Error(result_computer_nine))
    print(Mean_Squared_Error(result_computer_ten))
  
  #(correct, incorrect)

#sigma_list = [100, 500, 1000, 5000, 10000]
#k_means(data_set, k, class_name, counter_break,kernel)
s = 10000
k_list = [2, 3, 5, 11, 15]
k_list = [5]
counter = [3,5,15]
for c in counter:
  print("----------------------------------------------------------------------------------------------------------------------------")
  print("this is the K_means-KNN result for counter =", c)
  for k in k_list:
    print("----------------------------------------------------------------------------------------------------------------------------")
    print("this is the K_means-KNN result for k =", k)
    #print("abalone first training start")
    result_computer_one = KNN(computer_HP_set, k_means(computer_training_one, k, computer_class, c, True), computer_class, 1, regression=True, sigma=s)
    #print("abalone second training start")
    result_computer_two = KNN(computer_HP_set, k_means(computer_training_two, k, computer_class, c, True), computer_class, 1, regression=True, sigma=s)
    #print("abalone third training start")
    result_computer_three = KNN(computer_HP_set, k_means(computer_training_three, k, computer_class, c, True), computer_class, 1, regression=True, sigma=s)
    #print("abalone fourth training start")
    result_computer_four = KNN(computer_HP_set, k_means(computer_training_four, k, computer_class, c, True), computer_class, 1, regression=True, sigma=s)
    #print("abalone fifth training start")
    result_computer_five = KNN(computer_HP_set, k_means(computer_training_five, k, computer_class, c, True), computer_class, 1, regression=True, sigma=s)
    #print("abalone sixth training start")
    result_computer_six = KNN(computer_HP_set, k_means(computer_training_six, k, computer_class, c, True), computer_class, 1, regression=True, sigma=s)
    #print("abalone seventh training start")
    result_computer_seven = KNN(computer_HP_set, k_means(computer_training_seven, k, computer_class, c, True), computer_class, 1, regression=True, sigma=s)
    #print("abalone eighth training start")
    result_computer_eight = KNN(computer_HP_set, k_means(computer_training_eight, k, computer_class, c, True), computer_class, 1, regression=True, sigma=s)
    #print("abalone ninth training start")
    result_computer_nine = KNN(computer_HP_set, k_means(computer_training_nine, k, computer_class, c, True), computer_class, 1, regression=True, sigma=s)
    #print("abalone tenth training start")
    result_computer_ten = KNN(computer_HP_set, k_means(computer_training_ten, k, computer_class, c, True), computer_class, 1, regression=True, sigma=s)
    
    a = Mean_Squared_Error(result_computer_one)
    b = Mean_Squared_Error(result_computer_two)
    c = Mean_Squared_Error(result_computer_three)
    d = Mean_Squared_Error(result_computer_four)
    e = Mean_Squared_Error(result_computer_five)
    f = Mean_Squared_Error(result_computer_six)
    g = Mean_Squared_Error(result_computer_seven)
    h = Mean_Squared_Error(result_computer_eight)
    i = Mean_Squared_Error(result_computer_nine)
    j = Mean_Squared_Error(result_computer_ten)

    mean = a+b+c+d+e+f+g+h+i+j
    mean = mean/10
    print(mean)
  
  #(correct, incorrect)